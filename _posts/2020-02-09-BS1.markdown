---
layout: post
title: "Bayesian Statistics - Basics"
date: 2021-02-09 00:00:00
image: thumbnail_bayesian.png
tags: [bayesian]
categories: bayesian
use_math: true
---



In the following series of posts, we will be talking about **Bayesian Statistics**, which is one of the most probable field of applied statistics in 21st century. I have been into bayesian statistics for about an year now, and ever since it has given me some valuable insights on how to quantify uncertainties with its practical and intuitive framework. 

Just like any other field of study that is built upon mathematics, Bayesian Statistics is also filled with heavy amount of mathematical formulas and proofs, and I don't want to be overwhelmed by them (because I had a hard time with those nitty-gritty mathematics in the textbooks as an undergraduate student...). For this reason, I will try to keep the rigorous mathematical proofs as minimum as possible and rather try focus on the insight and the intuition Bayesian statistics can give us.

Hence, we will use **pymc3**, a renowned PPL (Probabilistic Programming Language) designed to run on python, to actually implement Bayesian inference throughout this series. As this is the very first post, let's begin with talking about some of the fundamentals that differentiates Bayesian Statistics from traditional statistics. 






---

# 1. Defining Probability

What is **probability**? Defining probability is the very first step in statistics as it is a field of study that tries to quantify the uncertainties in our lives by making use of probabilities. Refering to wikipedia, definition of probability is "*numerical descriptions of **how likely an event is to occur*** ". Although this definition might sound trivial, formally defining "**likeliness of an event**" (don't get confused with the *likelihood* in statistical sense) isn't actually that simple. Let me introduce three different perspectives on defining probability.

&nbsp;



- **Classical Definition**: 
  - Events that are likely to yield the **same outcome** have the **same probability**. For example, when tossing a coin, heads and tails are equally likely to show up and thus probability is 1/2 for each.
  - This is a primal definition long before the advance of statistics in 19th century.

&nbsp;

- **Frequentist (Classical Statistics)**: 
  - Assuming **infinite sequence of events**, probability is defined as the **relative frequency** of each events. For example, by infinitely repeating the tossing of a coin, heads and tails appear half&half.
  - By this perspective, definition of probability is somewhat **consequential** and this can sometimes contradict to our intuition (common sense) when we cannot assume infinite replication of events.
  - For example, we want to know whether it will rain tomorrow or not. Considering these binary outcomes, the probability is either 0/1 (it rained / it didn't rained). Why? - because we can observe "tomorrow" only once.

&nbsp;

- **Bayesian**:
  - Accounts for **subjectiveness** in the probability as long as it satisfies the basic [axioms of probability](https://en.wikipedia.org/wiki/Probability_axioms).
  - Compared to frequentist's definition, interpretation of probability is way more intuitive (ex. The probability of rain tomorrow is 0.73)
  - We can provide exact probabilistic answers to questions that were unanswerable in frequentist paradigm. (Ex. What is the probability that woman will like me? What is the probability that Korea will be unified in 10 years?)

&nbsp;

This might sound confusing, but the keyword here is that Bayesian's probability is **subjective**. So when I ask "What do you think is the probability that COVID-19 will end in 2022?" the answer will likely to be different depending on people because **each of us have different beliefs and concerns** about this virus. Even if it's not clear for now, we'll talk more in detail in a moment and so hang in there!



Then how exactly are we going to calculate probability in Bayesian sense? Here's a good news. In Bayesian Statistics, a single theorem solves everything! Let's take a look at Bayes' Theorem.



---



# 2. Bayes' Theorem

Bayes' Theorem is defined as the following:

<center>

$$
P(A|B) = \frac{P(A \cup B)}{P(B)}
$$

</center>

Let's interpret its meaning.

- Left side of equation: **probability of event A happening, presuming that event B had already happened**.
- Right side of equation: **probability of event A and B happening at the same time divided by probability of event B happening**.

From this equation, if we marginalize the numerator of the right side we have the following equation:



<center>

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$

</center>

The implication of this equation is that we can derive a particular conditional probability (left side) by marginalizing the joint probability into specific events. In a Bayesian sense, the conditional probability on the right side is called as the **posterior** and the numerator on the left side is each called as the **likelihood** and the **prior** while the denominator is evidence.



These three components - posterior, likelihood and prior - are the building blocks of Bayesian Statistics. In fact, the probability I mentioned to be calculated in Bayesian inference refers to posterior. Let's consider a simple example and see how it's done.



---

## 3. Frequentist vs Bayesian

For illustration, we will consider the following situation.

> Q. We have a sack which contains fair coins and unfair coins (fair coin meaning probability of heads and tails is equal). We sampled a single coin from this sack and we want to know whether this coin is fair or not. So we filpped the coin for 10 times and the result was 3 heads and 7 tails. Under this situation, can we say that our sampled coin is fair?

&nbsp;

#### Frequentist

First, let's try to solve this question by frequentists' approach. Like I mentioned earlier, frequentists assume a repetition of event and it is clear that consecutive tossing of a coin follows binomial distribution. To be more specific, this example follows binomial distribution of $n$ (number of repetition) equal to 10 and our goal is to make inference on the unknown parameter $p$.

In frequentist paradigm, we have to define an **estimator** $\hat p$ to make inference on the true parameter $p$ (for more information about estimators check out this [post](https://domug.github.io/2020/12/03/Estimation_Multicollinearity/)). There are several ways of defining an estimator, and for now let's use the most commonly used **Maximum Likelihood Method** to derive $p$ analytically. 

Maximum Likelihood Estimation in short, is assuming that our parameter is some **unknown but fixed constant** and trying to find the value that has the **highest density in the probability distribution of events** (likelihood) and use it as an estimator on behalf of the true parameter. In other words, considering our example, it is equivalent to finding $p$ that maximizes the binomial probability density $n \choose x$ $p^x (1-p)^{n-x}$. 

Since our main topic is on bayesian we are going to skip ahead to the conclusion, but note that analytical solution for our example is relatively trivial in that as binomial density is a concave function, we can just set the differential equation to 0 and solve it to get the answer. Anyways, if we do the math then our MLE (Maximum Likelihood Estimator) is $\frac{\sum_{i=1}^{n} x_i}{n}$ which is a sample mean of the number of heads. 

In conclusion, for our example we derived MLE of $p$ to be the sample mean, which is $\frac{3}{10}$. Then as a consequent step, can we say that our coin is fair regarding this estimator? The answer is "**No**" - if we toss this coin infinitely many times, the probability of heads is only about 30% which clearly indicates this coin is somehow biased. 

This seems to make sense, but let's say that we have already tossed coin and we are yet to see the outcome. In this situation the probability of heads is either 0 or 1. It's a bit awkward right? We would normally expect the answer to be in a probabilistic sense like 0.3, but in frequentist paradigm our parameter $p$ is a **fixed constant**, so the state of the outcome as head and tail can't co-exist at the same time.












[code]: https://github.com/domug/Codes-for-blogs/blob/master/Dimension%20Reduction/LDA.R






