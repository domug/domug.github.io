---
layout: post
title: "Factor Analysis"
date: 2020-12-06 00:00:00
image: thumbnail_fa.png
tags: [statistics, dimensionreduction, pca]
categories: [statistics]
---

### Previous Posts

[1. Statistical Estimation and Multicollinearity](https://domug.github.io/2020/12/03/Estimation_Multicollinearity/)

[2. Variable Selection Methods](https://domug.github.io/2020/12/04/Ridge_Lasso/)

[3. Principal Component Analysis](https://domug.github.io/2020/12/05/PCA/)

---

# Factor Analysis (FA)

In line with PCA, Factor Analysis (FA) is another famous dimension reduction method widely used for finding **latent variables**. Latant variable refers to variables that are hidden. Let's go back the example about BTS Jin that I illustrated when talking about multicollinearity. I said when there are variables about Jin's appearance like “width of eye”, “length of eye”, “color of eye”, these variables can be compressed into a single variable - "eye", which is a latent variable hidden behind those three variables.

Often the goal of factor analysis is to summarize the original dataset which has a lot of varibles by extracting some hidden commonalities in the variables. These commonalities are called as **"factors"** and they are used throughout subsequent statiscal analysis. So it is easy to think the factors as some groups of original variables. In mathematical sense, it is to reduce a p-dimensional random vector X into a fewer "k" latent variables.

Usually it is most common to use continuous & quantitative variables as our example will be so, but keep in mind that sometimes qualitative can also be used. For example, we might have nominal data such as [0, 1] where each number represents gender. In this case we can use factor analysis to find out gender characteristics or so.

Then let's dig into the details of factor analysis. You might wonder, "then what is the difference between PCA and FA?". This can formally be understood by looking at an equation of factor analysis.

<p align="center">
	<img src="{{site.baseurl}}/images/fa/fa_equation.png">
</p>

The above is the equational definition of orthogonal factor model. We can see that the original data matrix is decomposed into 3 parts. The first part is related to the unknown factors and the matrix Q and F are unknowns which are to be estimated. The second part is the error term, which accounts for individual variations of each variable. The third part is the mean of each variables, but since it's a custom to standardize each variable before analysis, this term is often ignored.

Recall that PCA was a linear transformation of X using all of the variables to create new principal components. In factor analysis, however, we decompose the original matrix into predefined factors that are **limited in numbers**. Therefore, since we are only using only a few factors, there's inevitably some noise, or error term associated with each variables. This is the main difference of PCA and FA. To put it simply, unlike PCA where we use all the variables to re-express original dataset, FA is like a regression on the original data by a few factors.

Then how are we going to find matrix Q and F? The most commonly used are "Principal Component Method", "Maximum Likelihood Method" and "Common Factor Method", and there is no absolute criterion to ensure which one is best. Since R conveniently does that for us, we will not go detail into each methods. 

The last theoritical part to look at before going into the implementation is **Factor Loadings**. After performing FA, most often we will get factor loadings matrix that tells us which factor is related to which original variables. The elements of factor loadings matrix are simply the correlations between every factors and variables.

During this process, the technic so called **"Factor Rotation"** is often used to maximize the relationship between factors and original variables. What it does is that it rotates the axes of each factors at the origin until each factor loadings values become close to 0 and 1. We will see how it works in a minute, so don't worry even if the idea doesn't come right into your head.


---

# FA example

We will use the same car data that we used for PCA. I will skip the data preprocessing steps as it will be the same as we did in the PCA, so if you are interested check out the [previous post][pca]. We will use `factanal` function in R. Here's the result.

<p align="center">
	<img width="900" height="700" src="{{site.baseurl}}/images/fa/fa.png">
</p>

Above is factor loadings matrix for a factor analysis using 4 factors with no factor rotation. The number of factors to be used is a hyper parameter that we have to choose for ourselves, and there is no rule for that. Actually after performing factor analysis, if we just execute the variable that contains the result of the factor analysis (which in our case is `fa`), then at the bottom some result of statiscal hypothesis tesing will be presented like the following.

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/fa_test.png">
</p>

Here, the null hypothesis is "The number of factors that we have used is sufficient". So if the p-value is larger than 0.05 (or 0.01 based on your preference) we can "statistically" say that the number of factors we used is decent. However this is just a reference and not to have blind faith on it. In our example, the p-value was a little bit less than 0.05, but as having 5 factors seem too much, I would conclude to use 4 factors for our dataset.

Anyways, back to our factor loadings matrix. It seems like we have a problem. Most of the elements in the loadings matrix have values larger than 0.3. This is problematic because in this situation, we can't really distinguish which factor is closely related to which variables. In other words, we can't marginalize the characteristics of each factors. To deal with this problem, we are going to use "factor rotation". There are various rotation methods, but among them the so called **"varimax"** rotation is mostly widely used so we will stick to that for our example.

<p align="center">
	<img width="900" height="700" src="{{site.baseurl}}/images/fa/fa_varimax.png">
</p>

Now the values of factor loadings became relatively more clear (values are close to 0 or 1) compared to when we didn't use rotation. Furthermore, it seems like we can name the factors so let's do that. Based on the factor scores in the loadings matrix, I've named each factor as the following.

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/fa_table.png">
</p>

As always, keep in mind that interpreting the results is very subjective. Nevertheless, if we can all agree on the naming of the factors as above, then as a next step, it's time to figure out if there is any pattern in the cars associated with each factors. Since we have four factors, it would be difficult to visualize the results in a single graph. Therefore we will sort the car dataset by the scores of each factor and see what it allows us to do. I will only present the results in here, but if you are interested in hard codes, they are on my [github][code].

---
### Factor 1 (Size of car)

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/result_1.png">
</p>

When we sort the cars according to the first factor, then the top 10 cars are from the company labeled as "1", which indicates Amercan companies. On the other hand, we can see that lots of cars are labeled as company "3" in the bottom 10. From this, we can derive the following insight.
- **In general, American cars are big in size and Europe cars are small in size.**

---
### Factor 2 (Inner space of car)

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/result_2.png">
</p>

By sorting the cars in terms of the second factor, we can see that most of the cars in the top of the list are labeled as company "1", while at the bottom relatively many cars are labeled as company "2". This indicates the following.
- **In general, American cars have larger inner spaces while japanese cars have small inner spaces.**

This doesn't seem new because from factor 1, we have found that American cars are usually big so it's no surprise that these cars have larger space inside as well.

---
### Factor 3 (Customer Service)

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/result_3.png">
</p>

Unlike the first two factors, it is evident that the top ranked cars are all from company label "2", meaning that they are japanese cars. By this, we can say that
- **Japanese cars provide good after service for customers.**

We can see the sincerity of japanese companies. That's the reason why we all love Japan right?

---
### Factor 4 (Price)

<p align="center">
	<img width="700" height="500" src="{{site.baseurl}}/images/fa/result_4.png">
</p>

If we look at the top ten cars, it looks like they are mostly American and European cars. But let's focus on the models this time. Although I don't know much about cars, I've heard of Cadillac, Lincoln, BMW, Audi and I know that they are all high class cars (My dream car is Audi A7!). So I would say factor 4 evidently indicates price (or class) of cars.

- **American cars vary a lot in terms of price, while European cars are expensive.**

---

We have finished the second topic on statistical dimension reduction method - Factor Analysis! 

We were lucky that we could actually name the factors and interpret our dataset based on that, but in real life there are many cases when we can't interpret the results as well. These situations often happen because factor loading scores are not distinguished clearly. So lot of rotation methods have been developed such as QUARTIMAX, EQUIMAX, VARIMAX et cetera, so check those out if you're interested!

In the next post, we will move on to **Cluster Analysis**, which is another super cool and useful dimension reduction method!



[pca]: https://domug.github.io/2020/12/05/PCA/
[code]: https://github.com/domug/Codes-for-blogs/blob/master/Dimension%20Reduction/PCA.R






---
