I"h4<p>This post is an overall summary of Chapter 5 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<p><strong>3.1. Convergence of Random Variables</strong></p>

<ul>
  <li>Convergence of Sequences</li>
  <li>Almost-sure convergence</li>
  <li>Convergence in probability</li>
  <li>Convergence in quadratic mean</li>
  <li>Convergence in distribution</li>
  <li>Relationships among various convergences</li>
  <li>Slutsky’s Theorem</li>
</ul>

<p><strong>3.2. Central Limit Theorem</strong></p>

<ul>
  <li>Classical CLT</li>
  <li>Lyapunov CLT</li>
  <li>Multivariate CLT</li>
</ul>

<p><strong>3.3. Delta Methods</strong></p>

<ul>
  <li>First-order delta method</li>
  <li>Second-order delta method</li>
  <li>Multivariate delta method</li>
</ul>

<p><strong>3.4. Some helpful theorems</strong></p>

<ul>
  <li>Dominated Convergence Theorem</li>
  <li>Continuous Mapping Theorem</li>
  <li>Stochastic Order Notation</li>
</ul>

<p> </p>

<hr />

<h2 id="3-large-sample-theory"><strong>3. Large Sample Theory</strong></h2>

<p>In this section, we review some of the fundamental concepts and techniques in <strong>asymptotic statistics</strong>.</p>

<p>Statistical inference often requires the knowledge of the underlying distribution of statistics which however, cannot often be derived in a closed form.</p>

<p>To this end, one of the main goals in asymptotic statistics is to address this issue by observing what happens to the statistic under the assumption of <strong>infinite samples</strong> (i.e. $n \to \infty$). Thus, we want to find out the <strong>limiting distribution</strong> of a given statistic, which is often much easier to handle than the actual distribution while precise enough to be useful in practical scenarios.</p>

<p> </p>

<hr />

<h3 id="31-convergence-of-random-variables">3.1 Convergence of Random Variables</h3>

<h4 id="convergence-of-sequences">Convergence of Sequences</h4>

<p>As a building block, let’s briefly review the concept of convergence in deterministic real numbers.</p>

<p>We say that a sequence of real numbers $a_1, a_2, \dots $ converges to a fixed real number $a$, if for every positive number $\epsilon$ there exists a natural number $N(\epsilon)$ such that for all $n \geq N(\epsilon), \quad|a_n - a| &lt; \epsilon$.</p>

<p>If this is the case, then we call $a$ the limit of the sequence and denote $\underset{n \to \infty}{\text{lim} a_n = a}$.</p>

<p>Now we focus on how to extend of this notion to the “sequences” of <em>random variables</em>. Specifically, we will see the following convergences:</p>

<ul>
  <li><strong>almost sure convergence</strong></li>
  <li><strong>convergence in probability</strong></li>
  <li><strong>convergence in quadratic mean</strong></li>
  <li><strong>convergence in distribution</strong></li>
</ul>

<p> </p>

<h4 id="almost-sure-convergence">Almost-sure convergence</h4>

<p><strong>&lt;Definition&gt;</strong></p>

<center>

$$
\begin{aligned}
&amp;P\Big( \big\{\omega \in \Omega: \underset{n \to \infty}{\text{lim}}X_n(\omega) = X(\omega) \big\}\Big) =  1 \\[10pt]
&amp;\Leftrightarrow \big| X_n(\omega) - X(\omega)\big| \leq \epsilon \quad \text{holds for every } \epsilon &gt; 0
\end{aligned}
$$

</center>

<p>Conceptually, this can be thought of as if there is some set of “exceptional” events on which the random variables can disagree, but these exceptional events have probability converging to 0 as $n \to \infty$.</p>

<p> </p>

<h4 id="convergence-in-probability">Convergence in probability</h4>

<p>A sequence of random variables $X_1, \dots, X_n$ converges in probability to a random variable $X$ if for every $\epsilon &gt; 0$, we have that</p>

<center>

$$
\begin{aligned}
&amp;\underset{n \to \infty}{\text{lim}} P\big(|X_n - X| \geq \epsilon \big) \leq \epsilon, \quad \forall\epsilon&gt;0 \\[8pt] 
&amp;\Leftrightarrow \underset{n \to \infty}{\text{lim}} P\big(|X_n - X| \geq \epsilon \big) = 0 \\[8pt]
&amp;\Leftrightarrow X_n \overset{p}{\to} X
\end{aligned}
$$

</center>

<p>Conceptually, <em>convergence in probability</em> implies that as $n$ gets large, the distribution of $X_n$ gets more peaked around the value of convergence. Hence the random variable converges in a “probabilistic” sense.</p>

<p>A famous example of <em>convergence in probability</em> is the <strong>weak law of large numbers</strong>. Suppose that $Y_1,Y_2,\dots$ are i.i.d. with $E[Y]=\mu$ and $Var(Y_i) = \sigma^2 &lt; \infty$, then</p>

<center>

$$
X_n = \frac{1}{n}\sum_{i=1}^nY_i \overset{p}{\rightarrow} \mu
$$

</center>

<p> </p>

<h4 id="convergence-in-quadratic-mean">Convergence in quadratic mean</h4>

<p><strong>&lt;Definition&gt;</strong></p>

<p>We say that a sequence converges to $X$ in quadratic mean if:</p>

<center>

$$
\begin{aligned}
&amp;E\big(X_n-X\big)^2 \overset{p}{\to} 0 \quad\text{ as } n\to\infty \\[8pt]
&amp;\Leftrightarrow X_n \overset{qm}{\to} 0
\end{aligned}
$$

</center>

<p>This convergence is often used to prove <em>convergence in probability</em> as it is a stronger condition.</p>

<p> </p>

<h4 id="convergence-in-distribution">Convergence in distribution</h4>

<p><strong>&lt;Definition&gt;</strong></p>

<p>We say that a sequence converges to $X$ in distribution if:</p>

<center>

$$
\underset{n \to \infty}{\text{lim}} F_{X_n}(t) = F_X(t), \quad \text{for all points }t \text{ where the }F_X \text{ is continuous}
$$

</center>

<p>This convergence is by far, the weakest form of convergence.</p>

<p>A fundamental example of <em>convergence in distribution</em> is the <strong>Central Limit Theorem (CLT)</strong> and we will see this in a second.</p>

<p> </p>

<h4 id="relationships-among-various-convergences">Relationships among various convergences</h4>

<ol>
  <li>
    <p><strong><em>Convergence in probability</em></strong> does <strong>not</strong> imply <strong><em>almost sure convergence</em></strong>.</p>
  </li>
  <li>
    <p><strong><em>Convergence in quadratic mean</em></strong> imples <strong><em>convergence in probability</em></strong> (reverse is not true).</p>
  </li>
</ol>

<ul>
  <li>Suppose that $X_1, \dots, X_n$  converges in quadratic mean to $X$. Then,</li>
</ul>

<center>

$$
P\big(|X_n - X| \geq \epsilon\big) = P\big(|X_n-X|^2 \geq \epsilon^2\big) \leq \frac{E[X_n-X]^2}{\epsilon^2} \to 0 \quad(\because \text{ Markov's inequality})
$$

</center>

<p>3.<strong><em>Convergence in probability</em></strong> imples <strong><em>convergence in distribution</em></strong> (reverse is not true).</p>

<ul>
  <li>Although the mathematical detail is omitted here, this statement can be proved with the idea of “trapping” the CDF of $X_n$ by the CDF of $X$ with an interval of length converging to 0.</li>
</ul>

<p> </p>

<h4 id="slutskys-theorem">Slutsky’s Theorem</h4>

<p><strong>&lt;Definition&gt;</strong></p>

<center>

$$
\begin{aligned}
&amp;\text{If }Y_n \overset{p}{\to} c \text{ and } X_n \overset{d}{\to} X, \quad (c\text{ is a constant)} \\[10pt]
&amp;\Rightarrow X_n + Y_n \overset{d}{\to} X + c \\[5pt]
&amp;\Rightarrow X_nY_n \overset{d}{\to} cX
\end{aligned}
$$

</center>

<ul>
  <li>Note that in general, $X_n  \overset{d}{\to} X$ and $Y_n  \overset{d}{\to} Y$ does not guarentee that the sum and product also converge.</li>
</ul>

<p> </p>

<hr />

<h3 id="32-central-limit-theorem">3.2 Central Limit Theorem</h3>

<p>The central limit theorm is one of the most famous and important examples of <em>convergence in distribution</em>.</p>

<p> </p>

<h4 id="classical-clt">Classical CLT</h4>

<p><strong>&lt;Definition&gt;</strong></p>

<p>Let $X_1, \dots, X_n$ be a independent sequence of random variables from an <strong>arbitrary probability distribution</strong> with finite mean and variance $\mu, \sigma^2$. Then for the sample mean $\bar X = \sum_{i=1}^n X_i / n$,</p>

<center>

$$
\begin{aligned}
&amp;Z_n = \frac{\sqrt{n}(\bar X_n - \mu)}{\sigma} \overset{d}{\to} Z = N(0,1) \\[10pt]
&amp;\Leftrightarrow\bar X_n \overset{d}{\approx} N\Big(\mu, \frac{\sigma^2}{n}\Big)
\end{aligned}
$$

</center>

<ul>
  <li>The variance $\sigma^2$ can be estimated by the sample variance $\hat \sigma^2$ and the CLT still holds (CLT with estimated variance).</li>
</ul>

<center>

$$
\frac{\sqrt{n}(\hat X_n - \mu)}{\hat \sigma_n} \overset{d}{\to} N(0,1)
$$

</center>

<ul>
  <li>
    <p>Note that the CLT is <strong>incredibly general</strong>, in that it can be applied to any kind of random variables.</p>
  </li>
  <li>
    <p>Consider that we want to perform a statistical test for the <strong>unknown mean</strong> $\mu$. In order to find the <strong>confidence interval</strong>, we have to know the underlying distribution of test statistic $T$. However, since CLT guarentees normal approximation of the sample mean (or equivalent summation), the confidence interval is asymptotically defined such that:</p>
  </li>
</ul>

<center>

$$
\begin{aligned}
&amp;P(\mu \in C) \geq 1-\alpha, \quad \big(C=[\hat \mu-t, \hat \mu +t] \text{ is the critical region, } \alpha\in(0,1)\big) \\[8pt]
&amp;\Rightarrow  P\big(|\hat\mu - \mu| \leq t\big) = P\Big(\frac{\sqrt{n}|\hat\mu - \mu|}{\sigma} \leq \frac{\sqrt{n}t}{\sigma} \Big) = P\big(|Z| \leq \frac{\sqrt{n}t}{\sigma} \big), \quad \big(Z\sim N(0,1)\big) \\[8pt]
&amp;\Rightarrow  \text{under the CDF of standard normal, } \frac{\sqrt{n}t}{\sigma} = z_{\alpha/2} \Leftrightarrow t = \frac{\sigma z_{\alpha/2}}{\sqrt{n}} \\[8pt]
&amp;\therefore C = \Big[\hat\mu - \frac{\sigma z_{\alpha/2}}{\sqrt{n}}, \; \hat\mu + \frac{\sigma z_{\alpha/2}}{\sqrt{n}} \Big] \quad\Rightarrow\quad P(\mu \in C) \to 1-\alpha
\end{aligned}
$$

</center>

<p> </p>

<p><strong>&lt;Proof&gt;</strong></p>

<p>The CLT can be proved by using the property of mgfs. Specifically, we are going to use the following facts:</p>

<center>

$$
\begin{aligned}
&amp;1.\quad \text{If }X \text{ and }Y \text{ are independent with mgfs }M_X, X_Y, \text{ then } Z = X + Y \; \text{ has mgf }\; M_Z(t) = M_X(t)M_Y(t) \\[8pt]
&amp;2.\quad \text{If }X \text{ has mgf } M_X,\text{ then }\;Y= a + bX \;\text{ has mgf } M_Y(t) = exp(at)M_X(bt) \\[8pt]
&amp;3.\quad \text{The derivative of the mgf at 0 returns the moments,} \;\text{ i.e. }\;M_X^{(r)}(0) = E[X^r] \\[8pt]
&amp;4.\quad \text{Let }X_1, \dots, X_n \text{ be a sequence of random variables with mgfs } M_{X_1}, \dots, M_{X_n}. \\
&amp;\quad\;\; \text{ If for all }t \text{ in an open interval around 0, } M_{X_n}(t) \to M_X(t) \text{ implies } X_n \overset{d}{\to} X \\[8pt] 
&amp;5.\quad \text{If }Z \sim N(0,1), \text{ then }M_Z(t) = e^{t^2/2} \\[15pt]
\end{aligned}
$$

$$
\begin{aligned}
&amp;Z_n = \frac{\sqrt{n}(\bar X_n - \mu)}{\sigma} = \frac{1}{\sqrt{n}}\sum_{i=1}^nA_i, \quad (A_i = \frac{X_i- \mu}{\sigma}) \\[10pt]
&amp;\Rightarrow M_{Z_n}(t) = E\Big[exp(tZ_n)\Big] = E\Big[exp(\frac{t}{\sqrt{n}}\sum_{i=1}^nA_i)\Big] = \prod_{i=1}^n E\Big[exp\big(\frac{t}{\sqrt{n}}A_i\big)\Big] = M_A\Big(\frac{t}{\sqrt{n}}\Big)^n\\[10pt]
&amp;\Rightarrow M_A\Big(\frac{t}{\sqrt{n}}\Big) \approx M_A(0) + \frac{t}{\sqrt{n}}M_A^\prime(0) + \frac{t^2}{2n}M_A^{\prime\prime}(0) = 1 + \frac{t^2}{2n} \quad (\because\text{taylor expansion of }M_A) \\[10pt]
&amp;\Leftrightarrow M_A\Big(\frac{t}{\sqrt{n}}\Big)^n \approx \Big(1 + \frac{t^2}{2n}\Big)^n \to e^{t^2/2} = M_Z(t) \quad \text{ as }n \to \infty \quad(\because \underset{n \to \infty}{\text{lim}} \Big(1 + \frac{x}{n}\Big)^n \to e^x)
\end{aligned}
$$

</center>

<p>Thus, we have shown that the mgf converges to that of the standard normal, which completes the proof.</p>

<p> </p>

<h4 id="lyapunov-clt">Lyapunov CLT</h4>

<p>Suppose that $X_1, X_2, \dots$ are independent, but <strong>not necessarily identically distributed.</strong></p>

<p>The CLT for the sum holds as desired, but it requires some extra conditions to ensure that one or a small number of abnormal random variables do not dominate the sum.</p>

<p>In this sense, the <em>Lyapunov Central Limit Theorem</em> guarentees normal approximation for this exceptional cases.</p>

<p><strong>&lt;Definition&gt;</strong></p>

<p>Suppose $X_1, \dots, X_n$ are independent. Let:</p>

<center>

$$
\begin{aligned}
\mu_i &amp;= \mathbb{E}\big[X_i\big] \\[5pt]
\sigma^2_i &amp;= Var(X_i) \\[5pt]
s_n^2 &amp;= \sum_{i=1}^n \sigma^2
\end{aligned}
$$

</center>

<p>Suppose that the following <em>Lyapunov condition</em> is satisfied:</p>

<center>

$$
\underset{n \to \infty}{\text{lim}} \frac{1}{s_n^{2+\delta}}\sum_{i=1}^n \mathbb{E}\big[|X_i-\mu|^{2+\delta}\big] = 0, \quad \text{for some } \delta &gt;0
$$

</center>

<p>Then, it holds that:</p>

<center>

$$
\frac{1}{s_n}\sum_{i=1}^n \big(X_i - \mu_i\big) \overset{d}{\to} N(0,1)
$$

</center>

<ul>
  <li>Intuitively, what can happen for the non-identically distributed random variables is that only one random variable can possibly dominate the sum, so we are not really averaging diverse information among random variables. To this end, the <em>Lyapunov condition</em> is added to prevent such extreme cases.</li>
</ul>

<p>&lt;/center&gt;</p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET