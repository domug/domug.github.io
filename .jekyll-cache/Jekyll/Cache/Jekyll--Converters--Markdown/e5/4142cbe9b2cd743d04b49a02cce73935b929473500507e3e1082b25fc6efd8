I"È<h3 id="previous-posts">Previous Posts</h3>

<p><a href="https://domug.github.io/2020/12/03/Estimation_Multicollinearity/">1. Statistical Estimation and Multicollinearity</a></p>

<p><a href="https://domug.github.io/2020/12/03/Ridge_Lasso/">2. Variable Selection Methods</a></p>

<hr />

<h1 id="statistical-dimension-reduction">Statistical Dimension Reduction</h1>

<p>Now it‚Äôs finally time to move on to the methods of dimension reduction! In this post I will introduce the basic concept of what dimension reduction is and the most renowned method - <strong>Principal Component Analysis (PCA)</strong>. Throughout the following posts on this category, we will apply various dimension reduction methods on a car dataset which consists of 74 different car types with 13 variables (features) measured for each.</p>

<p>From previous posts, I emphasized that when we are looking at a dataset we have, we don‚Äôt necessarily need all of the variables because just a subset of them might have sufficient amount of information. This was related to the issue of multicollinearity and as a one possible remedy for this problem, we‚Äôve looked at variable selection methods including ridge and lasso regression.</p>

<p>However, from now on we will use dimension reduction methods to extract new insight from the original data. As subcategory of dimension reduction, we will be talking about ‚Äúprincipal component analysis‚Äù, ‚Äúfactor analysis‚Äù, ‚Äúcluster analysis‚Äù and ‚Äúdiscriminant analysis‚Äù. Although these methods are grouped together as dimension reduction methods, how each of them works and what they allows us to do are all different.</p>

<p>to construct whole new variables from the originals. In other words, we will re-express and describe the original data in terms of the newly defined uncorrelated variables, each of which is derived by linear combinations of the original variables. So dimension reduction is reducing the dimensionality of original data by</p>

<p align="center">
	<img width="600" height="500" src="/images/pca/pca_illustration.jpg" />
</p>

<hr />

<h1 id="ridge-regression">Ridge Regression</h1>

<p>If you are familiar with linear regressions, you will probably remember the <strong>normal equation</strong>, which is used to derive the regression coefficients(LSE) of linear regression models. Here is the formula.</p>

<p><img src="/images/Variable_Selection/normal_equation.png" alt="" /></p>

<p>Note that the regression coefficient beta can only be derived when the design matrix X is of <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#:~:text=A%20matrix%20is%20said%20to,does%20not%20have%20full%20rank.">full rank</a>. When multicollinearity exists between the columns of a matrix, then that matrix is not full rank, which implies that we can‚Äôt derive the unique Least Squares Estimate of beta.</p>

<p>Ridge regression handles this problem by imposing a <strong>penalty</strong> on the size of the matrix to arbitrarily make normal equation solvable. Here‚Äôs the idea.</p>

<p><img src="/images/Variable_Selection/ridge.png" alt="" /></p>

<p>By adding Œª to the left side of the normal equation, we can make the matrix as full rank (or equivalently invertible). Here, Œª is called as <strong>complexity parameter</strong> and it controls the amount of <strong>‚Äúshrinkage‚Äù</strong>, meaning that the regression coefficients of irrelevant variables converges to 0. Important point here is that in ridge regression, the coefficients just infinitely converges to 0 and doesn‚Äôt become exactly 0. Thus the larger the value of Œª, the greater the amount of shrinkage. We can also see that the ridge regression coefficient can be derived precisely in a closed form because it is a linear combination of matrix X and Y.</p>

<p>Another importance point is that, the normal equation for ridge regression can be expressed explicitly in terms of the constraint as the last part of the equation above. As can be seen, the constraint for ridge regression is the <strong>square</strong> of the norm of Œ≤, so this panelty is often called as <strong>L2 panelty</strong>. Also, as ridge regression sort of manages, or ‚Äúregularizes‚Äù Œ≤ to control variance, another name for ridge regression is <strong>L2 Regularization</strong>.</p>

<p align="center">
	<img src="/images/Variable_Selection/ridge_ols.png" />
	<img width="600" height="500" src="/images/Variable_Selection/ridge2.png" />
</p>

<p>The graph above is a comparison between OLS (Ordinary Least Squares) coefficients and the ridge coefficients based on optimal Œª suggested by R. Each regression was fitted on a randomly created data from the multivariate normal distribution. The y-axis represents ordinary regression coefficients and the x-axis is ridge coefficients. You can clearly see that most of the ridge coefficients have converged near to 0, while in the ordinary linear regression the coefficients are all over the place.</p>

<hr />

<h1 id="lasso-regression">Lasso Regression</h1>

<p>Now let‚Äôs take a took at another famous shrinkage method as known as lasso regression. The general idea is similar to the ridge regression, while the only crucial difference being that lasso uses <strong>L1 panelty</strong>. In fact, lasso is an abbreviation for <strong>‚ÄúLeast Absolute Shrinkage and Selection Operator‚Äù</strong>. As it is explicitly mentioned in its name, lasso uses absolute panelty and although this might seem trivial, it creates a lot of difference and complexity compared to the ridge method.</p>

<p align="center">
  <img src="/images/Variable_Selection/ridge_lasso.png" />
</p>

<p>Because of the absolute panelty, we cannot derive the estimated coefficients of Œ≤ in a closed form since the normal equation is non-linear in X and Y. Therefore we need to use numerical optimization methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton Rhapson Method</a>. Conveniently, R or Python automatically does all of the calculation for us so let‚Äôs focus more on its implication.</p>

<p align="center">
  <img src="/images/Variable_Selection/lasso_ols.png" />
</p>

<p>By fitting lasso regression to the same data used in ridge regression, we can clearly see that some of the coefficients have disappeared completely. This is because unlike ridge regression where we used square panelty, the absolute panelty of lasso regression makes the regression coefficients of ‚Äúinsignificant‚Äù variables exactly 0. If we visualize this result we get the following graph.</p>

<p align="center">
  <img width="600" height="500" src="/images/Variable_Selection/lasso.png" />
</p>

<p>The x-axis corresponds to the log scaled values of Œª, which is the shrinkage parameter that we discussed earlier and y-axis is the corresponding lasso coefficients. The vertical red line is the best Œª suggested by R. We can visually examine that for the best Œª, most of the coefficients became 0. Hence, we can conclude that those variables with 0 coefficients are insignificant in terms of our linear regression model.</p>

<p>We have just conducted a variable selection! Keep in mind that the purpose of variable selection is to define only a subset of important variables which will be used in further analysis. Thus variable selection itself is not the ultimate goal, but rather a first step in a complete statistical analysis procedure.</p>

<p>Although lasso and ridge methods can be used to reduce multicollinearity, variable selection methods have inherent limitations as they merely tell us which variables to be used or not. In other words, these methods are subordinated to the original data - they don‚Äôt provide new insight or new viewpoint of the data. It‚Äôll probably not be clear for now, but it will soon make sense after we talk about <strong>dimension reduction methods</strong> in the following posts, so let‚Äôs call it a day for now.</p>

:ET