I"“<h3 id="previous-posts">Previous Posts</h3>

<p><a href="https://domug.github.io/2020/12/03/Estimation_Multicollinearity/">1. Statistical Estimation and Multicollinearity</a></p>

<p><a href="https://domug.github.io/2020/12/04/Ridge_Lasso/">2. Variable Selection Methods</a></p>

<p><a href="https://domug.github.io/2020/12/05/PCA/">3. Principal Component Analysis</a></p>

<p><a href="https://domug.github.io/2020/12/06/FA/">4. Factor Analysis</a></p>

<p><a href="https://domug.github.io/2020/12/06/CA/">5. Cluster Analysis</a></p>

<hr />

<h1 id="discriminant-analysis-da">Discriminant Analysis (DA)</h1>

<p>As a final topic of this series, we will talk about <strong>discriminant analysis</strong> and what it exactly does.</p>

<p>By now, you might be puzzled by different statistical dimension reduction methods which seem to be doing similar stuffs. When I first studied this topic, all these methods were tangled inside my head and I couldn‚Äôt figure out their differences. So before going into the details on the discriminant analysis, I want to spend some time to figure out the subtle differences in the methods we‚Äôve studied so far.</p>

<p>So far we‚Äôve looked at PCA, FA and CA. Let‚Äôs focus on the main goal of each. First of all, CA is evidently straightforward in that it tries to make clusters.</p>

<p>Since the goal of CA is evidently straightforward in that it tries to make clusters, let‚Äôs focus more on the differences between PCA, FA and DA. To make a long story short, Discriminant Analysis and the two previous methods, PCA and FA, are very similar by their nature. All three methods try to find the <strong>linear combination</strong> of the original variables to re-express the data. Nevertheless even if they share the same concept, their main focus on the implementation are all different.</p>

<p>For instance, if we can borrow the term from machine learning, PCA and FA are <strong>‚Äúunsupervised models‚Äù</strong> while DA is a <strong>‚Äúsupervised model‚Äù</strong>. That is, unlike PCA and FA where we don‚Äôt need any output variables, Discriminant Analysis requires output classes to be supplied. This is because DA focuses on <strong>maximizing the separability between each classes</strong>. On the other hand, PCA merely aims to re-express the data in terms of components which maximize the variance in the data, and FA tries to reveal the latent variables by focusing on the shared variance between the original variables.</p>

<p>To put it simply, we can summarize the differences as follows:</p>

<p align="center">
	<img width="700" height="500" src="/images/lda/lda_table.png" />
</p>

<p>Hope this makes you a little less confused on the distinction between each methods.</p>

<p>With this in mind, it‚Äôs time to really talk about discriminant analysis. In fact, I‚Äôve already demonstrated some of the details. The keyword of discriminant analysis is <strong>‚Äúseparation‚Äù</strong>, so we want to find some decent rule that can separate and classify the observations according to their classes. In this post we will focus on <strong>‚ÄúLinear Discriminant Analysis (LDA)‚Äù</strong> in which we use linear function to separate observations. However, keep in mind that as LDA tries to separate the observations with lines, if we have variables that are complicatedly intertwined, we have to search for alternative rule with is beyond simple lines and this is called ‚ÄúQuadratic Discriminant Analysis‚Äù.</p>

<p>An example of disciminant analysis is the fraud detection</p>

:ET