I"ª'<p>The following series of posts is an overall summary of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p>¬†</p>

<hr />

<h2 id="ch-1-foundations-of-statistics"><strong>Ch 1. Foundations of Statistics</strong></h2>

<p>¬†</p>

<h3 id="11-sigma-algebra">1.1 Sigma Algebra</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>A collection of subsets of sample space <em>S</em> is called a <strong><em>sigma algebra</em></strong> denoted by <strong><em>B</em></strong>, if it satisfies the following three properties:</p>

<center>

$$
\begin{aligned}
&amp;1.\quad \emptyset \in B \quad(\text{ the empty set is an element of }B) \\[5pt]
&amp;2.\quad \text{If } A \in B, \text{ then }A^c\in B \quad(B\text{ is closed under complementation}) \\[5pt]
&amp;3. \quad \text{If }A_1,A_2,\dots \in B, \text{ then } \cup_{i=1}^\infty A_i \in B \quad(B\text{ is closed under countable unions}) \\[20pt]
\end{aligned}
$$

</center>

<p>¬†</p>

<p><strong>Q. Reason why we want to care about <em>sigma algebras</em>?</strong></p>

<ul>
  <li>when we deal with a large sample space like real line, there are some <strong>pathological sets</strong> that <strong>break down the probability theory</strong>.</li>
  <li>To avoid such crazy sets (i.e. <strong>non-measurable sets</strong>), we restrict our attention to smaller but nicer subsets (i.e. <strong>measurable sets</strong>) for which the probability measure is well-defined and satisfies the <strong><em>Kolmogorov axioms</em> of probability</strong>.</li>
</ul>

<center>

$$
\begin{aligned}
&amp;1.\quad P(A) \geq 0 \text{ for all } A \in B \\[8pt]
&amp;2.\quad P(S) = 1 \\
&amp;3. \quad \text{If } A_1,A_2,\dots \in B \text{ are pairwise disjoint, then } P\big(\cup_{i=1}^\infty A_i \big) = \sum_{i=1}^\infty P(A_i)
\end{aligned}
$$

</center>
<p>¬†</p>

<h3 id="12-expected-values">1.2 Expected Values</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>The expected value or mean of a random variable $g(X)$, denoted as $E[g(X)]$ is defined as:</p>

<center>

$$
\begin{equation}
  E[g(X)]=\left\{
  \begin{array}{@{}ll@{}}
    \int_{-\infty}^{\infty} g(x)f_X(x)dx , &amp; \text{if } X \text{ is continuous} \\[10pt]
    \sum_{x\in \chi} g(x)f_X(x) , &amp; \text{if } X \text{ is discrete}
  \end{array}\right.
\end{equation}
$$

</center>

<p>provided that the integral or sum exists.</p>

<p>¬†</p>

<p><strong>&lt;Relationship to tail probability&gt;</strong></p>

<p>Let X be a ‚Äúnonnegative‚Äù random variable. Then in holds that:</p>

<center>

$$
E[X] = \int_{0}^{\infty} \big( 1 - F_X(x) \big)df = \int_{0}^{\infty}P(X &gt; x)dx 
$$

$$
\begin{aligned}
&amp;\Rightarrow x = \int_0^{x} 1dt = \int_0^{\infty} \mathbb{1}(t &lt; x)dt \\[10pt]
&amp;\Leftrightarrow E[X] = E\Big[\int_0^{\infty} \mathbb{1}(t &lt; x)dt\Big] = \int_0^{\infty} P(X &gt; t)dt \quad (\because \text{Tonelli's theorm})
\end{aligned}
$$

</center>

<p>A slight extension of this property to any random variable is that:</p>

<center>

$$
E[X] = \int_0^\infty P(X &gt; x)dx - \int_{-\infty}^0 P(X &lt; x)dx
$$

$$
\begin{aligned}
&amp;\text{Since } x\mathbb{1}(X\geq 0) = \int_{0}^x\mathbb{1}(X \geq 0)dt = \int_{0}^\infty \mathbb{1}(X \geq 0) \mathbb{1}(X &gt; t)dt = \int_0^\infty \mathbb{1}(X &gt; t)dt, \\[5pt]
&amp;\text{and }x = x\mathbb{1}(X \geq 0) +  x\mathbb{1}(X &lt; 0) =  x\mathbb{1}(X \geq 0) -  -(x)\mathbb{1}(-X &gt; 0),
\end{aligned}
$$

$$
\begin{aligned}
\Rightarrow &amp;E[ \mathbb{1}(X\geq 0)] = E\big[\int_0^\infty \mathbb{1}(X&gt;t)dt \big] = \int_0^\infty P(X&gt;t)dt \\[7pt]
&amp; E[-x\mathbb{1}(-X \geq 0)] = E\big[\int_0^\infty \mathbb{1}(-X &gt; t)dt\big] = \int_0^\infty P(X &lt; -t)dt = \int_{-\infty}^0P(X &lt; t)dt
\end{aligned}
$$

$$
\therefore E[X] = \int_0^\infty P(X &gt; x)dx - \int_{-\infty}^0 P(X &lt; x)dx
$$

</center>

<p>¬†</p>

<p><strong>&lt;Variance&gt;</strong></p>

<p>A useful alternative expression of variance is that for any arbitrary i.i.d. copy of random variable $X$ denoted by $X^\prime$, the variance is:</p>

<center>

$$
Var(X) = \frac{1}{2} E\big[ (X-X^\prime)^2\big]
$$

</center>

<p>From this, for a bounded random variable $x \in [a, b]$ the variance is upper bounded by:</p>

<center>

$$
Var(X) \leq \frac{(b-a)^2}{4}
$$

</center>

<p>This boundary cannot be improved in general.</p>

<p>¬†</p>

<h3 id="13-moment-generating-functions">1.3 Moment Generating Functions</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<center>

For each integer $n$, the n-th central moment of a random variable $X$ is defined as:
$$
\mu_n = E\big[(X-E[X])^n\big]
$$
</center>

<p>Then, the moment generating function (a.k.a. mgf) of a random variable $X$ is:</p>

<center>

$$
M_X(t) = E\big[ e^{tX} \big], \quad \;\forall t \in (-\delta, \delta), \;\delta &gt;0
$$

</center>

<p>¬†</p>

<p><strong>Q. Why is mgf useful?</strong></p>

<ul>
  <li>The mgf of $X$ gives us all moments of $X$.</li>
  <li>If the mgf exists, it uniquely determines the distribution of $X$.</li>
</ul>

<center>

$$
M_X(t) = M_Y(t) \;\Leftrightarrow\; X \overset{d}{\approx} Y
$$

</center>

<ul>
  <li>Convergence in mgf implies convergence in distribution. This property is used to prove the Central Limit Theorem.</li>
</ul>

<center>

$$
\underset{n \to \infty}{\text{lim}}M_{X_n}(t) = M_X(t) \;\Leftrightarrow\; \underset{n \to \infty}{\text{lim}}F_{X_n}(t) = F_X(t)
$$

</center>

<ul>
  <li>mgf is also useful to obtain a probability tail bound such as the <em>Hoeffding‚Äôs inequality</em>.</li>
</ul>

<p>Note that <strong>mgf doesn‚Äôt necessarily exists for all random variables</strong> (e.g. <em>Cauchy random variable</em>)</p>

<center>

$$
\begin{aligned}
&amp;\text{For Cauchy Random Variable }X, \\[5pt]
&amp;f(x) = \frac{1}{\pi}\frac{1}{x^2+1}, \quad (-\infty &lt; x &lt; \infty) \\[5pt]
\end{aligned}
$$

$$
\begin{aligned}
\Rightarrow \int_{-\infty}^\infty e^{tx}\frac{1}{\pi}\frac{1}{x^2+1}dx &amp;\geq \int_{0}^\infty e^{tx}\frac{1}{\pi}\frac{1}{x^2+1}dx \\[8pt]
&amp;\geq \int_{0}^\infty \frac{1}{\pi}\frac{tx}{x^2+1}dx \\[8pt]
&amp;= \underset{a \to \infty}{\text{lim}}\Big[\frac{t}{2\pi}log(a^2 + 1) \Big] = \infty
\end{aligned}
$$

</center>

<p>¬†</p>

<h3 id="14-characteristic-functions">1.4 Characteristic Functions</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>The characteristic function of a random variable $X$ is defined as:</p>

<center>

$$
\phi_X(t) = E\big[ exp(itX) \big] = E\big[cos(tX) + isin(tX)\big], \quad t\in\mathbb{R}
$$

</center>

<ul>
  <li>Characteristic function serves similar purposes with the moment generating function, but it exists for any kind of random variable.</li>
</ul>

<p>¬†</p>

<p><strong>&lt;Properties&gt;</strong></p>

<center>

$$
\begin{aligned}
&amp;1.\quad \phi_X(0) = 1 \;\text{ and }\; |\phi_X(t)| \leq 0. \\[7pt]
&amp;2.\quad \phi_X(t) \text{ is uniformly continuous } \big(\text{i.e. exists }\psi \text{ such that }  |\phi_X(t+n) - \phi_X(t)| \leq \psi(h)\big). \\[5pt]
&amp;3.\quad \text{If } X \overset{d}{=} -X \text{ (i.e. symmetric)}, \phi_X(t) \text{ is real-valued}. \\[5pt]
&amp;4.\quad X \overset{d}{=} Y \text{ if and only if } \phi_X(t) = \phi_Y(t).

\end{aligned}
$$

</center>

<p>¬†</p>

<h3 id="15-statistical-independence">1.5 Statistical Independence</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>Two random variables $X$ and $Y$ are independent if and only if :</p>

<center>

$$
P(X\in A, \;Y\in B) = P(X \in A)\;P(Y \in B)
$$

$$
\Leftrightarrow f_{XY}(x,y) = f_X(x)f_Y(y)
$$

</center>

<ul>
  <li>Note that function of independent random variables are also independent:</li>
</ul>

<center>

$$
\begin{aligned}
P\big(g(X) \in A, \;h(Y)\in B \big) &amp;= P\big(X \in g^{-1}(A), \; Y\in h^{-1}(B) \big) \\[5pt]
&amp;= P\big(X \in g^{-1}(A)\big) \;P\big( Y\in h^{-1}(B) \big) \\[5pt]
&amp;= P\big(g(X) \in A\big) \; P\big(h(Y)\in B \big)
\end{aligned}
$$

</center>

<ul>
  <li>If $X_1, \dots, X_n$ are independent,</li>
</ul>

<center>

$$
\begin{aligned}
&amp;\Rightarrow E\Big[\prod_{i=1}^n X_i \Big] = \prod_{i=1}^n E\big[ X_i\big] \\[5pt]
&amp;\Rightarrow Var\Big( \sum_{i=1}^n a_iX_i \Big) = \sum_{i=1}^na_i^2Var(X_i)
\end{aligned}
$$

</center>

<p>¬†</p>

<h3 id="16-covariance-and-correlation">1.6 Covariance and Correlation</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>The covariance of two random variables $X$ and $Y$ is:
\(\text{Cov}(X, Y) = E\big[(X-E[X])(Y-E[Y])\big] = E[XY] - E[X]E[Y]\)
The correlation of random variables $X$ and $Y$ is:
\(\text{Corr}(X, Y) = \rho_{XY} = \frac{\text{Cov}(X,Y)}{\sqrt{Var(X)Var(Y)}}, \quad (-1\leq \rho_{XY} \leq 1)\)</p>

<ul>
  <li>Note that covariance is scale-<strong>variant</strong>, while correlation is scale-<strong>invariant</strong></li>
</ul>

<p>[\text{Cov}(X, Y) \neq \text{Cov}(aX, bY)]</p>

<p>[\text{Corr}(aX,aY) = \frac{\text{Cov}(aX, bY)}{\sqrt{Var(aX)Var(bY)}} = \frac{ab\;\text{Cov}(X,Y)}{\sqrt{a^2b^2Var(X)Var(Y)}} = \text{Corr}(X,Y)]</p>

<h3 id="17-conditional-distributions">1.7 Conditional Distributions</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>Let $(X, Y)$ be a continuous bivariate random vector with joint pdf $f_{XY}$ and marginal pdfs $f_X$ and $f_Y$. For any $x$ such that $f_X(x)&gt;0$, the conditional pdf of $Y$ given that $X=x$ is the function of $y$ denoted by $f(y|x)$ defined as:
\(f(y|x) = \frac{f_{XY}(x,y)}{f_X(x)}\)</p>

<p><strong>&lt;Law of total expectation&gt;</strong></p>

<p>For any two random variables $X$ and $Y$, provided that expectations exist,
\(E[X] = E[E[X|Y]]\)
<strong>&lt;Law of total variance&gt;</strong></p>

<p>For any two random variables $X$ and $Y$,
\(Var(X) = E[Var(X|Y) + Var(E[X|Y])]\)
<strong>&lt;Law of total covariance&gt;</strong></p>

<p>For any three random variables $X, Y, Z$, it holds that:
\(\text{Cov}(X, Y) = E\big[ \text{Cov}(X,Y|Z)\big] + \text{Cov}\big(E[X|Z], E[Y|Z]\big)\)</p>

<p>¬†</p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET