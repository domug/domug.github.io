I"¨!<h3 id="previous-posts">Previous Posts</h3>

<p><a href="https://domug.github.io/2020/12/03/Estimation_Multicollinearity/">1. Statistical Estimation and Multicollinearity</a></p>

<p><a href="https://domug.github.io/2020/12/04/Ridge_Lasso/">2. Variable Selection Methods</a></p>

<p><a href="https://domug.github.io/2020/12/05/PCA/">3. Principal Component Analysis</a></p>

<hr />

<h1 id="factor-analysis-fa">Factor Analysis (FA)</h1>

<p>In line with PCA, Factor Analysis (FA) is another famous dimension reduction method widely used for finding <strong>latent variables</strong>. Latant variable refers to variables that are hidden. Let‚Äôs go back the example about BTS Jin that I illustrated when talking about multicollinearity. I said when there are variables about Jin‚Äôs appearance like ‚Äúwidth of eye‚Äù, ‚Äúlength of eye‚Äù, ‚Äúcolor of eye‚Äù, these variables can be compressed into a single variable - ‚Äúeye‚Äù, which is a latent variable hidden behind those three variables.</p>

<p>Often the goal of factor analysis is to summarize the original dataset which has a lot of varibles by extracting some hidden commonalities in the variables. These commonalities are called as <strong>‚Äúfactors‚Äù</strong> and they are used throughout subsequent statiscal analysis. So it is easy to think the factors as some groups of original variables. In mathematical sense, it is to reduce a p-dimensional random vector X into a fewer ‚Äúk‚Äù latent variables.</p>

<p>Usually it is most common to use continuous &amp; quantitative variables as our example will be so, but keep in mind that sometimes qualitative can also be used. For example, we might have nominal data such as [0, 1] where each number represents gender. In this case we can use factor analysis to find out gender characteristics or so.</p>

<p>Then let‚Äôs dig into the details of factor analysis. You might wonder, ‚Äúthen what is the difference between PCA and FA?‚Äù. This can formally be understood by looking at an equation of factor analysis.</p>

<p align="center">
	<img src="/images/fa/fa_equation.png" />
</p>

<p>The above is the equational definition of orthogonal factor model. We can see that the original data matrix is decomposed into 3 parts. The first part is related to the unknown factors and the matrix Q and F are unknowns which are to be estimated. The second part is the error term, which accounts for individual variations of each variable. The third part is the mean of each variables, but since it‚Äôs a custom to standardize each variable before analysis, this term is often ignored.</p>

<p>Recall that PCA was a linear transformation of X using all of the variables to create new principal components. In factor analysis, however, we decompose the original matrix into predefined factors that are <strong>limited in numbers</strong>. Therefore, since we are only using only a few factors, there‚Äôs inevitably some noise, or error term associated with each variables. This is the main difference of PCA and FA. To put it simply, unlike PCA where we use all the variables to re-express original dataset, FA is like a regression on the original data by a few factors.</p>

<p>Then how are we going to find matrix Q and F? The most commonly used are ‚ÄúPrincipal Component Method‚Äù, ‚ÄúMaximum Likelihood Method‚Äù and ‚ÄúCommon Factor Method‚Äù, and there is no absolute criterion to ensure which one is best. Since R conveniently does that for us, we will not go detail into each methods.</p>

<p>The last theoritical part to look at before going into the implementation is <strong>Factor Loadings</strong>. After performing FA, most often we will get factor loadings matrix that tells us which factor is related to which original variables. The elements of factor loadings matrix are simply the correlations between every factors and variables.</p>

<p>During this process, the technic so called <strong>‚ÄúFactor Rotation‚Äù</strong> is often used to maximize the relationship between factors and original variables. What it does is that it rotates the axes of each factors at the origin until each factor loadings values become close to 0 and 1. We will see how it works in a minute, so don‚Äôt worry even if the idea doesn‚Äôt come right into your head.</p>

<hr />

<h1 id="fa-example">FA example</h1>

<p>We will use the same car data that we used for PCA. I will skip the data preprocessing steps as it will be the same as we did in the PCA, so if you are interested check out the <a href="https://domug.github.io/2020/12/05/PCA/">previous post</a>. We will use <code class="language-plaintext highlighter-rouge">factanal</code> function in R. Here‚Äôs the result.</p>

<p align="center">
	<img width="900" height="700" src="/images/fa/fa.png" />
</p>

<p>Above is factor loadings matrix for a factor analysis using 4 factors with no factor rotation. The number of factors to be used is a hyper parameter that we have to choose for ourselves, and there is no rule for that. Actually after performing factor analysis, if we just execute the variable that contains the result of the factor analysis (which in our case is <code class="language-plaintext highlighter-rouge">fa</code>), then at the bottom some result of statiscal hypothesis tesing will be presented like the following.</p>

<p align="center">
	<img width="700" height="500" src="/images/fa/fa_test.png" />
</p>

<p>Here, the null hypothesis is ‚ÄúThe number of factors that we have used is sufficient‚Äù. So if the p-value is larger than 0.05 (or 0.01 based on your preference) we can ‚Äústatistically‚Äù say that the number of factors we used is decent. However this is just a reference and not to have blind faith on it. In our example, the p-value was a little bit less than 0.05, but as having 5 factors seem too much, I would conclude to use 4 factors for our dataset.</p>

<p>Anyways, back to our factor loadings matrix. It seems like we have a problem. Most of the elements in the loadings matrix have values larger than 0.3. This is problematic because in this situation, we can‚Äôt really distinguish which factor is closely related to which variables. In other words, we can‚Äôt marginalize the characteristics of each factors. To deal with this problem, we are going to use ‚Äúfactor rotation‚Äù. There are various rotation methods, but among them the so called <strong>‚Äúvarimax‚Äù</strong> rotation is mostly widely used so we will stick to that for our example.</p>

<p align="center">
	<img width="900" height="700" src="/images/fa/fa_varimax.png" />
</p>

<p>Now the values of factor loadings became relatively more clear (values are close to 0 or 1) compared to when we didn‚Äôt use rotation. Furthermore, it seems like we can name the factors so let‚Äôs do that. Based on the factor scores in the loadings matrix, I‚Äôve named each factor as the following.</p>

<p align="center">
	<img width="700" height="500" src="/images/fa/fa_table.png" />
</p>

<p>As always, keep in mind that interpreting the results is very subjective. Nevertheless, if we can all agree on the naming of the factors as above, then as a next step, it‚Äôs time to figure out if there is any pattern in the cars associated with each factors. Since we have four factors, it would be difficult to visualize the results in a single graph. Therefore we will sort the car dataset by the scores of each factor and see what it allows us to do. I will only present the results in here, but if you are interested in hard codes, they are on my <a href="https://github.com/domug/Codes-for-blogs/blob/master/Dimension%20Reduction/PCA.R">github</a>.</p>

<hr />

<h3 id="factor-1">Factor 1</h3>

<p align="center">
	<img width="700" height="500" src="/images/fa/result_1.png" />
</p>

<p>When we sort the cars according to the first factor, then the top 10 cars are from the company labeled as ‚Äú1‚Äù, which indicates Amercan companies. On the other hand, we can see that lots of cars are labeled as company ‚Äú3‚Äù in the bottom 10. From this, we can derive the following insight.</p>
<ul>
  <li><strong>In general, American cars are big in size and Europe cars are small in size.</strong></li>
</ul>

<hr />

<h3 id="factor-2">Factor 2</h3>

<p align="center">
	<img width="700" height="500" src="/images/fa/result_2.png" />
</p>

<p>By sorting the cars in terms of the second factor, we can see that most of the cars in the top of the list are labeled as company ‚Äú1‚Äù, while at the bottom relatively many cars are labeled as company ‚Äú2‚Äù. This indicates the following.
-**In general, American cars have larger inner spaces while japanese cars **</p>

<hr />

:ET