I"!<p>대부분의 현실적인 이상치 탐지 시나리오에서 우리는 고차원의 데이터셋을 다루게 된다. 많게는 수백, 수천개의 차원을 갖고 있는 데이터에서 앞서 살펴본 다양한 이상치 탐지 기법들은 제대로 작동하지 않을 우려가 커지는데, 이는 소위 말하는 <strong>차원의 저주 (curse of dimensionality)</strong> 에 의해 데이터가 고차원 공간의 매우 국소적인 부분 공간 (local subspace) 에 밀집해서 분포하기 때문이다. 이를 “data sparsity” 또는 “distance concentration” 이라고 일컫는다. 가령, 이상치 탐지에 흔히 활용되는 LOF 등의 거리 기반 알고리즘을 생각해보자. 해당 방법론들에서 아웃라이어는 다른 데이터들에 비해 상대적으로 “멀리 떨어진” 값들로 정의되지만 고차원 공간에서 이와 같은 거리에 대한 척도는 아웃라이어에 대한 지표로서의 기능을 제대로 수행하지 못하게 된다.</p>

<p>따라서 고차원 공간에서의 성공적인 이상치 탐지를 위해서는 데이터가 실제로 분포하고 있는 <strong>국소적인 부분 공간 (locally-relavant subspace)</strong> 을 잡아내는 것이 필수적이다. 하지만 이는 결코 만만한 작업이 아닌데, 그 이유는 수 많은 차원 중에서 어떤 차원들이 데이터를 잘 설명하는지를 사전에 알 수가 없으며, 모든 차원들에 대한 조합을 일일히 고려하는 것이 불가능하기 때문이다. 가령, 10차원 정도만 되더라도 가능한 차원의 모든 조합은 $2^{10} = 1024$ 로 상당한 컴퓨팅 자원을 소모할 것이다. 이와 더불어 이상치 탐지를 위해서는 정상적인 데이터와 아웃라이어 간의 상대적 위치를 최대한 극대화 또는 보존시키면서 저차원의 공간을 탐색해야 한다는 점에서 주성분 분석 (PCA) 등의 보편적인 차원축소 방법론 역시 쉽게 활용할 수 없게 된다.</p>

<p>해당 내용을 직관적으로 이해하기 위해 다음의 예시를 살펴보자. 이는 (가상의) 고차원 데이터셋에서 랜덤하게 선택된 두개의 축에 데이터를 투영 (projection) 시킨 것이다.</p>

<center>
  <img src="/images/outlier/12.png" width="700" height="500" /> 
 <br />
 <em><span style="color:grey">Figure 1: The outlier behavior is masked by the irrelevant attributes in high dimensions.
</span></em>
</center>

<p> </p>

<p>위 데이터에서 $A$ 와 $B$ 는 실제로 다른 generative process로 부터 생성된 아웃라이어이다. 이와 관련해서 그림 (a) ~ (d) 는 모두 동일하게 2차원의 부분 공간임에도 불구하고 그 시각에 따라서 $A$, $B$ 가 아웃라이어로서 부각이 될 수도, 되지 않을 수도 있다는 점을 보여준다. 가령, (b)와 (c)는 이상치 탐지의 맥락에서 전혀 유용하지 않은 부분 공간이라고 할 수 있다.</p>

<p>이처럼 아웃라이어는 전체 데이터셋의 아주 일부분의 차원에서만 발견할 수 있는 경우가 많다. 예를 들어, 어떠한 생산 공정에서 만들어진 제품이 수백개의 성능 점검 테스트를 거친다고 가정해보자. 이 과정에서 앞선 수백개의 테스트를 통과했다 하더라도, 마지막 단 한개의 테스트를 통과하지 못할 경우 해당 제품은 이상치로 분류되어 폐기될 것이다. 이처럼 고차원 데이터셋에서는 “irrelavant feature (subspace)” 들에 의해 아웃라이어들이 가려지는 <strong>“masking effect”</strong>가 발생할 가능성이 크다. 이러한 맥락에서 해당 포스트는 고차원 데이터에서의 이상치 탐지와 관련한 몇가지 방법론들에 대해 정리해볼 예정이다.</p>

<p> </p>

<hr />

<h1 id="1-axis-parallel-subspaces">1. Axis-parallel Subspaces</h1>

<h3 id="11-subspace-outlier-detection">1.1. Subspace Outlier Detection</h3>

<p>“Axis-parallel subspace”란 전체 데이터셋의 차원 (feature) 에 대한 부분 집합을 의미한다. 가령, 총 6개의 feature $X_1,\dots, X_6$ 를 갖고 있는 데이터를 ${X_1, X_2}$ 의 단 두개의 차원으로 투영할 경우, $\text{Span}({X_1, X_2})$ 가 바로 axis-parallel subspace라고 할 수 있다. 이는 아웃라이어들이 특정한 몇개의 차원에 의한 부분 공간에서 더욱 부각될 수 있다는 아이디어를 바탕으로 하는데, 실제 상황에서는 여러개의 부분 공간을 임의로 선정한 다음, 해당 공간들에서의 이상치 탐지 결과를 수합해서 최종적으로 아웃라이어를 판단하는 앙상블 기법이 주로 활용된다. 이후 살펴볼 feature bagging, rotated bagging, isolation forest 등이 그 대표적인 예시라고 할 수 있다.</p>

<p>이렇게 전체 데이터셋의 부분 공간에서 아웃라이어를 파악하려는 시도를 <strong>“subspace outlier detection”</strong> 이라고 일컫는다. 이는 <a href="http://charuaggarwal.net/outl.pdf">해당 논문</a>에서 처음으로 제안되었는데, 저자들은 데이터의 부분 공간 중 비정상적으로 낮은 밀도 (density) 를 갖는 저차원의 공간들을 유전학적인 알고리즘을 바탕으로 탐색한 다음, 데이터들이 해당 부분 공간에 포함되는지 여부를 바탕으로 아웃라이어를 정의한다. 즉, 해당 방법론의 핵심은 바로 낮은 밀집도를 갖는 저차원 공간을 찾아내는 것이라고 할 수 있으며, 따라서 컴퓨팅 속도가 주된 이슈가 된다. 이와 관련해서 탐색의 효율성을 높일 수 있는 다양한 방법론들이 후속 연구들에 의해 제안되었다 (<a href="https://ieeexplore.ieee.org/document/4053098">참고1</a>, <a href="https://link.springer.com/article/10.1007/s10115-006-0020-z">참고2</a>).</p>

<p> </p>

<h3 id="12-feature-bagging">1.2. Feature Bagging</h3>

<p>한편, 위 방법과는 대조적으로 부분 공간에 대한 탐색 자체가 완벽히 랜덤하게 수행될 수도 있다. 이러한 기법을 바로 “feature bagging” 또는 “random subspace ensemble” 방법이라고 부르는데, 대략적인 개요는 다음과 같다:</p>

<ol>
  <li>데이터셋의 차원 수 $d$ 에 대해, $d/2$ 에서 $d-1$ 사이의 랜덤한 정수 (integer) $r$ 을 선택한다.</li>
  <li>데이터셋에서 랜덤하게 $r$ 개의 feature를 비복원추출하여 차원 축소된 데이터셋 $D_r$ 을 얻는다.</li>
  <li>$D_r$ 에 대해 이상치 탐지 모형 $O$ 를 적용한다.</li>
  <li>1 - 3 의 과정을 앙상블의 개수 $t$ 만큼 반복한 다음 결과를 수합해서 아웃라이어를 정의한다.</li>
</ol>

<p>해당 방법은 다소 나이브한 것처럼 보일 수 있으나, 실제 적용시 많은 상황에서 full-dimensional approach에 비해 우월한 성능을 보일 수 있다는 점이 연구된 바 있다 <a href="https://www.researchgate.net/publication/221653185_Feature_bagging_for_outlier_detection">(참고)</a>. 비록 부분 공간에 대한 탐색이 optimal하게 이루어지지는 않으나, 여러개의 부분 공간의 결과를 수합해서 정의된 아웃라이어는 충분히 많은 부분 공간들에서 아웃라이어임이 드러난, 이른바 “강건한 아웃라이어 (robust outliers)” 라는 특징이 있다. 이처럼 여러개의 모형을 개별적으로 학습시켜 (weak-learners) 더 나은 결과를 얻을 수 있다는 점은 분류 문제에서</p>

<p>This phenomenon can also be formally explained in terms of the notion of how ensemble methods reduce representational bias [170] (T. Dietterich. Ensemble Methods in Machine Learning. First International Workshop on Multiple Classifier Systems, 2000.)</p>

<hr />

<p> </p>

<p>##</p>

<p> </p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Aggarwal, C. C. (2013). <em>Outlier Analysis</em>. Springer. ISBN: 978-1-4614-6396-2</li>
  <li>C. C. Aggarwal and P. S. Yu. Outlier Detection in High Dimensional Data. ACM SIGMOD Conference, 2001.</li>
  <li>J. Zhang, Q. Gao, and H. Wang. A Novel Method for Detecting Outlying Subspaces in High-dimensional Databases Using Genetic Algorithm. ICDM Conference, 2006</li>
  <li>J. Zhang and H. Wang. Detecting Outlying Subspaces for High-Dimensional Data: the New Task, Algorithms and Performance. Knowledge and Information Systems, 10(3), pp. 333–355, 2006.</li>
  <li>A. Lazarevic and V. Kumar. Feature Bagging for Outlier Detection. ACM KDD Con- ference, 2005</li>
</ul>

:ET