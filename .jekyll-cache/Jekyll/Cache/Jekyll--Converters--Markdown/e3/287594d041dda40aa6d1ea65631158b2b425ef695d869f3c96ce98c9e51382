I"œ<p>This post is an overall summary of Chapter 5 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>3.1. Convergence of Random Variables</strong></p>

<ul>
  <li>Convergence of Sequences</li>
  <li>Almost-sure convergence</li>
  <li>Convergence in probability</li>
  <li>Convergence in quadratic mean</li>
  <li>Convergence in distribution</li>
  <li>Relationships among various convergences</li>
  <li>Slutskyâ€™s Theorem</li>
</ul>

<p><strong>3.2. Central Limit Theorem</strong></p>

<ul>
  <li>Classical CLT</li>
  <li>Lyapunov CLT</li>
  <li>Multivariate CLT</li>
</ul>

<p><strong>3.3. Delta Methods</strong></p>

<ul>
  <li>First-order delta method</li>
  <li>Second-order delta method</li>
  <li>Multivariate delta method</li>
</ul>

<p><strong>3.4. Some helpful theorems</strong></p>

<ul>
  <li>Dominated Convergence Theorem</li>
  <li>Continuous Mapping Theorem</li>
  <li>Stochastic Order Notation</li>
</ul>

<p>Â </p>

<hr />

<h2 id="ch-3-large-sample-theory"><strong>Ch 3. Large Sample Theory</strong></h2>

<p>In this section, we review some of the fundamental concepts and techniques in <strong>asymptotic statistics</strong>.</p>

<p>Statistical inference often requires the knowledge of the underlying distribution of statistics which however, cannot often be derived in a closed form.</p>

<p>To this end, one of the main goals in asymptotic statistics is to address this issue by observing what happens to the statistic under the assumption of <strong>infinite samples</strong> (i.e. $n \to \infty$). Thus, we want to find out the <strong>limiting distribution</strong> of a given statistic, which is often much easier to handle than the actual distribution while precise enough to be useful in practical scenarios.</p>

<p>Â </p>

<h3 id="31-convergence-of-random-variables">3.1 Convergence of Random Variables</h3>

<h4 id="convergence-of-sequences">Convergence of Sequences</h4>

<p>As a building block, letâ€™s briefly review the concept of convergence in deterministic real numbers.</p>

<table>
  <tbody>
    <tr>
      <td>We say that a sequence of real numbers $a_1, a_2, \dots $ converges to a fixed real number $a$, if for every positive number $\epsilon$ there exists a natural number $N(\epsilon)$ such that for all $n \geq N(\epsilon)$, $</td>
      <td>a_n - a</td>
      <td>&lt; \epsilon$.</td>
    </tr>
  </tbody>
</table>

<p>If this is the case, then we call $a$ the limit of the sequence and write $\underset{n \to \infty}{\text{lim} a_n = a$.</p>

<ul>
  <li>Now we focus on how to extend of this notion to the sequences of <em>random variables</em>.</li>
  <li>Specifically, we will see the following convergences:
    <ul>
      <li><strong><em>almost sure convergence</em></strong></li>
      <li><strong><em>convergence in probability</em></strong></li>
      <li><strong><em>convergence in quadratic mean</em></strong></li>
      <li><strong><em>convergence in distribution</em></strong></li>
    </ul>
  </li>
</ul>

<h4 id="almost-sure-convergence"><em>Almost-sure convergence</em></h4>

<p>&lt;/center&gt;</p>

<p>Â </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET