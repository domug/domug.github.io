I"º<p>This post is an overall summary of Chapter 5 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>3.1. Convergence of Random Variables</strong></p>

<ul>
  <li>Convergence of Sequences</li>
  <li>Almost-sure convergence</li>
  <li>Convergence in probability</li>
  <li>Convergence in quadratic mean</li>
  <li>Convergence in distribution</li>
  <li>Relationships among various convergences</li>
  <li>Slutskyâ€™s Theorem</li>
</ul>

<p><strong>3.2. Central Limit Theorem</strong></p>

<ul>
  <li>Classical CLT</li>
  <li>Lyapunov CLT</li>
  <li>Multivariate CLT</li>
</ul>

<p><strong>3.3. Delta Methods</strong></p>

<ul>
  <li>First-order delta method</li>
  <li>Second-order delta method</li>
  <li>Multivariate delta method</li>
</ul>

<p><strong>3.4. Some helpful theorems</strong></p>

<ul>
  <li>Dominated Convergence Theorem</li>
  <li>Continuous Mapping Theorem</li>
  <li>Stochastic Order Notation</li>
</ul>

<p>Â </p>

<hr />

<h2 id="3-large-sample-theory"><strong>(3) Large Sample Theory</strong></h2>

<p>In this section, we review some of the fundamental concepts and techniques in <strong>asymptotic statistics</strong>.</p>

<p>Statistical inference often requires the knowledge of the underlying distribution of statistics which however, cannot often be derived in a closed form.</p>

<p>To this end, one of the main goals in asymptotic statistics is to address this issue by observing what happens to the statistic under the assumption of <strong>infinite samples</strong> (i.e. $n \to \infty$). Thus, we want to find out the <strong>limiting distribution</strong> of a given statistic, which is often much easier to handle than the actual distribution while precise enough to be useful in practical scenarios.</p>

<p>Â </p>

<h3 id="31-convergence-of-random-variables">3.1 Convergence of Random Variables</h3>

<h4 id="convergence-of-sequences">Convergence of Sequences</h4>

<p>As a building block, letâ€™s briefly review the concept of convergence in deterministic real numbers.</p>

<table>
  <tbody>
    <tr>
      <td>We say that a sequence of real numbers $a_1, a_2, \dots $ converges to a fixed real number $a$, if for every positive number $\epsilon$ there exists a natural number $N(\epsilon)$ such that for all $n \geq N(\epsilon)$, $|a_n - a</td>
      <td>&lt; \epsilon$.</td>
    </tr>
  </tbody>
</table>

<p>If this is the case, then we call $a$ the limit of the sequence and denote $\underset{n \to \infty}{\text{lim} a_n = a}$.</p>

<p>Now we focus on how to extend of this notion to the â€œsequencesâ€ of <em>random variables</em>. Specifically, we will see the following convergences:</p>

<ul>
  <li><strong><em>almost sure convergence</em></strong></li>
  <li><strong><em>convergence in probability</em></strong></li>
  <li><strong><em>convergence in quadratic mean</em></strong></li>
  <li><strong><em>convergence in distribution</em></strong></li>
</ul>

<p>Â </p>

<h4 id="almost-sure-convergence"><em>Almost-sure convergence</em></h4>

<p><strong>&lt;Definition&gt;</strong></p>

<table>
  <tbody>
    <tr>
      <td>P({Ï‰âˆˆÎ©:limnâ†’âˆXn(Ï‰)=X(Ï‰)})=1â‡”</td>
      <td>Xn(Ï‰)âˆ’X(Ï‰)</td>
      <td>â‰¤Ïµholds for every Ïµ&gt;0</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Conceptually, this can be thought of as if there is some set of â€œexceptionalâ€ events on which the random variables can disagree, but these exceptional events have probability converging to 0 as $n \to \infty$.</li>
</ul>

<p>Â </p>

<h4 id="convergence-in-probability"><em>Convergence in probability</em></h4>

<p><strong>&lt;Definition&gt;</strong></p>

<p>A sequence of random variables $X_1, \dots, X_n$ converges in probability to a random variable $X$ if for every $\epsilon &gt; 0$, we have that</p>

<table>
  <tbody>
    <tr>
      <td>limnâ†’âˆP(</td>
      <td>Xnâˆ’X</td>
      <td>â‰¥Ïµ)â‰¤Ïµ,âˆ€Ïµ&gt;0â‡”limnâ†’âˆP(</td>
      <td>Xnâˆ’X</td>
      <td>â‰¥Ïµ)=0â‡”Xnâ†’pX</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>Conceptually, <em>convergence in probability</em> implies that as $n$ gets large, the distribution of $X_n$ gets more peaked around the value of convergence. Hence the random variable converges in a â€œprobabilisticâ€ sense.</p>
  </li>
  <li>
    <p>A famous example of <em>convergence in probability</em> is the <strong>weak law of large numbers</strong>. Suppose that $Y_1,Y_2,\dots$ are i.i.d. with $E[Y]=\mu$ and $Var(Y_i) = \sigma^2 &lt; \infty$, then</p>

    <p>Xn=1nâˆ‘i=1nYiâ†’pÎ¼</p>
  </li>
</ul>

<h4 id="convergence-in-quadratic-mean"><em>Convergence in quadratic mean</em></h4>

<p><strong>&lt;Definition&gt;</strong></p>

<p>We say that a sequence converges to $X$ in quadratic mean if:</p>

<p>E(Xnâˆ’X)2â†’p0 as nâ†’âˆâ‡”Xnâ†’qm0</p>

<ul>
  <li>This convergence is often used to prove <em>convergence in probability</em> as it is a stronger condition.</li>
</ul>

<p>&lt;/center&gt;</p>

<p>Â </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET