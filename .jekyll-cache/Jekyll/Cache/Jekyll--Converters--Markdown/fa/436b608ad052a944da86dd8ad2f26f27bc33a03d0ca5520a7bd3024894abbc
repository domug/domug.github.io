I"°<p>This post is an overall summary of Chapter 7 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>1.1. Sigma Algebra</strong></p>

<p><strong>1.2. Expected Values</strong></p>

<p><strong>1.3. Moment Generating Functions</strong></p>

<p><strong>1.4. Characteristic Functions</strong></p>

<p><strong>1.5. Statistical Independence</strong></p>

<p><strong>1.6. Covariance and Correlation</strong></p>

<p><strong>1.7. Conditional Distributions</strong></p>

<p>¬†</p>

<hr />

<h2 id="4-statistical-estimation">4. Statistical Estimation</h2>

<p><strong>Statistical estimation</strong> is the very procedure of an endeavor to understand various aspects of an <strong>underlying population</strong> based on collected <strong>samples</strong>, so it can naturally be regarded as the core of statistics.</p>

<p>But typically, we can‚Äôt afford to have lot of samples (at least those which are ‚Äúmeaningful‚Äù), so we need a <strong>statistical model</strong> that best reveals our understanding of the underlying distribution. In this sense, a statistical model is the set of all possible distributions <em>F</em> for a specific event of our interest. Broadly, we have two kinds of statistical models - ‚Äúparametric‚Äù and ‚Äúnon‚Äù-parametric models.</p>

<p>In a <strong>parametric model,</strong> the set of possible distributions <em>F</em> can be described by a finite number of parameters. A canonical example is the Gaussian model with the location $\mu$ and scale parameter $\sigma$ which can be represented as:</p>

<center>

$$
\begin{aligned}
F = \Big\{ f(x;\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\text{exp}\Big(-\frac{(x-\mu)^2}{2\sigma^2} \Big), \quad \mu \in \mathbb{R}, \sigma &gt; 0 \Big\}
\end{aligned}
$$

</center>

<p>On the other hand, the <strong>non-parametric model</strong> is one in which $F$ cannot be parameterized by a finite number of parameters. The density estimation task is one of the popular examples.</p>

<p>¬†</p>

<hr />

<h3 id="41-overview">4.1 Overview</h3>

<p>¬†</p>

<h4 id="point-estimation">Point Estimation</h4>

<p>First of all, we focus our attention to the <strong>point estimation</strong> where we are dealing with a single ‚Äúbest guess‚Äù from the possible values of an unknown quantity of interest. Here, the quantity of interest is often called as the <strong>parameter</strong> which we often denote as $\hat\theta$ or $\hat\theta_n$.</p>

<p>So basically, a point <strong>estimator</strong> is a function of the random variables $X_1, \dots, X_n$ such that</p>

<center>

$$
\hat\theta_n = g(X_1, \dots, X_n)
$$

</center>

<p>which makes $\hat\theta_n$ itself another <strong>random variable</strong>.</p>

<p>Because any function of the collected samples can be regarded as an estimator (it is easy to think of estimator as some kind of decision rule), we need some <strong>criteria</strong> to measure the performance of the many possible estimators and possibly define the best one. This was studied intensively in the field of mathematical statistics in the early 20th century, and most fundamental criteria include <strong>unbiasedness</strong> and <strong>consistency</strong>.</p>

<p>The <strong>bias</strong> of an estimator $\hat\theta$ is defined as:</p>

<center>

$$
b(\hat\theta) = \mathbb{E}_\theta \big[\hat\theta\big] - \theta
$$

</center>

<p>So basically, the implication of an ‚Äú<em>unbiasedness</em>‚Äù is that the expectation of an estimator is exactly the same as the true parameter.</p>

<p>On the other hand, we call that an estimator is <em>‚Äúconsistent‚Äù</em> if the estimator converges to the true parameter in probability such that:</p>

<center>

$$
\text{for any }\epsilon&gt;0,\quad \mathbb{P}_\theta \big(|\hat\theta_n - \theta| \geq \epsilon\big) \to 0 \quad \text{as } n \to \infty
$$

</center>

<p>So by the definition, It is no doubt that these two criteria are the very least conditions to be satisfied for an estimator to be statistically valid.</p>

<p>Then for the class of estimators that passes the above two guardrail properties, the performance of an estimator is often measured by its <strong>variance</strong>, which is given by:</p>

<center>

$$
\text{Var}_\theta[\hat\theta] = \mathbb{E}_\theta\big[(\hat\theta_n - \mathbb{E}_\theta[\hat\theta])^2\big]
$$

</center>

<p>In this sense, the ‚Äúbest‚Äù estimator is the one that achieves the <strong>minimum variance</strong>, and the existance of such best estimator (a.k.a. <em>MVUE</em>) for various statistics have been studied intensively in the field of mathematical statistics.</p>

<p>¬†</p>

<hr />

<p>&lt;/center&gt;</p>

<p>¬†</p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET