I"+<p>컴퓨팅 기술이 고도화됨에 따라 알고리즘적인 접근이 많이 사용되긴 하지만, 그럼에도 불구하고 통계학의 확률적 모형들 역시 지금까지도 이상치 탐지 분야에서 상당히 유용하게 사용된다. 이러한 통계적 방법론들의 기원은 19세기까지 거슬러 올라가는데, 해당 포스트에서는 전통적인 통계적 기법들이 어떻게 이상치 탐지 분야에 자연스레 활용될 수 있는지를 살펴볼 예정이다.</p>

<p>이상치 탐지와 관련해서 가장 많이 쓰이는 통계적 방법론은 <strong><em>extreme univariate test</em></strong> 와 관련이 있다. 얼핏보면 단순히 1차원에서의 이상치 탐지가 뭐 그리 대수냐라고 할 수 있겠으나, 조금만 생각해보면 이는 굉장히 유용하게 활용될 여지가 있다는 사실을 깨달을 수 있다. 가령, 대부분의 이상치 탐지 알고리즘은 각 데이터 인스턴스마다 1개씩 <strong>일차원의 outlier score</strong>를 반환한다. 이후 이러한 outlier score 들로부터 극단값을 판단한 다음 최종적으로 outlier threshold를 설정하게 되는데, 이 과정에서 자연스럽게 extreme value test가 활용될 수 있다.</p>

<p>물론 이러한 univariate test 이외에도 아예 직접적으로 확률 모형을 이상치 탐지에 활용할 수도 있다. 그 중에서도 특히나 유용한 것은 <strong>혼합 모형 (mixture models)</strong> 인데, 이는 일종의 “통계적” 클러스터링 기법이라고 볼 수 있다. 이러한 혼합 모형의 큰 장점은 mixture component 만 잘 정의된다면 비교적 다양한 구조의 데이터에 폭넓게 활용될 수 있다는 점이다. 또한 해당 모형은 generative model이기 때문에 자연스럽게 likelihood를 outlier score로 활용할 수 있다.</p>

<p> </p>

<h3 id="목차">&lt;목차&gt;</h3>

<p><strong>1. Extreme-value analysis for univariate data</strong></p>

<p><strong>2. Extreme-value analysis for multivariate data</strong></p>

<p><strong>3. Probabilistic modeling of outliers</strong></p>

<p><strong>4. Limitations of probabilistic models for outlier analysis</strong></p>

<p> </p>

<hr />

<h2 id="1-extreme-value-analysis-for-univariate-data">1. Extreme-value analysis for univariate data</h2>

<p>통계학에서 이상치에 대한 연구는 특정한 분포에서 꼬리의 성질을 바탕으로 (tail property) 일차원 데이터에서 이상치를 파악하려는 시도로부터 출발했다. 이와 관련해 <strong>“extreme-value analysis”</strong> 란, 특정한 통계 분포의 분포적 꼬리 (i.e. <strong><em>distribution tail</em></strong> ) 를 규명하려는 작업을 의미한다. 직관적으로, 통계 분포에서 낮은 확률값 (likelihood) 을 갖는 부분은 이상치로 정의하는 것이 자연스러울 것이다.</p>

<p>한편, 실제 상황에서는 데이터의 분포를 가정하는 것이 쉽지 않을 수 있다. 이러한 경우 사용될 수 있는 한가지 유용한 수학적 도구는 바로 <strong>“tail inequalities”</strong> 인데, 이를 바탕으로 분포의 형태와 무관하게 어떠한 확률 변수의 값이 가질 수 있는 이론적 확률에 대한 범위를 제한시킬 수 있다. 이렇게 얻어진 이론적 boundary를 넘어가는 값들은 정상적인 매커니즘이 아닌 “다른 프로세스”로 부터 발생된 것으로 간주할 수 있으며, 이는 우리가 찾고자 하는 이상치가 된다.</p>

<p>이 과정에서 기본적으로 더 강한 가정이 사용될 수록 더 좁은 (tight 한) 범위로 확률 변수의 값을 제한할 수 있다. 다시 말해, 별다른 가정사항이 없는 inequality의 경우 범용성은 넓을 수 있으나 tail의 bound가 더 넓다는 단점이 있으며, 이와는 반대로 보수적인 tail bound를 잡기 위해서는 좀 더 strong한 condition이 필요하다. 이러한 맥락에서 구체적으로 네 가지의 부등식을 살펴보도록 하겠다.</p>

<p> </p>

<h3 id="11-markov-inequality">1.1 Markov Inequality</h3>

<p><em>Markov Inequality</em> 는 가장 general하게 적용될 수 있는 기본적인 tail inequality 인데, 이는 <strong>양의 실수에서 정의되는 확률 변수</strong> $X$ 에 대해 평균이 $\mathbb{E}[X]$ 일 경우 어떠한 상수 $\alpha$ 에 대해 다음을 보장한다.</p>

<p> </p>

<center>

$$
P(X &gt; \alpha) \leq \frac{\mathbb{E}[X]}{\alpha}
$$

</center>

<p> </p>

<p><em>Markov inequality</em> 는 다음의 두가지 가정 사항을 필요로 한다. 두 조건 모두 그렇게 강한 가정 사항은 아니기 때문에 이로부터 얻어지는 tail bound 역시 그렇게 실용적이지는 않다.</p>

<ul>
  <li>확률변수 $X$는 음수를 제외한 실수 전체에서 정의된다.</li>
  <li>1차 적률 (평균) 값이 유한한 값으로 제한 (bounded) 된다.</li>
</ul>

<p> </p>

<h3 id="12-chebychev-inequality">1.2. Chebychev Inequality</h3>

<p>앞서 살펴본 <em>Markov Inequality</em> 는 음수가 아닌 확률 변수에 대해서만 정의될 수 있었다. 이를 좀 더 완화시킨 것이 바로 <strong><em>Chebychev Inequality</em></strong> 인데, 구체적으로 이는 어떠한 확률 변수 $X$ 에 대해 $Y = |X - E[X]|$의 변수 변환을 적용해서 다음과 같이 정의된다.</p>

<p> </p>

<center>

$$
P\big(|X-\mathbb{E}[X]| &gt; \alpha \big) \leq \frac{Var(X)}{\alpha^2}
$$

</center>

<p> </p>

<p>이와 같이 특정한 확률변수에 non-negative한 함수를 씌워 치역을 음수가 아닌 실수의 영역으로 바꿔준 다음 <em>Markov inequality</em> 를 적용하는 것은 굉장히 유용하게 사용되는 테크닉이다. 곧 살펴볼 <em>Chernoff bound</em> 와 <em>Hoeffding inequality</em> 역시 이러한 아이디어를 바탕으로 정의된다.</p>

<p>현재까지 살펴본 <em>Markov inequality</em> 와 <em>Chebychev inequality</em> 는 상대적으로 약한 부등식이기 때문에 이로 부터 실용적인 결과를 얻기는 힘들 수 있다. 이제 좀 더 강한 조건들을 추가해서 부등식이 제시하는 tail bound를 발전시켜 보도록 하자.</p>

<p> </p>

<h3 id="13-chernoff-bound">1.3. Chernoff Bound</h3>

<p><strong><em>Chernoff Bound</em></strong> 는 서로 독립인 $N$개의 베르누이 확률변수의 합으로 정의되는 확률변수 $X = \sum_{i=1}^N X_i$ 에 대한 부등식이다. 앞선 부등식들에 비해서 <strong>베르누이 분포의 합</strong>이라는 좀 더 강한 제약 조건이 붙었다. 조금 특이한 것은 lower tail과 upper tail 각각에 대한 바운더리가 서로 대칭이 아니라는 점이다.</p>

<p> </p>

<center>

$$
\begin{aligned}
&amp;X_i \overset{i.i.d.}{\sim} Bernoulli(p), \quad (i=1,\dots,N) \\[20pt]
&amp;\text{For any }\delta \in (0,1), \\[10pt] 
&amp;\Rightarrow P\Big(X &lt; (1-\delta) \mathbb{E}[X]\Big) &lt; e^{-\mathbb{E}[X] \times \delta^2/2 } \quad \text{(lower-tail Chernoff Bound)} \\[20pt]

&amp;\text{For any }\delta \in (0,2e-1), \\[10pt] 
&amp;\Rightarrow P\Big(X &gt; (1+\delta) \mathbb{E}[X]\Big) &lt; e^{-\mathbb{E}[X] \times \delta^2/4 } \quad \text{(upper-tail Chernoff Bound)} \\[10pt]
\end{aligned}
$$

</center>

<p> </p>

<p><em>Chernoff Bound</em> 가 적용될 수 있는 가장 대표적인 사례는 제조 공장 등에서의 고장 진단 (fault diagnosis) 이다.</p>

<p>예를 들어, 불량 제품의 발생 확률이 $p$ 인 어떤 생산 공정이 있다고 해보자. 이 때, 이로부터 생산된 $N$개의 제품을 바탕으로 해당 생산 공정이 제대로 작동하고 있는지를 확인해볼 수 있다. 구체적인 적용 과정을 좀 더 살펴보자면, $p=0.01$인 생산 공정에서 총 1000개의 제품 중 12개의 불량 제품이 발생한 경우, <em>Chernoff Bound</em> 를 바탕으로 12개 이상의 불량 제품이 관측될 확률의 upper bound를 구해보면 약 8.2%가 나온다 ($\delta = 2$).</p>

<p> </p>

<h3 id="14-hoeffdings-inequality">1.4. Hoeffding’s Inequality</h3>

<p>다음으로는 <strong><em>Hoeffding Inequality</em></strong> 에 대해 알아보자. <em>Hoeffding Inequality</em>는 $[l_i, u_i]$의 범위에 bounded 되어 있는 확률 변수 $X_i$ 들의 합 $X = \sum_{i=1}^N X_i$ 에 대해서 임의의 양수 $\theta&gt;0$ 에 대해 다음의 probability bound를 제시한다. 확률변수들에 대한 분포 가정이 없다는 점에서 꽤나 실용적이라고 할 수 있다.</p>

<p> </p>

<center>

$$
\begin{aligned}
P(X - \mathbb{E}[X] &gt; \theta) \leq exp\Big({-\frac{2\theta^2}{\sum_{i=1}^N (u_i - l_i)^2}} \Big)
\end{aligned}
$$

</center>

<p> </p>

<p>Hoeffiding inequality의 한가지 흥미로운 점은, tail bound가 모수 $\theta$ 에 대해 지수적으로 감소한다는 점이다. 이는 정규분포의 성질과 매우 유사한데, 사실 이는 중심극한 정리를 바탕으로 조금만 생각해보면 당연한 결과라는 것을 알 수 있다. 즉, 표본의 크기가 충분할 경우 중심극한 정리에 의해 확률변수의 합은 정규분포로 수렴한다는 점에서, 이를 바탕으로 asympotically exact한 바운더리를 구할 수 있기 때문이다.</p>

<p> </p>

<p>여태까지 특정한 확률변수의 이론적 극단값을 제한시킬 수 있는 여러가지 수학적 부등식에 대해 살펴보았다. 이러한 부등식들은 그 가정 사항에 따라 probability bound의 strength가 달라진다는 점 역시 살펴보았는데, 이를 정리하면 다음과 같다.</p>

<center>
  <img src="/images/outlier/5.png" width="600" height="400" /> 
 <br />
 <em><span style="color:grey">Table 1: Comparison of different methods used to bound tail probabilities</span></em>
</center>

<p> </p>

<h3 id="15-the-normal-based-tail-confidence-tests">1.5. The normal-based tail confidence tests</h3>

<p>한편, 앞서 살펴본 부등식들과는 다르게, 만약 특정한 확률변수의 정확한 분포 (또는 근사 분포) 를 정의할 수 있다면 해당 분포에서의 누적확률밀도 값을 이용해서 tail bound를 정의할 수 있다. 이와 관련해서 현실속의 많은 문제들에서 우리는 중심극한정리에 의해 확률 변수의 근사적 분포를 정규분포로 설정할 수 있는 점은 꽤나 중요한 사실이다.</p>

<p>이러한 측면에서 교재에서는 구체적으로 정규분포와 t 분포에 대해서 소개하고 있는데, 크게 새로운 내용은 없으므로 자세한 설명은 생략하도록 하겠다. 이상치 탐지의 맥락에서 tail bound는 z-score $Z = \frac{X-\mu}{\sigma}$ 에 대한 표준 정규 분포에서의 누적 확률 밀도를 바탕으로 정의된다.</p>

<hr />

<hr />

<p> </p>

<p> </p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Aggarwal, C. C. (2013). <em>Outlier Analysis</em>. Springer. ISBN: 978-1-4614-6396-2</li>
</ul>

:ET