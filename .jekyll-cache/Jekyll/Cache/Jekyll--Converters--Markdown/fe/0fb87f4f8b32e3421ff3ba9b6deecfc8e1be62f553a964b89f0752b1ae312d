I"Ø
<p>This post is an overall summary of Chapter 8 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>1.1. Sigma Algebra</strong></p>

<p><strong>1.2. Expected Values</strong></p>

<p><strong>1.3. Moment Generating Functions</strong></p>

<p><strong>1.4. Characteristic Functions</strong></p>

<p><strong>1.5. Statistical Independence</strong></p>

<p><strong>1.6. Covariance and Correlation</strong></p>

<p><strong>1.7. Conditional Distributions</strong></p>

<p>¬†</p>

<hr />

<h1 id="5-hypothesis-testing">5. Hypothesis Testing</h1>

<p>¬†</p>

<h2 id="51-basics-of-hypothesis-testing">5.1 Basics of Hypothesis Testing</h2>

<h3 id="511-overview">5.1.1. Overview</h3>

<p>The typical and most clich√©ic setting of a statistical hypothesis testing is that from the observations $X_1, \dots, X_n \overset{i.i.d.}{\sim} f_\theta$, we want to test if $\theta = \theta_0$ or not.</p>

<p>A more concrete example is where we have a coin and we are interested in finding out whether the coin is fair (i.e. equal probability of heads and tails) or not. This procedure can formally be expressed as:</p>

<center>

$$
\begin{aligned}
X_1, \dots, X_n &amp;\overset{i.i.d.}{\sim} \text{Bernoulli}(p)\\[10pt]
H_0&amp;: p = \frac{1}{2} \\
H_1&amp;: p \neq \frac{1}{2}
\end{aligned}
$$

</center>

<p>To generalize this problem, we have two sets of parameters $\Theta_0$ and $\Theta_1$ which are <strong>non-overlapping</strong> (i.e. $\Theta_0 \cap \Theta_1 = \emptyset$), and would like to test the hypothesis:</p>

<center>

$$
\begin{aligned}
H_0&amp;: \theta \in \Theta_0 \\[5pt]
H_1&amp;: \theta \in \Theta_1
\end{aligned}
$$

</center>

<p>For the case when $\Theta_0$ is a single point, we call it as a <strong>simple null</strong>, whereas the more general case is refered to as a <strong>composite null</strong>.</p>

<p>As a general reminder, <strong>statistical hypothesis testing never guarentees optimality</strong>. Namely, the question is never if the null hypothesis is true or not. Rather, it is whether we have sufficient evidence to reject the null hypothesis or not, because the result of a hypothesis test is one of the two possibilities - ‚Äúreject the null‚Äù or ‚Äúretain the null‚Äù.</p>

<p>¬†</p>

<h3 id="type-i-error--type-ii-error">Type I error &amp; Type II error</h3>

<p>&lt;/center&gt;</p>

<p>¬†</p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET