I".'<p>This post is an overall summary of Chapter 3, 4 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>1.1. Sigma Algebra</strong></p>

<p><strong>1.2. Expected Values</strong></p>

<p><strong>1.3. Moment Generating Functions</strong></p>

<p><strong>1.4. Characteristic Functions</strong></p>

<p><strong>1.5. Statistical Independence</strong></p>

<p><strong>1.6. Covariance and Correlation</strong></p>

<p><strong>1.7. Conditional Distributions</strong></p>

<p> </p>

<hr />

<h2 id="ch-2-inequalities"><strong>Ch 2. Inequalities</strong></h2>

<p>One of the main goals in statistics and related fields is to make an inference about unknown parameters. For this purpose, we define an <strong>estimator</strong> that can reasonably surrogate the true parameter.</p>

<p>However, the problem is that there are many possible choices of estimators. So we often consider a <strong>risk function</strong> between the true parameter $\theta$ and its estimator $\hat \theta$ to evaluate its performance.</p>

<p>Nevertheless, the problem still exists in that computing a risk function is not so simple . Thus, we make use of various inequalities to <strong>upper bound</strong> the risk function by a function that is more easy to manipulate and study how fast the risk function goes to zero as sample sizes increase.</p>

<p> </p>

<h3 id="21-inequalities-for-expectations">2.1 Inequalities for expectations</h3>

<h4 id="jensens-inequality"><em>Jensen’s Inequality</em></h4>

<p>For a convex function $g$ such that</p>

<center>

$$
\lambda g(x) + (1-\lambda)g(y) \geq g(\lambda x + (1-\lambda)y)
$$

</center>

<p>for all $\lambda \in (0,1)$ and $x,y \in \mathbb{R}$. Then provided that expectations exist for both random variables,</p>

<center>

$$
E\big[ g(X)\big] \geq g\big(E[X] \big)
$$

</center>

<ul>
  <li>An application of Jensen’s Inequality is the relationship between different means.</li>
</ul>

<center>

$$
\begin{aligned}
&amp;\text{Arithmetric Mean (AM)} = \frac{\sum_{i=1}^n x_i}{n} \\[5pt]
&amp;\text{Geometric Mean (GM)} = \Big(\prod_{i=1}^n x_i\Big)^{1/n} \\[5pt]
&amp;\text{Harmonic Mean (HM)} = \frac{1}{\frac{1}{n}\sum_{i=1}^n\frac{1}{x_i}}
\end{aligned}
$$

$$
\Rightarrow \text{HM} \leq \text{GM} \leq \text{AM}
$$

</center>

<ul>
  <li>Another famous application is the positiveness of <strong><em>Kullback-Leibler divergence</em></strong>.</li>
</ul>

<center>

$$
D_{KL}P(||Q) = \sum_x p(x)log\Big(\frac{p(x)}{q(x)}\Big) = E\Big[ log\Big(\frac{p(X)}{q(X)}\Big)\Big] = E\Big[-log\Big(\frac{q(X)}{p(X)}\Big)\Big] \geq 0
$$

</center>

<p> </p>

<h4 id="hölders-inequality"><em>Hölder’s Inequality</em></h4>

<p>For $p, q \in (1,\infty)$ with $\frac{1}{p} + \frac{1}{q} = 1$,</p>

<center>

$$
\big|E[XY]\big| \leq E\big[|XY|\big] \leq \big(E\big[|X|^p\big]\big)^{1/p}  \big(E\big[|Y|^q\big]\big)^{1/q}
$$

</center>

<ul>
  <li>A special case of Hölder’s Inequality when $p=q=2$ is the <strong><em>Cauchy-Schwarz inequality</em></strong> such that:</li>
</ul>

<center>

$$
\big|E[XY]\big| \leq E\big[|XY|\big] \leq \sqrt{E\big[X^2\big]  E\big[Y^2\big]}
$$

</center>

<ul>
  <li>By applying the <em>Cauchy-Schwarz inequality</em>, we have the covariance inequality such that:</li>
</ul>

<center>

$$
\begin{aligned}
Cov(X, Y) = E\big[(X-\mu_X)(Y-\mu_Y) \leq \{E\big[(X-\mu_X)^2\big] \}^{1/2} \{E\big[(Y-\mu_Y)^2\big] \}^{1/2}
\end{aligned}
$$

$$
\Leftrightarrow \{Cov(X, Y)\}^2 \leq Var(X)Var(Y)
$$

</center>

<p> </p>

<h4 id="minkowskis-inequality"><em>Minkowski’s Inequality</em></h4>

<p>For two random variables $X$ and $Y$ and a constant $1 &lt; p &lt; \infty$,</p>

<center>

$$
\big(E[|X+Y|^p] \big)^{1/p} \leq \big(E[|X|^p]\big)^{1/p} \big(E[|Y|^q]\big)^{1/q}
$$

$$
||X+Y||_p\; \leq \;||X||_p \;+\; ||Y||_p
$$

</center>

<p>which implies <strong>triangle inequality</strong> in p-dimensional space.</p>

<p> </p>

<h4 id="association-inequality"><em>Association Inequality</em></h4>

<p>Suppose we have a random variable $X$ and functions $f, g: \mathbf{R} \to \mathbf{R}$. Assuming that all expectations are well-defined, we have:</p>

<center>

$$
\begin{aligned}
&amp;1.\quad f,g \text{ non-decreasing implies } E\big[f(X)g(X)\big] \geq E\big[f(X)\big]E\big[g(X)\big] \\[10pt]
&amp;2.\quad f,g \text{ non-increasing implies } E\big[f(X)g(X)\big] \geq E\big[f(X)\big]E\big[g(X)\big] \\[10pt]
&amp;3.\quad f \text{ non-decreasing and }g \text{ non-increasing implies } E\big[f(X)g(X)\big] \leq E\big[f(X)\big]E\big[g(X)\big]  \\[10pt]
\end{aligned}
$$

</center>

<ul>
  <li>By a direct application of association inequality, we have:</li>
</ul>

<center>

$$
E\big[X^4\big] \geq E\big[X\big]E\big[X^3\big]
$$

$$
E\big[Xe^{-X}\big] \leq E\big[X\big]E\big[e^{-X}\big]
$$

$$
E\big[X\mathbb{1}(X\geq a)\big] \geq E\big[X\big] P\big(X \geq a\big)
$$

</center>

<p> </p>

<h3 id="22-inequalities-for-variances">2.2 Inequalities for variances</h3>

<h4 id="efron-stein-inequality"><em>Efron-Stein Inequality</em></h4>

<p>Suppose that $X_1, \dots, X_n, X_1^\prime, X_n^\prime$ are independent with $X_i$ and $X_i^\prime$ having the same distribution for all $i \in {1,\dots,n}$. Let $X = (X_1, \dots, X_n)$ and $X^{(i)} = (X_1, \dots, X_{i-1}, X_i^\prime, X_{i+1}, \dots, X_n)$. Then,</p>

<center>

$$
Var\big[f(X)\big] \leq \frac{1}{2}\sum_{i=1}^nE\big[\{f(X) - f(X^{(i)})\}^2\big]
$$

</center>

<ul>
  <li>As an example, we can apply the <em>Efron-Stein Inequality</em> to the mean function such that:</li>
</ul>

<center>

$$
\begin{aligned}
&amp;\text{For }f(X) = \frac{1}{n}\sum_{i=1}^n X_i, \\[5pt]
&amp;\Rightarrow Var\big(f(X)\big) \leq \sum_{i=1}^n \frac{1}{2n^2}E\big[(X_i - X_i^\prime)^2\big] = \frac{Var(X)}{n}
\end{aligned}
$$

</center>

<p> </p>

<h3 id="23-inequalities-for-probabilities">2.3 Inequalities for probabilities</h3>

<h4 id="markovs-inequality"><em>Markov’s Inequality</em></h4>

<p>For any $t \geq 0$ and integrable nonnegative random variable $X$, we have:</p>

<center>

$$
\begin{aligned}
P(X \geq t) \leq \frac{E[X]}{t} 
\end{aligned}
$$

$$
\Leftrightarrow E[X] \geq t P(X \geq t)
$$

</center>

<ul>
  <li><em>Markov’s Inequality</em> is the weakest inequality, though it cannot be improved in general, unless we put some restrictions on the shape of a distribution function. For example, if we assume that the density is non-increasing (i.e. $f_X^\prime(x) \leq 0$ for all $x \geq 0$), we have:</li>
</ul>

<center>

$$
P(X \geq t) \leq \frac{E[X]}{2t}
$$

</center>

<p> </p>

<h4 id="chebyshevs-inequality"><em>Chebyshev’s Inequality</em></h4>

<p>Let $X$ be a random variable with finite mean $\mu$ and finite variance $\sigma^2 &gt; 0$. Then,</p>

<center>

$$
P\big(|X-\mu|\geq t\big) \leq \frac{Var(X)}{t^2}
$$

</center>

<ul>
  <li>The proof is simple, which is just to square both sides of the equation and apply <em>Markov’s inequality</em>.</li>
</ul>

<p> </p>

<h4 id="hoeffdings-inequality"><em>Hoeffding’s Inequality</em></h4>

<p>Before digging into the <em>Hoeffding’s Inequality</em>, let’s recall <strong><em>Hoeffiding’s lemma</em></strong> such that:</p>

<center>

$$
E\big[exp(tX)\big] \leq exp\big(\frac{1}{8}t^2(b-a)^2\big)
$$

</center>

<p>Based on this lemma, suppose $X_1, \dots, X_n$ are independent bounded random variables in the range of $[a_i, b_i]$ for each $X_i$. Then, for any $t &gt; 0$, we have
\(\begin{aligned}
 P\Big\{ \big|\sum_{i=1}^n(X_i - E[X_i])\big| \geq t \Big\} \leq exp\Big(\frac{-2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\Big)
\end{aligned}\)</p>

<ul>
  <li>Note that the upper bound of <em>Chebyshev’s Inequality</em> goes up <strong>polynomially</strong>, while the bound of <em>Hoeffding’s Inequality</em> goes up <strong>logarithmically</strong>. Suppose that we have i.i.d. <a href="https://en.wikipedia.org/wiki/Rademacher_distribution"><em>Rademacher</em> random variables</a> such that $E[X] = 0$ and $Var(X) = 1$, let’s apply both inequalities to the sample mean of $X_1, \dots, X_n$. Then we have:</li>
</ul>

\[\begin{aligned}
&amp;\text{Chebyshev:} \\[8pt]
&amp; P\big(|\bar X - E[\bar X]| \geq t\big) \leq \frac{1}{nt^2} \\
&amp;\Leftrightarrow P\big(|\bar X - E[\bar X]| \geq \sqrt{\frac{1}{n\delta}} \big) \leq \delta, \quad (\delta = \frac{1}{nt^2}) \\[30pt]
&amp;\text{Hoeffding:} \\[8pt]
&amp; P\big(|\bar X - E[\bar X]| \geq t\big) \leq 2exp(-\frac{nt^2}{2}) \\
&amp;\Leftrightarrow P\Big(|\bar X - E[\bar X]| \geq \sqrt{\frac{2}{n}log(\frac{2}{\delta})}\;\Big)  \leq \delta, \quad (\delta = 2exp(-\frac{nt^2}{2}))
\end{aligned}\]

<ul>
  <li>Thus, suppose we assume $\delta = 0.001$. Then with probability 0.999, the sample mean is within:</li>
</ul>

\[\begin{aligned}
&amp;\text{Chebyshev:} \\[8pt]
&amp;\sqrt{\frac{1}{n \times 0.001}} \approx \frac{31}{\sqrt{n}} \\[30pt]
&amp;\text{Hoeffding:} \\[8pt]
&amp;\sqrt{\frac{2}{n}log(\frac{2}{0.001})} \approx \frac{2.57}{\sqrt{n}}
\end{aligned}\]

<ul>
  <li>We can clearly see that <em>Hoeffing’s inequality</em> is a huge improvement over the bound of <em>Chebyshev’s inequality</em>.</li>
</ul>

<h4 id="dvoretzky-keifer-wolfowitz-dkw-inequality"><em>Dvoretzky-Keifer-Wolfowitz (DKW) Inequality</em></h4>

<p><em>DKW inequality</em> can be useful when we want to upper bound the deviation of an emphirical cdf from the true pdf.</p>

<p>Specifically, there exists a finite positive constant $C$ such that
\(Pr\Big(\underset{x\in\mathbb{R}}{\text{sup}} \big|F_n(x) - F(x)\big| \geq t \Big) \leq Ce^{-2nt^2}, \quad (\text{for all } t \geq 0)\)</p>

<ul>
  <li>The best possible constant $C$ is known as 2 due to Massart (1990).</li>
</ul>

<p>&lt;/center&gt;</p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET