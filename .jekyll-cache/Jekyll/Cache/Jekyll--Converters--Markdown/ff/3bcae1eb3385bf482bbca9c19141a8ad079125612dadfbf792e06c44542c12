I"3<p>대부분의 현실적인 이상치 탐지 시나리오에서 우리는 고차원의 데이터셋을 다루게 된다. 많게는 수백, 수천개의 차원을 갖고 있는 데이터에서 앞서 살펴본 다양한 이상치 탐지 기법들은 제대로 작동하지 않을 우려가 커지는데, 이는 소위 말하는 <strong>차원의 저주 (curse of dimensionality)</strong> 에 의해 데이터가 고차원 공간의 매우 국소적인 부분 공간에 밀집해서 분포하기 때문이다. 이를 “data sparsity” 또는 “distance concentration” 이라고 일컫는다. 가령, 이상치 탐지에 흔히 활용되는 LOF 등의 거리 기반 알고리즘을 생각해보자. 해당 방법론들에서 아웃라이어는 다른 데이터들에 비해 상대적으로 “멀리 떨어진” 값들로 정의되지만 고차원 공간에서 이와 같은 거리에 대한 척도는 아웃라이어에 대한 지표로서의 기능을 제대로 수행하지 못하게 된다.</p>

<p>따라서 고차원 공간에서의 성공적인 이상치 탐지를 위해서는 데이터가 실제로 분포하고 있는 <strong>국소적인 부분 공간 (locally-relavant subspace)</strong> 을 잡아내는 것이 필수적이다. 하지만 이는 결코 만만한 작업이 아닌데, 그 이유는 수 많은 차원 중에서 어떤 차원들이 데이터를 잘 설명하는지를 사전에 알 수가 없으며, 모든 차원들에 대한 조합을 일일히 고려하는 것이 불가능하기 때문이다. 가령, 10차원 정도만 되더라도 가능한 차원의 모든 조합은 $2^{10} = 1024$ 로 상당한 컴퓨팅 자원을 소모할 것이다. 이와 더불어 이상치 탐지를 위해서는 정상적인 데이터와 아웃라이어 간의 상대적 위치를 최대한 극대화 또는 보존시키면서 저차원의 공간을 탐색해야 한다는 점에서 주성분 분석 (PCA) 등의 보편적인 차원축소 방법론 역시 쉽게 활용할 수 없게 된다.</p>

<p>해당 내용을 직관적으로 이해하기 위해 다음의 예시를 살펴보자. 이는 (가상의) 고차원 데이터셋에서 랜덤하게 선택된 두개의 축에 데이터를 투영 (projection) 시킨 것이다.</p>

<center>
  <img src="/images/outlier/12.png" width="700" height="500" /> 
 <br />
 <em><span style="color:grey">Figure 1: The outlier behavior is masked by the irrelevant attributes in high dimensions.
</span></em>
</center>

<p> </p>

<p>위 데이터에서 $A$ 와 $B$ 는 실제로 다른 generative process로 부터 생성된 아웃라이어이다. 이와 관련해서 그림 (a) ~ (d) 는 모두 동일하게 2차원의 부분 공간임에도 불구하고 그 시각에 따라서 $A$, $B$ 가 아웃라이어로서 부각이 될 수도, 되지 않을 수도 있다는 점을 보여준다. 가령, (b)와 (c)는 이상치 탐지의 맥락에서 전혀 유용하지 않은 부분 공간이라고 할 수 있다.</p>

<p>이처럼 아웃라이어는 전체 데이터셋의 아주 일부분의 차원에서만 발견할 수 있는 경우가 많다. 예를 들어, 어떠한 생산 공정에서 만들어진 제품이 수백개의 성능 점검 테스트를 거친다고 가정해보자. 이 과정에서 앞선 수백개의 테스트를 통과했다 하더라도, 마지막 단 한개의 테스트를 통과하지 못할 경우 해당 제품은 이상치로 분류되어 폐기될 것이다. 이처럼 고차원 데이터셋에서는 “irrelavant feature (subspace)” 들에 의해 아웃라이어들이 가려지는 <strong>“masking effect”</strong>가 발생할 가능성이 크다. 이러한 맥락에서 해당 포스트는 고차원 데이터에서의 이상치 탐지와 관련한 몇가지 방법론들에 대해 정리해 볼 예정이다.</p>

<p> </p>

<hr />

<h1 id="1-axis-parallel-subspaces">1. Axis-parallel Subspaces</h1>

<h3 id="11-subspace-outlier-detection">1.1. Subspace Outlier Detection</h3>

<p>“Axis-parallel subspace”란 전체 데이터셋의 차원 (feature) 에 대한 부분 집합을 의미한다. 가령, 총 6개의 feature $X_1,\dots, X_6$ 를 갖고 있는 데이터를 ${X_1, X_2}$ 의 단 두개의 차원으로 투영할 경우, $\text{Span}({X_1, X_2})$ 가 바로 axis-parallel subspace라고 할 수 있다. 이는 아웃라이어들이 특정한 몇개의 차원에 의한 부분 공간에서 더욱 극명하게 보여질 수 있다는 아이디어를 바탕으로 하는데, 실제 상황에서는 여러개의 부분 공간을 임의로 선정한 다음, 해당 공간들에서의 이상치 탐지 결과를 수합해서 최종적으로 아웃라이어를 판단하는 앙상블 기법이 주로 활용된다. 이후 살펴볼 feature bagging, rotated bagging, isolation forest 등이 그 대표적인 예시라고 할 수 있다.</p>

<p>이처럼 전체 데이터셋의 부분 공간에서 아웃라이어를 파악하려는 시도를 <strong>“subspace outlier detection”</strong> 이라고 일컫는다. 이는 <a href="http://charuaggarwal.net/outl.pdf">해당 논문</a>에서 처음으로 제안되었는데, 저자들은 데이터의 부분 공간 중 비정상적으로 낮은 밀도 (density) 를 갖는 저차원의 공간들을 유전학적인 알고리즘을 바탕으로 탐색한 다음, 데이터들이 해당 부분 공간에 포함되는지 여부를 바탕으로 아웃라이어를 정의하려는 접근법을 제안했다. 즉, 해당 방법론의 핵심은 바로 낮은 밀집도를 갖는 저차원 공간을 모델링하는 것이라고 할 수 있으며 컴퓨팅 속도가 주된 이슈가 된다. 이와 관련해서 탐색의 효율성을 높일 수 있는 다양한 방법론들이 후속 연구들에 의해 제안되었다 (<a href="https://ieeexplore.ieee.org/document/4053098">참고1</a>, <a href="https://link.springer.com/article/10.1007/s10115-006-0020-z">참고2</a>).</p>

<p> </p>

<h3 id="12-feature-bagging">1.2. Feature Bagging</h3>

<p>한편, 위 방법과는 대조적으로 부분 공간에 대한 탐색 자체가 완벽히 랜덤하게 수행될 수도 있다. 이러한 기법을 바로 “feature bagging” 또는 “random subspace ensemble” 방법이라고 부르는데, 대략적인 개요는 다음과 같다:</p>

<ol>
  <li>데이터셋의 차원 수 $d$ 에 대해, $d/2$ 에서 $d-1$ 사이의 랜덤한 정수 (integer) $r$ 을 선택한다.</li>
  <li>데이터셋에서 랜덤하게 $r$ 개의 feature를 비복원추출하여 차원 축소된 데이터셋 $D_r$ 을 얻는다.</li>
  <li>$D_r$ 에 대해 이상치 탐지 모형 $O$ 를 적용한다.</li>
  <li>1 - 3 의 과정을 앙상블의 개수 $t$ 만큼 반복한 다음 결과를 수합해서 아웃라이어를 정의한다.</li>
</ol>

<p>해당 방법은 다소 나이브한 것처럼 보일 수 있으나, 실제 적용시 많은 상황에서 full-dimensional approach에 비해 우월한 성능을 보일 수 있다는 점이 연구된 바 있다 <a href="https://www.researchgate.net/publication/221653185_Feature_bagging_for_outlier_detection">(참고)</a>. 비록 부분 공간에 대한 탐색이 optimal하게 이루어지지는 않으나, 여러개의 부분 공간의 결과를 수합해서 정의된 아웃라이어는 충분히 많은 부분 공간들에서 아웃라이어임이 드러난, 이른바 “강건한 아웃라이어 (robust outliers)” 라는 특징이 있다. 이처럼 여러개의 모형을 개별적으로 학습시켜 (weak-learners) 더 나은 결과를 얻을 수 있다는 점은 classification 도메인의 <a href="https://link.springer.com/chapter/10.1007/3-540-45014-9_1">선행 연구</a>를 통해 이론적으로 증명된 부분이다.</p>

<p> </p>

<h3 id="13-projected-clustering-ensembles">1.3. Projected Clustering Ensembles</h3>

<p>군집 분석에서 “projected clustering methods” 란 각각의 데이터 포인트를 저차원의 공간에 위치하는 클러스터에 할당하는 방법론이다.</p>

<center>
  <img src="/images/outlier/13.png" width="400" height="300" /> 
 <br />
 <em><span style="color:grey">Figure 2: Example 2D space with subspace clusters</span></em>
</center>

<p> </p>

<p>위 <a href="https://en.wikipedia.org/wiki/Clustering_high-dimensional_data">그림</a>을 통해서 확인할 수 있듯이, 일정한 거리에 대한 척도를 바탕으로 데이터들은 저차원의 공간에 투영된다. 이 과정에서 정상적인 데이터들이 속한 클러스터에는 비슷한 성질을 갖는 데이터 포인트들이 분포할 것이며, 아웃라이어들에 대한 클러스터에는 상대적으로 성질이 균등하지 않은 데이터 포인트들이 많이 포함될 것이다. 이러한 맥락에서 최종적인 아웃라이어는 각 클러스터에 속한 데이터 포인트들의 상대적 크기, 차원수, 중심점까지의 거리 등을 고려해서 판단되며, 일반적으로 여러개의 차원에 대한 결과를 앙상블하여 결과를 도출하는 방식이 사용된다. 대표적으로는 <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.7839&amp;rep=rep1&amp;type=pdf">PROCLUS</a> 클러스터링 알고리즘을 바탕으로 하는 <a href="http://adrem.uantwerpen.be/bibrem/pubs/outrank.pdf"><em>OutRank</em></a> 가 있다.</p>

<p> </p>

<h3 id="14-isolation-forests">1.4. Isolation Forests</h3>

<p>Isolation forest는 classification 도메인에서 저명한 random forest 기법을 비지도 학습에 맞게 변형한 알고리즘이라고 할 수 있다. Isolation forest의 핵심 아이디어는 데이터를 랜덤하게 선택된 axis-parallel subspace로 반복해서 분할해나가며 각각의 데이터 포인트를 특정한 leaf (노드) 에 고립시키는 방식으로 트리 모형을 학습시키는 것이다. 이 과정에서 아웃라이어들은 표본 공간에서 상대적으로 희소한 부분 공간에 위치할 것이기 때문에, 학습의 초기 단계에서 특정한 leaf로 분류될 가능성이 높아진다. 이러한 맥락에서 outlier score는 각 데이터 포인트가 위치한 leaf와 root까지의 거리로 정의되며, 그 값이 작을수록 아웃라이어일 가능성이 높아진다 <a href="https://www.researchgate.net/figure/Isolation-Forest-learned-iForest-construction-for-toy-dataset_fig1_352017898">(이미지 출처)</a>.</p>

<center>
  <img src="/images/outlier/14.png" width="400" height="300" /> 
 <br />
 <em><span style="color:grey">Figure 3: Example of Isolation Forest</span></em>
</center>

<p> </p>

<p>이러한 맥락에서 병렬적인 “isolation tree” 여러개를 합쳐 놓은 것이 바로 “isolation forest”이다. 따라서 자연스럽게 이는 앙상블 기법을 활용한 subspace outlier detection 방법론이라고 할 수 있으며, 이 때 각각의 가지 (branch)는 하나의 지역적인 부분 공간 (local subspace)에 대응된다.</p>

<p>한편, 다른 subspace outlier detection 방법론들에 비해 Isolation forest는 컴퓨팅 속도의 측면에서 상당한 장점을 갖는다. <a href="https://ieeexplore.ieee.org/document/4781136">논문</a>의 저자들은 각 트리의 훈련 과정에서 전체 데이터셋의 일부분을 샘플링해서 학습을 진행하는 것을 제안했는데, 이로 인해 모델 학습에 대한 시간 복잡도가 일정한 상수로 고정되어 큰 효율성을 갖는다는 특징이 있다. 이 밖에도 학습 시간의 효율을 높이기 위해 첨도 (kurtosis)를 활용하여 트리 분할에 사용될 feature를 선정하는 것과, 트리를 끝까지 분할시키지 않고 일정한 기준을 넘어가면 조기 종료시키는 아이디어가 저자들에 의해 제안되었다. 이러한 특징들에 의해 isolation tree는 앙상블 학습에 초점이 맞춰진, “extremely randomized algorithm” 이라고 할 수 있다.</p>

<p> </p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Aggarwal, C. C. (2013). <em>Outlier Analysis</em>. Springer. ISBN: 978-1-4614-6396-2</li>
  <li>C. C. Aggarwal and P. S. Yu. Outlier Detection in High Dimensional Data. ACM SIGMOD Conference, 2001.</li>
  <li>J. Zhang, Q. Gao, and H. Wang. A Novel Method for Detecting Outlying Subspaces in High-dimensional Databases Using Genetic Algorithm. ICDM Conference, 2006</li>
  <li>J. Zhang and H. Wang. Detecting Outlying Subspaces for High-Dimensional Data: the New Task, Algorithms and Performance. Knowledge and Information Systems, 10(3), pp. 333–355, 2006.</li>
  <li>A. Lazarevic and V. Kumar. Feature Bagging for Outlier Detection. ACM KDD Con- ference, 2005</li>
  <li>T. Dietterich. Ensemble Methods in Machine Learning. First International Workshop on Multiple Classifier Systems, 2000.</li>
  <li>C. C. Aggarwal, C. Procopiuc, J. Wolf, P. Yu, and J. Park. Fast Algorithms for Pro- jected Clustering. ACM SIGMOD Conference, 1999.</li>
  <li>E. Muller, I. Assent, P. Iglesias, Y. Mulle, and K. Bohm. Outlier Analysis via Subspace Analysis in Multiple Views of the Data. ICDM Conference, 2012.</li>
  <li>F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation Forest. ICDM Conference, 2008.</li>
</ul>

:ET