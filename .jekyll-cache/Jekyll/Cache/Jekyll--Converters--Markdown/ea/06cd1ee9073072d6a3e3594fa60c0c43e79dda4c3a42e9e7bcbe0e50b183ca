I"$><p>This post is an overall summary of Chapter 8 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>1.1. Sigma Algebra</strong></p>

<p><strong>1.2. Expected Values</strong></p>

<p><strong>1.3. Moment Generating Functions</strong></p>

<p><strong>1.4. Characteristic Functions</strong></p>

<p><strong>1.5. Statistical Independence</strong></p>

<p><strong>1.6. Covariance and Correlation</strong></p>

<p><strong>1.7. Conditional Distributions</strong></p>

<p> </p>

<hr />

<h1 id="5-hypothesis-testing">5. Hypothesis Testing</h1>

<p> </p>

<hr />

<h2 id="51-basics-of-hypothesis-testing">5.1 Basics of Hypothesis Testing</h2>

<p> </p>

<h3 id="511-overview-of-statistical-testing-procedure">5.1.1. Overview of statistical testing procedure</h3>

<p>The typical and most clichéic setting of a statistical hypothesis testing is that from the observations $X_1, \dots, X_n \overset{i.i.d.}{\sim} f_\theta$, we want to test if $\theta = \theta_0$ or not.</p>

<p>A more concrete example is where we have a coin and we are interested in finding out whether the coin is fair (i.e. equal probability of heads and tails) or not. This procedure can formally be expressed as:</p>

<center>

$$
\begin{aligned}
X_1, \dots, X_n &amp;\overset{i.i.d.}{\sim} \text{Bernoulli}(p)\\[10pt]
H_0&amp;: p = \frac{1}{2} \\
H_1&amp;: p \neq \frac{1}{2}
\end{aligned}
$$

</center>

<p>To generalize this problem, we have two sets of parameters $\Theta_0$ and $\Theta_1$ which are <strong>non-overlapping</strong> (i.e. $\Theta_0 \cap \Theta_1 = \emptyset$), and would like to test the hypothesis:</p>

<center>

$$
\begin{aligned}
H_0&amp;: \theta \in \Theta_0 \\[5pt]
H_1&amp;: \theta \in \Theta_1
\end{aligned}
$$

</center>

<p>For the case when $\Theta_0$ is a single point, we call it as a <strong>simple null</strong>, whereas the more general case is refered to as a <strong>composite null</strong>.</p>

<p>As a general reminder, <strong>statistical hypothesis testing never guarentees optimality</strong>. Namely, the question is never if the null hypothesis is true or not. Rather, it is whether we have sufficient evidence to reject the null hypothesis or not, because the result of a hypothesis test is one of the two possibilities - “reject the null” or “retain the null”.</p>

<p> </p>

<h3 id="512-type-i-error--type-ii-error">5.1.2. Type I error &amp; Type II error</h3>

<p>Since the hypothesis testing is based on random samples, it is always prone to result in incorrect decisions. Regarding this, there are two important concepts in hypothesis testing called as <strong><em>Type I error</em></strong> and <strong><em>Type II error</em></strong>. At a high level, the more critical error among these two are the “type I error”, so often the strategy is to first bound the type I error at a desired level ($\alpha$) and minimize the type II error subsequently.</p>

<center>
  <img src="/images/mathstat/4.png" /> 
  <br />
  <em><span style="color:grey">Results of a statistical test</span></em>
</center>

<p> </p>

<h3 id="513-construction-of-tests">5.1.3. Construction of Tests</h3>

<p>In a nutshell, the canonical way of constructing a test is:
\(\begin{aligned}
&amp;\text{1. Choose a test statistic }T_n = T(X_1, X_2, \dots, X_n) \\[5pt]
&amp;\text{2. Choose a rejection region }R \\[5pt]
&amp;\text{3. If }T_n \in R, \text{ reject }H_0\text{ and otherwise retain }H_0 \\[5pt]
\end{aligned}\)</p>

<p> </p>

<h3 id="514-evaluating-tests">5.1.4. Evaluating Tests</h3>

<p>Then let’s discuss how to evaluate different tests.</p>

<p>Suppose our decision rule for a specific testing problem is that the null hypothesis is rejected when $T(X_1, \dots, X_n) \in R$.</p>

<p>In this setting, we can define the <strong>power function</strong> as:
\(\beta(\theta) = P_\theta\Big( (X_1, \dots, X_n) \in R \Big)\)
Naturally, we would want $\beta(\theta)$ to be small under the null hypothesis $\Theta_0$ and big over $\Theta_1$.</p>

<p>In this sense, the <strong>Neyman-Pearson paradigm</strong> is the following:
\(\begin{aligned}
&amp;\text{1. Fix the type I error  }\alpha \in (0,1) \\[5pt]
&amp;\text{2. Then try to maximize the power }\beta(\theta) \text{ over }\Theta_1, \text{ subject to } \underset{\theta \in \Theta_0}{\text{sup}} \beta(\theta) \leq \alpha
\end{aligned}\)
Tests of this form are called as <strong>level-alpha tests</strong> which guarentees the boundedness of type I error.</p>

<p> </p>

<h4 id="example-a-one-sided-test">Example: A One-sided Test</h4>

<p>Okay, so let’s quickly go over an example.</p>

<p>Suppose $X_1, \dots, X_n \overset{i.i.d.}{\sim} N(\theta, \sigma^2)$, with known $\sigma^2$.</p>

<p>We want to test the <em>one-sided alternative</em> such that:</p>

<center>

$$
\begin{aligned}
H_0&amp;: \theta = \theta_0 \\[5pt]
H_1&amp;: \theta &gt; \theta_0
\end{aligned}
$$

</center>

<p>A natural test statistic we can think of is the scaled average of the samples:</p>

<center> 

$$
T_n(X_1, \dots, X_n) = \frac{\bar X - \theta_0}{\sigma / \sqrt{n}}
$$

</center>

<p>Then, our strategy is to reject the null if $T_n &gt; t$ for some threshold $t$. To choose the optimal threshold, let’s consider the power function:</p>

<center> 

$$
\begin{aligned}
\beta(\theta) &amp;= P_\theta(T_n &gt; t) = P_\theta\Big( 
\frac{\bar X - \theta_0}{\sigma / \sqrt{n}} &gt; t \Big) \\[5pt]
&amp;= P\Big( \frac{\bar X}{\sigma / \sqrt{n}} &gt;  t + \frac{\theta_0}{\sigma/\sqrt{n}} \Big) \\[5pt]
&amp;= P\Big( \frac{\bar X - \theta}{\sigma / \sqrt{n}} &gt;  t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big) \\[5pt]
&amp;= P\Big( Z &gt; t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big), \quad\quad \because \frac{\bar X - \theta}{\sigma / \sqrt{n}} \sim N(0,1) \text{ by CLT} \\[5pt]
&amp;= 1 - \Phi\Big( t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big)
\end{aligned}
$$

</center>

<p>, where $\Phi$ is the cdf of standard normal distribution.</p>

<p>Then, following the Neyman-Pearson paradigm, we can define the optimal threshold $t$ that maximizes the power subject to:</p>

<center> 

$$
\begin{aligned}
&amp;\underset{\theta \in \Theta_0}{\text{sup}}\Big\{  1 - \Phi\Big( t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big) \Big\} \leq \alpha \\[10pt]
\Leftrightarrow \quad &amp;1 - \Phi(t) \leq \alpha \\[10pt]
\therefore \quad &amp;t = \Phi^{-1}(1-\alpha)
\end{aligned}
$$

</center>

<p>Similarly, for the <em>two-sided alternative</em> such that:</p>

<center> 

$$
\begin{aligned}
H_0&amp;: \theta = \theta_0 \\[5pt]
H_1&amp;: \theta \neq \theta_0
\end{aligned}
$$

</center>

<p>The decision rule becomes:</p>

<center> 

$$
|T_n| &gt; t
$$

</center>

<p>and the corresponding power function is:</p>

<center> 

$$
\beta(\theta) = P_\theta(T_n &lt; -t) + P_\theta(T_n &gt; t)
$$

</center>

<p>which as before can be expanded as:</p>

<center> 

$$
\begin{aligned}
\beta(\theta) &amp;= P\Big( \frac{\bar X - \theta}{\sigma / \sqrt{n}} &gt;  -t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big) + 
P\Big( \frac{\bar X - \theta}{\sigma / \sqrt{n}} &gt;  t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big) \\[10pt]
&amp;= \Phi\Big( -t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big) + 1 - \Phi\Big( t + \frac{\theta_0 - \theta}{\sigma/\sqrt{n}} \Big)
\end{aligned}
$$

</center>

<p>To implement the Neyman-Pearson paradigm under this setting, we just have to plug in the null parameter $\theta_0$ which gives us:</p>

<center> 

$$
\begin{aligned}
\beta(\theta_0) &amp;= \Phi(-t) + 1 - \Phi(t) \\[5pt]
&amp;= 2 \Phi(-t) \leq \alpha, \quad\quad \because \Phi(t) = 1 - \Phi(-t) \\[10pt]
\Leftrightarrow \quad t &amp;= -\Phi^{-1}(\alpha/2) = \Phi^{-1}(1 - \alpha/2)
\end{aligned}
$$

</center>

<p>To summarize what we’ve been doing so far, we have discussed how to set up a statistical hypothesis testing problem formally, and how we can find the optimal rejection threshold that controls the Type I error while at the same times attains the maximum power, i.e. Neyman-Pearson paradigm.</p>

<p> </p>

<h2 id="52-neyman-pearson-test">5.2 Neyman-Pearson Test</h2>

<p>Next, let’s discuss a general method for constructing an <strong>optimal test</strong>. For simplicity, we will focus on a simple hypothesis testing problems, but the general idea also applies to the more extended settings as well.</p>

<p> </p>

<h3 id="521-simple-hypothesis">5.2.1. Simple hypothesis</h3>

<p>The setting we are going to consider is where both the null and alternate hypotheses are simple:</p>

<center>

$$
\begin{aligned}
H_0&amp;: \theta = \theta_0 \\[5pt]
H_1&amp;: \theta = \theta_1
\end{aligned}
$$

</center>

<p>Furthermore, let’s denote the null density as $f_0$ and the alternate density as $f_1$.</p>

<p> </p>

<h3 id="523-power-of-a-statistical-test">5.2.3. Power of a statistical test</h3>

<p>To recap, the power of a test is defined as <strong>the probability of correctly rejecting the null hypothesis when the alternative hypothesis is true</strong>. This is by definition, the inverse probability of a type II error $\beta$.</p>

<center>

$$
1 - \beta = P\Big(\text{reject } H_0 \;\Big|\; H_1 = \text{True} \Big)
$$

</center>

<p>On the other hand, remember that our primary goal was to bound the type I error to a certain level of <strong>significance level</strong> $\alpha$ in advance. Since type I error and type II error have tradeoff relationship, we can infer that an “optimal” test will be the case <strong>when the type I error actually attains the upper bound</strong> (i.e. significance level). We will see how we can make this happen in a second, but first let’s define some terminologies:</p>

<center>

$$
\begin{aligned}
&amp;\text{Define test function }\phi(\mathbf{x}) \text{ such that: }\quad 
\phi(\mathbf{x})  = \begin{cases} 1, \quad \text{if }\mathbf{x} \in R  \\[7pt]
0, \quad \text{if }\mathbf{x} \notin R \end{cases} \\[15pt]
&amp;\Rightarrow \quad \text{power} = \int \phi(\mathbf{x}) f_1(\mathbf{x})d\mathbf{x} \\[5pt]
&amp;\Rightarrow \quad \text{size} = \int \phi(\mathbf{x}) f_0(\mathbf{x})d\mathbf{x}
\end{aligned}
$$

</center>

<p>, where $R$ is the rejection region.</p>

<p> </p>

<h3 id="524-the-neyman-pearson-testing-procedure">5.2.4 The Neyman-Pearson testing procedure</h3>

<p>The key takeaway of the Neyman-Pearson procedure is that it provides a “general recipe” for creating an optimal test - <strong>to make the <em>type I error</em> equal to the significance level</strong>, allbeit it is only applied to the restricted class of simple hypothesis tests.</p>

<p>For $\mathbf{x} = (x_1, \dots, x_n)$, the Neyman-Pearson test statistic is to take the likelihood ratio such that:</p>

<center>

$$
\Lambda(\mathbf{x}) = \frac{L(\theta_0 | \mathbf{x})}{L(\theta_1 | \mathbf{x})} = \frac{f_0(\mathbf{x})}{f_1(\mathbf{x})}
$$

</center>

<p>and reject the null hypothesis if the value is within the rejection region.</p>

<p>Since we want our test to be “optimal”, we can define the threshold for the rejection region such that the t<em>ype I error</em> attains the upper bound (significance level):</p>

<center>

$$
P_{H_0}\Big(\Lambda(\mathbf{x}) \leq t^* \Big) = \alpha
$$

</center>

<p>Then for a simple hypothesis testing problem, we can analytically find the threshold $t^*$, which guarentees the optimality of the test. So in a nutshell, this kind of test is the best we can think of whenever it is applicable. There is a theoretical foundation of this result, which is called as the <strong>Neyman-Pearson Lemma</strong>.</p>

<p> </p>

<h3 id="525-the-neyman-pearson-lemma">5.2.5. The Neyman-Pearson Lemma</h3>

<p>Let’s formally state the Neyman-Pearson lemma.</p>

<p><strong>&lt;Definition&gt;</strong></p>

<center>

$$
\begin{aligned}
&amp;\text{Consider a test with hypotheses } H_0:\theta=\theta_0 \text{ and }H_1:\theta=\theta_1, \text{ where the pdf (of pmf) is } f_i(\mathbf{x}) \quad (i\in\{0,1\}). \\[10pt]
&amp;\text{The Neyman-Pearson test is defined such that} \\[5pt]
&amp;: \phi_{NP}(\mathbf{x}) = \mathbb{1}\Big( \frac{f_0(\mathbf{x})}{f_1(\mathbf{x})} \leq t^* \Big), \quad \text{where }t^* \text{ is chosen to satisfy }\int \phi_{NP}(\mathbf{x}) f_0(\mathbf{x}) d\mathbf{x} = \alpha. \\[10pt]
&amp;\text{Then the power of }\phi_A(\mathbf{x}) \text{ is at most the power of the Neyman-Pearson test, that is} \\[5pt]
\quad&amp;\Rightarrow \quad \int \phi_{NP}(\mathbf{x}) f_1(\mathbf{x}) d\mathbf{x} \geq \int \phi_{A}(\mathbf{x}) f_0(\mathbf{x}) d\mathbf{x} \\[15pt]
&amp;\therefore \quad \text{NP test is optimal}
\end{aligned}
$$

</center>

<p><strong>&lt;Proof&gt;</strong></p>

<p>First, let’s clarify some notations:</p>

<center>

$$
\begin{aligned}
&amp;\phi_{NP}(\mathbf{x}) = \mathbb{1}\Big(\mathbf{x} \in C\Big), \\[5pt]
&amp;\Rightarrow C = \Big\{ x: \frac{f_0(\mathbf{x})}{f_1(\mathbf{x})} \leq t^* \Big\} \text{ and }\int_C f_0(\mathbf{x}) d\mathbf{x} = \alpha \quad (\cdots \;\text{NP test}) \\[15pt]

&amp;\phi_{A}(\mathbf{x}) = \mathbb{1}\Big(\mathbf{x} \in A\Big), \\[5pt]
&amp;\Rightarrow \int_A f_0(\mathbf{x}) d\mathbf{x} \leq \alpha \quad (\cdots \;\text{level }\alpha \text{ test})

\end{aligned}
$$

<center>

Our goal is to show that the power of NP test is always greater than any level $\alpha$ tests:

<center>

$$
\Leftrightarrow \quad \int_C f_1(\mathbf{x}) d\mathbf{x} \geq \int_A f_1(\mathbf{x}) d\mathbf{x} \quad\cdots\quad (*)
$$

</center>

Then, we use the property of set operations, namely:

<center>

$$
\begin{aligned}
C &amp;= C \cap \Big[A \cup A^c\Big] = \Big[C \cap A\Big] \cup \Big[C \cap A^c\Big] \\[5pt]
A &amp;= \Big[ A \cap C \Big] \cup \Big[ A \cap C^c \Big]
\end{aligned}
$$

</center>

Thus,

<center>
$$
\begin{aligned}
(*) &amp;= 
\int_{C \cap A} f_1(\mathbf{x})d\mathbf{x} + 
\int_{C \cap A^c} f_1(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C} f_1(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C^c} f_1(\mathbf{x})d\mathbf{x} 
\geq 0 \\[15pt]

&amp;\Leftrightarrow \quad 
\int_{C \cap A^c} f_1(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C^c} f_1(\mathbf{x})d\mathbf{x}
\geq 0 \\[15pt]

\end{aligned}
$$

$$
\begin{aligned}
&amp;\text{If } \mathbf{x} \in C, \; \\[5pt]
&amp; \Rightarrow \quad \frac{f_0(\mathbf{x})}{f_1(\mathbf{x})} \leq t^* \\[5pt]
&amp; \Leftrightarrow \quad f_1(\mathbf{x}) \geq \frac{f_0(\mathbf{x})}{t^*} \\[20pt]

&amp;\text{Likewise if } \mathbf{x} \in C^c, \; \\[5pt]
&amp; \Rightarrow f_1(\mathbf{x}) &lt; \frac{f_0(\mathbf{x})}{t^*}

\end{aligned} \\[10pt]
$$

$$
\begin{aligned}
\therefore \quad

\int_{C \cap A^c} f_1(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C^c} f_1(\mathbf{x})d\mathbf{x}
&amp;\geq
\frac{1}{t^*}\Big[ \int_{C \cap A^c} f_0(\mathbf{x})d\mathbf{x} - \int_{A \cap C^c} f_0(\mathbf{x})d\mathbf{x} \Big] \\[10pt]
&amp;\geq 
\frac{1}{t^*}\Big[
\int_{C \cap A^c} f_0(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C} f_0(\mathbf{x})d\mathbf{x} + 
\int_{C \cap A} f_0(\mathbf{x})d\mathbf{x} - 
\int_{A \cap C^c} f_0(\mathbf{x})d\mathbf{x}
\Big] \\[10pt]
&amp;= 
\frac{1}{t^*}\Big[
\Big(
\int_{C \cap A^c} f_0(\mathbf{x})d\mathbf{x} + \int_{C \cap A} f_0(\mathbf{x})d\mathbf{x} 
\Big)
- 
\Big( 
\int_{A \cap C^c} f_0(\mathbf{x})d\mathbf{x} + \int_{A \cap C} f_0(\mathbf{x})d\mathbf{x}
\Big)
\Big] \\[10pt]
&amp;= 
\frac{1}{t^*}\Big[
\int_C f_0(\mathbf{x})d\mathbf{x} - \int_A f_0(\mathbf{x})d\mathbf{x}
\Big] \\[10pt]
&amp;= \frac{1}{t^*}\Big(\alpha - \int_A f_0(\mathbf{x})d\mathbf{x} \Big) \\[10pt]
&amp;\geq 0 \quad\quad (\because \int_A f_0(\mathbf{x}) d\mathbf{x} \leq \alpha)

\end{aligned}
$$

which completes the proof.



## 5.3. Wald Test





---

</center>

&nbsp;

---

# Reference

- Casella, G., &amp; Berger, R. L. (2002). *Statistical inference.* 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.

</center></center>
:ET