I"∑<p>¬†</p>

<p>This post is an overall summary of different testing methodologies used in clinical trials.</p>

<p><a href="#1-one-sample-t-test">1. One-Sample T-test</a></p>

<p><a href="#2-two-sample-t-test">2. Two-Sample T-test</a></p>

<hr />

<h1 id="1-one-sample-t-test">1. <strong>One-Sample T-test</strong></h1>

<p><em>One-Sample T-test</em> is used to infer whether an <strong>unknown population mean</strong> differs from a <strong>hypothesized value</strong>.</p>

<p><strong>&lt;Assumptions&gt;</strong></p>

<ul>
  <li>i.i.d. samples from normal distribution with unknown mean Œº.</li>
</ul>

<p><strong>&lt;Test Setting&gt;</strong></p>

<center>
  <img src="/images/tests/ttest1.png" width="500" height="300" /> 
</center>

<p>¬†</p>

<p><strong>&lt;Takeaways&gt;</strong></p>

<ul>
  <li>A special case of the <em>one-sample t-test</em> is to determine whether a mean response changes under different experimental conditions by using ‚Äúpaired observations‚Äù ($\mu_d$=the mean difference, $\mu_0$= 0 , $y_i$‚Äôs = the paired-differences).</li>
  <li>It can be shown that the <em>t-test</em> is equivalent to the <strong><em>Z-test</em></strong> for <strong>infinite degrees of freedom</strong>. In other words, Z-test is equivalent to t-test in the limit of sample size. In practice, a ‚Äòlarge‚Äô sample is usually considered to be $n$ ‚â• 30.</li>
  <li>If the assumption of <strong>normality</strong> cannot be guarenteed, the ‚Äúmean‚Äù might not be the best measure of central tendency. In such cases, a non- parametric test such as the <em>Wilcoxon signed-rank test</em> might be more appropriate choice.</li>
  <li>A non-significant result does not necessarily imply that the null hypothesis is true. It only asserts <strong>insufficient evidence</strong> for contradicting the original statement.</li>
  <li>Statistical significance <strong>does not imply causality</strong> in observational studies, only for <strong>‚Äúrandomized‚Äù</strong> controlled trials (RCT).</li>
</ul>

<hr />

<h1 id="2-two-sample-t-test">2. <strong>Two-sample T-test</strong></h1>

<p><em>Two-sample T-test</em> is used to compare the means of two independent populations, denoted as $\mu_1$ and  $\mu_2$.</p>

<p>It has ubiquitous application in the analysis of controlled clinical trials.</p>

<p><strong>&lt;Assumptions&gt;</strong></p>

<ul>
  <li>The populations are normally distributed.</li>
  <li>Homogeneity of variance - the populations share the same variance $\sigma^2$.</li>
</ul>

<p><strong>&lt;Test Setting&gt;</strong></p>

<center>
  <img src="/images/tests/ttest2.png" width="500" height="300" /> 
</center>

<center>
  <img src="/images/tests/ttest2-1.png" width="250" height="150" /> 
</center>

<p>¬†</p>

<p><strong>&lt;Takeaways&gt;</strong></p>

<ul>
  <li>The assumption of equal variances can be tested using the <strong><em>F-test</em></strong>:</li>
</ul>

<center>
  <img src="/images/tests/ttest2-2.png" width="500" height="300" /> 
</center>

<ul>
  <li>If the assumption of equal variances is rejected, a slight modification of the <em>t-test</em> called as the <a href="https://en.wikipedia.org/wiki/Welch%27s_t-test"><em>Welch‚Äôs t-test</em></a>:</li>
</ul>

<center>
  <img src="/images/tests/ttest2-3.png" width="500" height="300" /> 
</center>

<center>
  <img src="/images/tests/ttest2-4.png" width="200" height="100" /> 
</center>

<ul>
  <li>The assumption of normality can formally be checked by the <em><a href="https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test">Shapiro-Wilk test</a></em> or the <em><a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov test</a></em>.</li>
  <li>For large enough samples, <strong>CLT (central limit theorem)</strong> holds. However, if the data is heavily skewed such that normal approximation guarenteed by CLT is not sufficiently accurate, we have to consider changing test statistic from mean to median and use <em>Wilcoxon rank-sum test</em> instead.</li>
</ul>

<hr />

<h1 id="3-one-way-anova">3. <strong>One-way ANOVA</strong></h1>

<p><em>One-way ANOVA</em> is used to simultaneously compare two or more group means based on independent samples from each group.</p>

<p><strong>&lt;Assumptions&gt;</strong></p>

<ul>
  <li>normally distributed data</li>
  <li>samples are independent w.r.t. each group</li>
  <li><strong>variance homogeneity</strong>, meaning that the within-group variance is constant across groups. This can be expressed as $\sigma_1 = \sigma_2 = \dots = \sigma_k = \sigma$</li>
</ul>

<p><strong>&lt;Test Setting&gt;</strong></p>

<center>
  <img src="/images/tests/anova1.png" width="500" height="300" /> 
</center>

<p><strong>&lt;Remarks&gt;</strong></p>

<ul>
  <li>Intuitively, if the ‚Äú<em>F</em> statistic‚Äù that is roughly the ratio of <strong><em>between-group variability</em></strong> and <strong><em>within-group variability</em></strong> is far from 1, it is evident that the group means indeed differ.</li>
  <li>
    <p>On the other hand, <em>F</em> statistic will be close to 1, if the variation <strong>among groups</strong> and the variation <strong>within groups</strong> are independent estimates of the same measurement variation, $\sigma^2$.</p>
  </li>
  <li>
    <p>In this sense, <em>MSG</em> is an estimate of the variability ‚Äúamong groups‚Äù, and <em>MSE</em> is an estimate of the variability ‚Äúwithin groups‚Äù.</p>
  </li>
  <li>The parameters associated with the <em>F</em>-distribution are the upper and lower degrees of freedom.</li>
  <li>
    <p>An ‚ÄúANOVA table‚Äù is the conventional method of summarizing the results (shown above).</p>
  </li>
  <li>
    <p>Similar to t-test, <em>Shapiro-Wilk test</em> or <em>Kolmogorov-Smirnoff test</em> can be used to check the normality assumption.</p>
  </li>
  <li>
    <p>For variance homogeneity assumption, <em><a href="https://en.wikipedia.org/wiki/Levene%27s_test">Levene‚Äôs test</a></em> or <em><a href="https://en.wikipedia.org/wiki/Bartlett%27s_test">Bartlett‚Äôs test</a></em> can be used.</p>
  </li>
  <li>
    <p>When <strong>comparing</strong> <strong>more than two means</strong> ($k$ &gt; 2), a significance of the <em>F-test</em> indicates that at least one pair of means are different, but it doesn‚Äôt tell us which specific pairs are. Therefore, if the null hypothesis is rejected, further analysis should be taken to investigate where the differences lie.</p>
  </li>
  <li>95% confidence intervals can also be obtained for the mean difference between any pairs of groups (e.g, Group <em>i</em> vs Group <em>j</em>) by the formula:</li>
</ul>

<center>
  <img src="/images/tests/anova2.png" width="300" height="150" /> 
</center>

<ul>
  <li>If there are <strong>only two groups (k = 2)</strong>, the p-value for Group effect using an <strong><em>ANOVA</em></strong> is the same as that of a standard <strong><em>two-sample t-test</em></strong>. This is because the F and t-distributions enjoy the relationship that, with 1 upper degree of freedom, the <strong>F-statistic is the square of the t-statistic</strong>. When k = 2, the <strong>MSE</strong> in <em>ANOVA</em> is identical to the pooled variance in the <em>two-sample t-test</em>.</li>
</ul>

<hr />

<p>¬†</p>

<p>¬†</p>

<p>¬†</p>

<center>
  <img src="/images/tests/ttest1.png" width="500" height="300" /> 
</center>

<hr />

:ET