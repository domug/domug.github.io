I"?<p>데이터 마이닝에서 앙상블 기법은 다양한 알고리즘들의 성능을 향상시키기 위해 활용될 수 있는 간단하면서도 효과적인 방법이다. 앙상블 기법의 핵심적인 아이디어는 서로 다른 알고리즘 (base detector)의 결과를 하나로 합쳐서 최종적인 결과를 도출하려는 것인데, 이는 데이터셋에서 각 알고리즘들이 성능이 좋은, 이른바 “특화된” 부분이 있다는 전제를 바탕으로 여러 알고리즘의 결과를 통해 더 나은 결론을 도출할 수 있다는 이른바 “<a href="https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds">대중의 지혜 (the wisdom of crowds)</a>“를 그 기반으로 한다. 이러한 앙상블 기법은 클러스터링, 분류, 이상치 탐지, 추천 시스템 등 데이터 마이닝의 다양한 분야 및 시계열, 네트워크, 평점 데이터 등 광범위한 데이터 도메인에서 성공적으로 활용되고 있다.</p>

<p>한편, 앙상블 기법이 이상치 탐지에 활용되기 시작한 것은 다른 분야에 비해 상대적으로 그 역사가 길지 않다. 앙상블 기법이 가장 효과적으로 사용되는 분류 (classification) 도메인과 비교했을 때, 이에 대한 단편적인 이유는 바로 <strong>“라벨값의 부재”</strong>이다. 가령, 주어진 라벨값을 학습에 활용하는 부스팅 등의 supervised ensemble algorithm과는 다르게, 이상치 탐지를 위해 개발된 모형들은 그 특성상 라벨, 즉 response가 없다는 가정 아래에서 (fully) unsupervised learning을 수행한다. 이처럼 ground-truth를 알 수 없는 환경에서는 앙상블을 학습의 장점이라고 할 수 있는 특정한 알고리즘에서의 부족한 부분을 다른 알고리즘이 집중적으로 학습하여 보완할 수 있다는 점이 상대적으로 덜 부각된다.</p>

<p>하지만 이러한 태생적인 한계점에도 불구하고, 이상치 탐지에 앙상블 기법을 적용하는 것은 분명한 장점이 있다. 그 대표적인 사례는 바로 <a href="https://domug.github.io/2022/08/04/OD3/">이전 포스트</a>에서 살펴보았던 “subspace outlier detection” 인데, 특정한 아웃라이어의 탐지에 특화된 부분 공간 여러개의 결과를 하나로 합치는 것을 통해 좀 더 강건한 (robust) 결과를 얻을 수 있다는 점은 여러 선행 연구들에서 경험적으로 입증된 사실이다. 이러한 맥락에서 앙상블 학습을 통해 비지도 학습으로 인한 각 알고리즘들의 “불확실성”을 줄일 수 있다는 점은 상당히 매력적이라고 할 수 있다.</p>

<p>이러한 맥락에서 해당 포스트에서는 이상치 탐지에서 활용되는 앙상블 학습의 디자인과 몇가지 대표적인 알고리즘들에 대해 살펴볼 예정이다.</p>

<p> </p>

<hr />

<h1 id="1-categorization-of-ensemble-methods">1. Categorization of Ensemble Methods</h1>

<p>우선 첫 번째로, 앙상블 기법은 어느 부분에서 다양성, 즉 앙상블이 발생하는지에 따라서 크게 다음의 두가지의 카테고리로 분류될 수 있다.</p>

<ol>
  <li><strong>Model-centric ensembles</strong>
    <ul>
      <li>해당 앙상블은 이상치 탐지 모형의 관점에서 다양성이 발생한다.</li>
      <li>e.g. 서로 다른 독립적인 모형의 결과를 앙상블, 동일한 모형의 다른 하이퍼파라미터 셋팅에서의 결과를 수합</li>
    </ul>
  </li>
  <li><strong>Data-centric ensembles</strong>
    <ul>
      <li>해당 앙상블은 모형의 인풋값, 즉 데이터의 관점에서 다양성이 발생한다.</li>
      <li>간단히 말하자면 주어진 데이터를 여러 형태로 변형해서 각각에 대해 모형을 학습시키는 것이다.</li>
      <li>e.g. data subsampling, subspace sampling, random projections, noise addition, …</li>
    </ul>
  </li>
</ol>

<p> </p>

<p>이와 비슷하지만 약간 다른 맥락에서, 앙상블에 사용되는 base detector 간의 관계성에 따라 다음의 두 카테고리로 앙상블 기법을 분류하기도 한다.</p>

<ol>
  <li><strong>Independent ensembles</strong>
    <ul>
      <li>해당 앙상블에서 base detector들의 학습은 완벽히 독립적으로 수행된다. 즉, 이전 모형의 학습 결과가 이후 모형의 학습에 영향을 미치지 않는다.</li>
      <li>이상치 탐지와 관련한 대부분의 앙상블 알고리즘들이 해당 카테고리에 속한다고 할 수 있다.</li>
    </ul>
  </li>
  <li><strong>Sequential ensembles</strong>
    <ul>
      <li>특정한 base detector가 학습되는 과정에서 이전 단계의 결과를 참고해서, 더 나은 결과를 유도한다.</li>
      <li>이것의 대표적인 사례는 분류 도메인의 부스팅 기법이라고 할 수 있다.</li>
      <li>이상치 탐지에서는 비록 그 사례가 적으나, 이전 단계에서 “이상치”로 판별된 데이터들을 이후 단계의 학습에서 제외시키는 것을 통해 “normal”인 데이터만으로 구축된 모형을 만드는 “OOD (out-of-distribution) Detection” 등이 있다.</li>
    </ul>
  </li>
</ol>

<p> </p>

<p>이러한 분류는 서로 배타적이 아니며, 방법론의 성격에 따라 “Independent ensemble” 모형이 “model-centric”과 “data-centric” 두가지 전부에 속할 수도 있다 (e.g. Isolation Forest). 이는 단지 이상치 탐지와 관련해서 앙상블 학습을 어떻게 설계할 수 있으며, 그에 따른 특징이 어떤지를 알아보기 위한 것이다. 이에 대한 좀 더 디테일한 논의는 해당 <a href="https://link.springer.com/book/10.1007/978-3-319-54765-7">텍스트북</a>에 나와있다.</p>

<p> </p>

<hr />

<h1 id="2-design-of-ensemble-methods">2. Design of Ensemble Methods</h1>

<p>다음으로는 앙상블 방법의 디자인, 즉 서로 다른 base detector의 결과를 어떤 방법으로 수합할지에 대해서 논의를 이어가보자.</p>

<p> </p>

<h3 id="21-score-normalization">2.1. Score Normalization</h3>

<p>노테이션의 편의상, $N \times d$  크기의 데이터셋 $D$ 를 바탕으로 학습된 $m$ 번째 base detector $B_m$에 대해, 이로부터 얻어진 $i$ 번째 데이터 포인트 $D_{(i)}$에 대한 outlier score를 $s_m(i)$ 라고 정의하자. 즉, 모든 $D_{(i)}, \; (i=1,\dots, N)$ 에 대해서 $s_1(i), \dots, s_m(i)$ 의 총 $m$ 개의 outlier score가 얻어진다. 이 때 앙상블 학습의 결과는 이로 부터 하나의 통일된 outlier score $S(i)$ 를 반환하게 된다.</p>

<p>이 과정에서 만약 각각의 base detector가 서로 다른 범위의 outlier score를 반환할 경우, 이를 일정한 기준을 바탕으로 표준화시키는 작업, 즉 <strong>“score normalization”</strong> 이 필요하다. 예를 들어, 첫 번째 base detector $B_1$으로는 LOF (local outlier factor)가, $B_2$로는 GMM (gaussian mixture model)이 사용되었다고 가정해보자. 이 때, LOF가 반환하는 outlier score는 거리에 대한 척도로서, 그 값이 크면 클수록 이상치일 가능성이 높아진다. 반면에, GMM의 outlier score는 (log) likelihood 값으로, 그 값이 작은 엔트리들을 이상치로 판단한다. 이처럼 애당초 부호가 상반되는 경우 이외에도, LOF처럼 normalized된 outlier score값을 반환하는 알고리즘과 kNN처럼 스케일링이 되지 않은 outlier score 반환하는 알고리즘이 있기 때문에 다양한 base detector로 부터 얻어진 값들을 일정한 범위로 표준화 하는 작업은 성공적인 앙상블 학습을 위해 필수적인 작업이라고 할 수 있다.</p>

<p>이와 관련해서, 가장 일반적인 표준화 방법으로는 “Min-max scaling”과 “Z-transformation”이 있다.</p>

<ul>
  <li><strong>Min-max scaling</strong></li>
</ul>

<center>

$$
S_j(i) = \frac{s_j(i) - min_j}{max_j - min_j}
$$

</center>

<ul>
  <li><strong>Z-transformation</strong></li>
</ul>

<center>

$$
S_j(i) = \frac{s_j(i) - \mu_j}{\sigma_j}
$$

</center>

<p>둘 방법간 우위를 직접적으로 비교할 수는 없으나, 일반적으로 후자가 조금 더 안정적인 스코어를 얻을 수 있다는 점에서 더 선호되는 것으로 보인다. 그 이유는 Min-max scaling의 경우 outlier score가 최대값과 최소값에 크게 의존할 수 밖에 없는데, “이상치”의 특징을 고려할 때 이러한 극단적인 값들은 수치적으로 상당히 불안정할 수 있기 때문이다. 이 밖에도 outlier score를 직접적인 확률값으로 변환시키려는 아이디어가 <a href="(https://www.cse.msu.edu/~ptan/papers/ICDM2.pdf)">해당 논문</a>에서 제안되었다.</p>

<p> </p>

<h3 id="22-score-combination">2.2. Score Combination</h3>

<p>한편, score normalization을 통해 각 base detector마다 통일된 범위 내의 outlier score를 얻었다면, 그 다음으로는 해당 값들을 어떻게 하나의 지표로 압축할지에 대한 논의가 필요하다. 이러한 맥락에서 고려해볼 수 있는 접근법은 크게 “평균”, “최대값”, “랭크”를 이용하는 것이다.</p>

<ol>
  <li>
    <p><strong>Score Averaging</strong></p>

    <center>

$$
\text{Average}(i) = \frac{\sum_{j=1}^m S_j(i)}{m}
$$

   

</center>

    <ul>
      <li>이 경우, 최종적인 outlier score는 개별적인 base detector의 값들을 평균낸 것으로 정의된다.</li>
      <li>해당 접근법의 장점은 통계적으로 더 낮은 분산을 갖는 outlier score를 얻을 수 있다는 점이다.</li>
    </ul>

    <p> </p>
  </li>
  <li>
    <p><strong>Score Maximization</strong></p>

    <center>

$$
\text{Maximum}(i) = \text{max}_{j=1}^m S_j(i)
$$

</center>

    <ul>
      <li>
        <p>이 경우, 최종적인 outlier score는 개별적인 base detector에서의 최대값으로 정의된다.</p>
      </li>
      <li>
        <p>이는 각 base detector마다 특화된 이상치들이 있을 것이라는 가정을 전제로 하는데, 따라서 base detector의 개별적인 성능이 상당히 중요해진다.</p>
      </li>
      <li>
        <p>또한 이상치의 특성을 고려할 때, 최대값이라는 지표는 outlier score의 분산을 증가시켜 안정성이 떨어질 수 있다는 우려가 있다.</p>

        <p> </p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Score Ranking</strong></p>

    <ul>
      <li>이 경우, 개별적인 base detector</li>
    </ul>
  </li>
</ol>

<p> </p>

<hr />

<h1 id="3-theoretical-foundations-of-outlier-ensembles">3. Theoretical Foundations of Outlier Ensembles</h1>

<p>다음으로는 개념적인 측면에서 이상치 탐지 과제에 앙상블 학습을 적용하는 것이 갖는 의의에 대해서 살펴보자. 다시 말해, 앙상블 학습이 도대체 왜 (일반적으로) 더 나은 결과를 보장하는지에 대한 단서를 찾아보고자 하는 것이다.</p>

<p> </p>

<h3 id="31-bias-variance-tradeoff-in-supervised-analysis">3.1. Bias-variance tradeoff in Supervised Analysis</h3>

<p>이를 위해 우리는 분류, 클러스터링 등의 supervised data mining 분야에서 등장하는 개념인 <strong>“bias-variance tradeoff”</strong> 에 주목할 필요가 있다. 이에 대한 구체적인 설명은 <a href="https://domug.github.io/2020/12/03/Statistical_Inference/">이전의 포스트</a>에서 다룬적이 있는데, 일반적인 분류 문제를 예로 들 때 “bias”는 학습에 사용된 데이터를 얼마나 잘 설명하는지를 판단하는 지표이며 (i.e. overfitting), “variance”는 해당 모형이 학습에 사용되지 않은 데이터에서 얼마나 잘 일반화되는지를 나타내는 지표라고 이해하면 편하다. 이와 관련해 일반적으로 bias와 variance를 둘 다 감소시키는 것은 불가능하다는 점에서 (tradeoff relationship), 지도 학습을 통해 알고리즘은 bias와 variance 간의 적절한 밸런스를 갖추도록 설계된다. 즉, bias를 최소화하기 위해 훈련용 데이터를 바탕으로 모형을 최적화하는 동시에, 학습에 사용되지 않은 테스트 데이터에서의 성능을 평가함으로서 variance를 낮추려고 하는 것이다.</p>

<p>이처럼 supervised algorithm에서는 주어진 데이터의 <strong>“라벨”</strong>에 대한 정보를 적극적으로 활용해서 bias-variance의 최적화를 수행한다. 한편, 우리의 관심사인 <strong>이상치 탐지 과제에서는 이러한 식의 bias-variance에 대한 최적화가 불가능하다</strong>. 그 이유는 앞서 수차례 언급한 것처럼 이상치 탐지 과제의 본질이 <strong>비지도학습</strong>을 기반으로 하기 때문이다. 이처럼 어떤 것이 이상치인지에 대한 라벨이 부재한 상황 속에서는 만들어진 이상치 탐지 모형에 대한 bias와 variance를 객관적으로 평가하는 것이 불가능해진다.</p>

<p> </p>

<h3 id="32-bias-variance-tradeoff-in-outlier-analysis">3.2. Bias-variance tradeoff in Outlier Analysis</h3>

<p>그렇다면 다음으로는 이러한 이슈가 앙상블 기법과 어떠한 관련이 있는지에 대해서 살펴보도록 하겠다.</p>

<p>모든 이상치 탐지 과제는 주어진 데이터의 <strong>정상적인 생성 과정 (generative process)</strong> 을 모델링한 “모형”을 바탕으로 수행된다. 이를 수식을 사용해서 정의하자면, 주어진 데이터 $D$ 에 대한 정상적인 생성 과정 $f(\cdot)$ 을 효과적으로 카피할 수 있는 모형 $g(\cdot)$ 를 만드는 것이다. 이 때 true model 인 $f(\cdot)$ 은 우리가 절대로 알 수 없는 이른바 “true oracle” 이다.</p>

<p>한편, 이 과정에서 모형의 성능을 평가하기 위한 총 $n$개의 테스트 데이터 $\bar X_1, \cdots, \bar X_n$가 있다고 “가정”해보자. 물론 비지도 학습의 특성상 이러한 테스트 데이터들은 실제로 관측되지 않는다. 그럼에도 불구하고 해당 값들이 “이론적으로는” 존재한다고 생각해 볼 수 있다. 이러한 테스트 데이터들은 저마다 true model인 $f(\cdot)$ 에서의 outlier score $f(\bar X_i)$ 와, 우리가 만들어낸 모형 $g(\cdot)$ 에서의 outlier score $g(\bar X_i \;; D)$ 를 가질 것이다.</p>

<p>자, 이제 우리의 관심사는 $g(\bar X_i \; ; D)$ 가 $f(\bar X_i)$ 에 비해 얼마나 차이가 나는지를 파악하는 것이다. 이에 대한 지표로 MSE (mean squared error)는 다음과 같이 정의된다.</p>

<center> 

$$
MSE = \frac{1}{n}\sum_{i=1}^n \Big( f(\bar X_i) - g(\bar X_i \; ; D)  \Big)^2
$$

</center>

<p>한편, 여기서 주의해야 할 점은 바로 위 MSE가 특정한 데이터셋 $D$ 에 대한 지표라는 점이다. 우리가 모형의 학습에 사용한 $D$ 는 $f(\cdot)$ 라는 generative process를 바탕으로 생성 (realized) 된 $N$개의 데이터이다. 이 말인 즉슨, $D$ 는 오직 특정한 한 시점의 데이터셋이며, $f(\cdot)$ 으로 부터 $N$개의 데이터를 생성하는 과정을 여러번 반복할 경우 그 결과로 얻어지는 replication $D_1, D_2, \dots, D_\infty$ 은</p>

<p>우리가 정말로 관심이 있는 값은</p>

<p> </p>

<p> </p>

<hr />

<center>
  <img src="/images/abtest/32.png" width="700" height="500" /> 
 <br />
 <em><span style="color:grey"></span></em>
</center>

<p> </p>

<p>##</p>

<p> </p>

<p> </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Aggarwal, C. C. (2013). <em>Outlier Analysis</em>. Springer. ISBN: 978-1-4614-6396-2</li>
  <li>C. C. Aggarwal and S. Sathe. Outlier Ensembles: An Introduction. Springer, 2017.</li>
  <li>J. Gao and P.-N. Tan. Converting Outlier Scores from Outlier Detection Algorithms into Probability Estimates. ICDM Conference, 2006.</li>
  <li>
    <p>Z.-H. Zhou. Ensemble Methods: Foundations and Algorithms. Chapman and</p>

    <p>Hall/CRC Press, 2012.</p>
  </li>
</ul>

:ET