I"íD<h3 id="previous-posts">Previous Posts</h3>

<p><a href="https://domug.github.io/2021/02/09/BS1/">1. Bayesian Statistics - Basics</a></p>

<p><a href="https://domug.github.io/2021/02/15/BS2/">2. Bayesian Statistics - Conjugacy</a></p>

<p><a href="https://domug.github.io/2021/02/20/BS3/">3. Bayesian Statistics - Hierarchical Models and Monte Carlo Simulation</a></p>

<hr />

<p>¬†</p>

<h1 id="when-and-why-do-we-need-mcmc">When and why do we need MCMC?</h1>

<p>In the previous post, we saw that we cannot derive the posterior analytically when we have the following setup of prior and likelihood.</p>

<p>¬†</p>

<center>

$$
likelihood:\;\;\;\;y_i | \mu,\sigma^2 \stackrel{iid}{\sim} N(\mu, \sigma^2)\\
prior\;of\;\mu: \;\;\;\; \mu|\sigma^2 \sim N(\mu_0, \frac{\sigma^2}{w_0}) \\
prior\;of\;\sigma: \;\;\;\;\ \sigma^2 \sim IG(\nu_0, \beta_0)
$$

</center>

<p>¬†</p>

<p>Although we know we can‚Äôt, let‚Äôs try it out to examine where exactly we get stuck.</p>

<p>¬†</p>

<center>

$$
\begin{aligned}
f(\mu, \sigma^2 | y_1,...,y_n) &amp;\propto f(y_1,...,y_n|\mu,\sigma^2)f(\mu|\sigma^2)f(\sigma^2) \\[10pt]

&amp;= \prod_{i=1}^n\{N(y_i|\mu,\sigma^2)\} \; N(\mu|\mu_0, \frac{\sigma^2}{w_0}) \; IG(\sigma^2|\nu_0, \beta_0) \\[10pt]

&amp;= \prod_{i=1}^n\{\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(y_i-\mu)^2)\} 
\{\frac{1}{\sqrt{2\pi\sigma_0^2}}exp(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2)\}
\{\frac{\beta_0^{\nu_0}}{\Gamma(\nu_0)}(\sigma^2)^{-(v_0+1)}exp(-\frac{\beta_0}{\sigma^2})\} \\[10pt]

&amp;\propto \{(\sigma^2)^{-\frac{n}{2}}exp(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu)^2)\}
\{exp(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2)\}
\{(\sigma^2)^{-(v_0+1)}exp(-\frac{\beta_0}{\sigma^2})\}
\end{aligned}
$$

</center>

<p>¬†</p>

<p>From the last part of the equation, we have a very complicated form of the posterior kernel even after we have removed all the terms that are independent of $\mu$ and $\sigma^2$. Thus, we cannot not go further and we have to conclude that the posterior cannot be derived as a close-form.</p>

<p>On the other hand, even under the circumstances, we can use Monte-Carlo simulation and approximate our sample distribution to the posterior like I introduced in the previous post. This methodology is called as <strong>MCMC</strong> and let‚Äôs now see how it works.</p>

<hr />

<p>¬†</p>

<h1 id="principle-of-mcmc">Principle of MCMC</h1>

<p>As its name implies, MCMC is an abbreviation that stands for <strong><em>Markov Chain - Monte Carlo</em></strong>. In short, it is a collection of efficient sampling methodologies which is based on the mathematical concepts of <em>Markov Chain</em> and <em>Monte Carlo</em>. With that being said, let‚Äôs see how the two components are used in the MCMC.</p>

<p>From a given probability distribution, MCMC draws random samples based on the previously generated sample. In other words, only the current state is involved in the process of generating the next state, which is a characteristic of <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov Chain</a>. In theory, Markov Chain has a <a href="https://brilliant.org/wiki/stationary-distributions/">stationary distribution</a> if it satisfies what is known as <a href="https://en.wikipedia.org/wiki/Detailed_balance">Detailed Balance Condition</a>. Roughly speaking, stationary distribution is the limiting distribution of a specific Markov Chain. For example, if we infinitely repeat rolling a six sided fair dice, then the probability of each eyes will converge to the stationary distribution of $p(x_i) = \frac{1}{6}, \;\;(i = 1, ‚Ä¶ , 6)$.</p>

<p>Using this principle of Markov Chain, MCMC defines a Markov Chain whose stationary distribution is the target distribution of our interest. Then our Markov Chain moves around the parameter space to collect random samples and based on these samples we can apply Monte-Carlo methods to get the posterior distribution like we did in the <a href="https://domug.github.io/2021/02/20/BS3/">previous post</a>.</p>

<p>In summary, we are using MCMC to approximate the posterior distribution from a well-defined Markov Chain by moving around the states and getting samples to apply the Monte-Carlo method. Thus, the success or failure of a MCMC critically depends on the <strong>quality of the samples</strong>, which is determined by how effieciently the Markov Chain can explore our parameter space. There are various MCMC algorithms and for now, we will focus on the most basic and fundamental sampling method known as <em>Metropolis-Hastings</em> algorithm.</p>

<hr />

<p>¬†</p>

<h1 id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h1>

<p>Metropolis-Hastings is a MCMC method for collecting a sequence of random samples from a probability distribution where direct sampling is difficult. Suppose we are interested in evaluating the following posterior distribution.</p>

<p>¬†</p>

<center>

$$
f(\mu | y_1,...,y_n) \propto 
\frac{exp\{n(\bar y\mu - \frac{\mu^2}{2})\}}{1+\mu^2}
$$

</center>

<p>¬†</p>

<p>Because of the <em>normalizing constant</em> $1+\mu^2$ of the denominator, we cannot define the posterior as a closed-form probability distribution. Considering this example, let‚Äôs use Metropolis Hastings algorithm to evaluate the posterior.</p>

<p>¬†</p>

<hr />

<p><strong>&lt;Steps of MH&gt;</strong></p>

<ol>
  <li>
    <p>Define a function $g(\mu)$ that is proportional to our target distribution $f(\mu)$.</p>

    <p>¬†</p>
  </li>
  <li>
    <p>Furthermore, define a conditional probability distribution 
\(q(\mu' | \mu)\)
, which is easy to get samples from. In general, standard distributions like normal distribution is used. This distribution is called as <strong>proposal distribution</strong>.</p>

    <p>¬†</p>
  </li>
  <li>
    <p>Since we don‚Äôt know the transition probability of our Markov Chain, randomly draw a <strong>candidate</strong> $\mu_{t+1}$ from the proposal distribution and decide whether to accept the candidate as our next sample or not by comparing the difference between 
\(q(\mu_{t+1} | \mu_t)\)
and 
\(f(\mu_{t+1} | \mu_t)\)
 by <strong>acceptance ratio $\alpha_{t, t+1}$</strong>.</p>

    <p>¬†</p>
  </li>
  <li>
    <p>In <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling scheme</a>, we define an <strong>acceptance ratio</strong> which is a probability to accept the proposed state $\mu_{t+1}$. (I will not go into the details of rejection sampling here, but note that Metropolis Hastings algorithm is an application of rejection sampling)</p>

    <center>

$$
\alpha_{t, t+1} = \frac{f(\mu_{t+1}|\mu_t)}{q(\mu_{t+1}|\mu_t)}
$$

</center>

    <p>This ratio being equal to 1 means that we accept the candidate because the sample is just as likely to have came from the proposal distribution as the target distribution (Note that in rejection sampling, $q$ is an upper bound of our target distribution so the acceptance ratio cannot be greater than 1).</p>

    <center>

$$
f(\mu_{t+1} | \mu_t) = q(\mu_{t+1} | \mu_t)
$$
</center>

    <p>However, for most of the time it will be vice versa, that the acceptance ratio will be less than 1. In this case we are not sure about our candidate sample, so we ‚Äúprobabilistically‚Äù accept our candidate to be the next sample.</p>

    <p>¬†</p>
  </li>
  <li>
    <p>Likewise, we need to define an acceptance ratio for our Metropolis Hastings algorithm in a way that our Markov Chain satisfies the <em>Detailed Balance Condition</em>.</p>

    <center>

$$
detailed\;balance\;condition: \;\;\;\;
\frac{f(x|x')}{f(x'|x)} = \frac{f(x)}{f(x')}
$$

</center>

    <p>The transition probabilty can be represented as following.</p>

    <center>

$$
\begin{aligned}
f(\mu_{t+1}|\mu_t) &amp;= q(\mu_{t+1}|\mu_t) \alpha_{t,t+1} \\[10pt]
   
   
   
f(\mu_t|\mu_{t+1}) &amp;= q(\mu_t | \mu_{t+1})\alpha_{t+1, t}
\end{aligned}
$$

</center>

    <p>Plug this equation to the detailed balance condition and we get</p>

    <center>

$$
\frac{f(\mu_t)}{f(\mu_{t+1})} = \frac{q(\mu_t | \mu_{t+1})\;\alpha_{t+1, t}}{q(\mu_{t+1}|\mu_t)\;\alpha_{t,t+1}} \\[20pt]
\Leftrightarrow \\[20pt]
$$

</center>

    <center>

$$
\begin{aligned}
\frac{\alpha_{t,t+1}}{\alpha_{t+1,t}} &amp;= \frac{f(\mu_{t+1})q(\mu_{t}|\mu_{t+1})}{f(\mu_t)q(\mu_{t+1}|\mu_{t})}\\[10pt]
&amp;= \frac{g(\mu_{t+1})q(\mu_{t}|\mu_{t+1})}{g(\mu_t)q(\mu_{t+1}|\mu_{t})}
   
\end{aligned}
$$

</center>

    <p>Thus if we define acceptance ratio $\alpha$ to be</p>
  </li>
</ol>

<p>\(\alpha_{t, t+1} = min(1, \frac{g(\mu_{t+1})}{g(\mu_t)}\frac{q(\mu_t|\mu_{t+1})}{q(\mu_{t+1}|\mu_t)})\)
, then it satisfies the detailed balance condition because either $\alpha_{t, t+1}$ or $\alpha_{t+1, t}$ is always 1.</p>

<p>Note that if we use <strong>symmetric</strong> proposal distribution, the calculation gets much simpler because 
\(q(\mu_t | \mu_{t+1}) = q(\mu_{t+1}|\mu_t)\)
. In fact, this is the <strong>Metropolis algorithm</strong>, and Metropolis Hastings can be seen as a extension of Metropolis algorithm.</p>

<p>¬†</p>

<ol>
  <li>Repeat above steps until our Markov Chain converges.</li>
</ol>

<hr />

<p>I tried to be as specific as possible, but for practical implementation the above steps can be simplified as the following.</p>

<p>¬†</p>

<p align="center">
	<img width="500" height="400" src="/images/bayesian/mh.jpg" />
  <br />
  <em><span style="color:grey">Simplification of MH Algorithm for implementation</span></em>
</p>

<p>¬†</p>

<p>Now it‚Äôs time to actually implement Metropolis Hastings algorithm using Python!</p>

<hr />

<p>¬†</p>

<h1 id="metropolis-hastings-in-python">Metropolis-Hastings in Python</h1>

<p>Normally, we wouldn‚Äôt have to bothered about implementing these MCMC algorithms because we can just use the built-in functions from the module pymc3 or jags, which provides us with a high-level abstraction. But for the sake of learning, we will hard-code MH algorithm and see how it works.</p>

<p>Here‚Äôs the setting of our problem that we would like to derive the posterior from.</p>

<p>¬†</p>

<center>

$$
\begin{aligned}
likelihood:\;y_i|\mu \stackrel{iid}{\sim} N(\mu, 1)\\[10pt]
prior:\;\mu \sim t(0,1,1)\\[10pt]
\end{aligned}
$$

&nbsp;


$$
\begin{aligned}
f(\mu|y_1,...y_n) &amp;= \frac{f(y_1,...y_n|\mu)f(\mu)}{\int f(y_1,...y_n|\mu)f(\mu)du}\\[10pt]
&amp;\propto f(y_1,...,y_n)f(\mu)\\[10pt]
&amp;= \prod_{i=1}^n [N(y_i|\mu)] [t(\mu|0,1,1)]\\[10pt]
&amp;= \frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}\sum_{i=1}^n (y_i-\mu)^2)\frac{1}{1+\mu^2}\\[10pt]
&amp;\propto \frac{exp(n(\bar{y}\mu - \frac{\mu^2}{2} ))}{1+\mu^2}
\end{aligned}
$$
</center>

<p>¬†</p>

<p>In this setting, we are interested in finding out the distribution of $\frac{exp(n(\bar{y}\mu - \frac{\mu^2}{2} ))}{1+\mu^2}$ that has no closed-form solution. For simplicity we will use the symmeric proposal distribution, so strictly speaking, we are implementing Metropolis algorithm which is a special case of Metropolis Hastings.</p>

<hr />

<p><strong>1. Define $g(\mu)$</strong></p>

<p>We define $g(\mu)$ to be the log-scaled version of the original function to prevent possible underflow errors.</p>

<p align="center">
	<img src="/images/bayesian/mh1.png" />
</p>

<p>¬†</p>

<p><strong>2. Random Walk Metropolis algorithm</strong></p>

<p align="center">
	<img src="/images/bayesian/mh2.png" />
</p>

<p>¬†</p>

<p><strong>3. Apply Metropolis algorithm - Bad Example</strong></p>

<p>Let‚Äôs suppose that we have 9 observations such that [1.2, 1.4, -0.5, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9].</p>

<p align="center">
	<img src="/images/bayesian/mh3.png" />
</p>

<p>¬†</p>

<p>Using these observations, let‚Äôs sample 1,000 posterior samples. The choice of hyperparameter of the proposal distribution effects the speed of the convergence, so it is important to use descent starting point. First, let‚Äôs set the standard deviation of our proposal distribution to be 5 and see what happens.</p>

<p>There are several ways of examining the quality of our MCMC samples, and here we will use <strong>trace plot</strong>.</p>

<p align="center">
	<img src="/images/bayesian/mh4.png" />
  <br />
  <em><span style="color:grey">posterior kde and trace plot for high variance</span></em>
</p>

<p>¬†</p>

<p>If we look at the trace plot, we can see that our Markov Chain is frequently stucked in some states during the iterations. Our acceptance rate is only about 0.075, meaning that the samples were frequently rejected because the variance was set too big. Ideally, we would want our acceptance rate to be within 0.23 ~ 0.5.</p>

<p>On the other hand, let‚Äôs see what happens if we use too small standard deviation for the proposal distribution.</p>

<p align="center">
	<img src="/images/bayesian/mh5.png" />
  <br />
  <em><span style="color:grey">posterior kde and trace plot for small variance</span></em>
</p>

<p>¬†</p>

<p>We can clearly find an upward-moving pattern in the trace plot and the acceptance rate is as high as 0.965. This indicates that our Markov Chain has unduly accepted the candidates, resulting in a highly correlated posterior samples which are practically of no importance.</p>

<p>There are two remedies to this problem - first is to find ideal starting point, and second is to increase the number of iterations.</p>

<p>¬†</p>

<p><strong>4. Apply Metropolis algorithm - Good Example</strong></p>

<p>This time, we will use standard normal distribution as our proposal distribution, which is assumed to perform well under our metropolis algorithm.</p>

<p align="center">
	<img src="/images/bayesian/mh6.png" />
  <br />
  <em><span style="color:grey">posterior kde and trace plot for optimal variance</span></em>
</p>

<p>¬†</p>

<p>We can see that our trace plot randomly moves around the mean without any patterns and the acceptance rate is about 0.36, which altogether is good. Thus, we can conclude that our Markov Chain seem to have converged to the stationary distribution (which in this case is the posterior distribution).</p>

<p>¬†</p>

<p><strong>5. Posterior Analysis</strong></p>

<p>Now let‚Äôs try visualize the result of our MCMC simulation.</p>

<p align="center">
	<img src="/images/bayesian/mh7.png" />
</p>

<p>By running this code we get the following plot.</p>

<p align="center">
	<img width="600" height="400" src="/images/bayesian/mh8.png" />
  <br />
  <em><span style="color:grey">Fig. 1. Comparision of the posterior with the prior</span></em>
</p>

<p>¬†</p>

<p>We can see that the prior has moved to the center of the observations and formed a posterior, which matches our intuition. This is indeed a satisfactory result and we managed to derive the posterior of $\frac{exp(n(\bar{y}\mu - \frac{\mu^2}{2} ))}{1+\mu^2}$, which we couldn‚Äôt analytically.</p>

<p>We have successfully derived the posterior using Metropolis-Hastings sampler! However, this method has some limitations as MH sampler gets very inefficient when we are dealing with posteriors that have multiple parameters. In the following post, we are going to take a look at another MCMC method - <strong>Gibbs Sampler</strong>, which is designed to work well on the multi-parameter problems as well.</p>

<hr />

<p>¬†</p>

<h1 id="remarks">Remarks</h1>

<p>There are some important points worth paying attention when making inference about the posterior using MCMC.</p>

<p>First, regarding the convergence of the Markov Chain, our Markov Chain is guarenteed to converge to the stationary distribution using MCMC methods if we have infinitely many samples. However it is just in theory and we can‚Äôt afford to sample infinitely. Therefore, we have to endure some possible tradeoff between the accuracy and computational practicality. In practice, we use several <strong>parallel Markov Chains with different starting points</strong> to assure that the chains converge to the same stationary distribution.</p>

<p>Second, we should only use the samples that are simulated after the Markov Chain has converged. In other words, we have to exclude the primal samples of the chains for making inferences. The time until the convergence is called as the <strong>burn-in period</strong>.</p>

<p>Third, posterior samples are inevitably dependent with each other as they have been simulated from a Markov Chain. Therefore it is important to check the <strong>effective sample size</strong> for the posterior samples, which indicates the amount of <strong>independent</strong> information in them.</p>

<p>Fourth, assessment of the quality of the posterior samples can be done in a various ways where trace plot is the most simple and straightforward method. Apart from the trace plot, we can also numerically diagnose the convergence of our Markov Chain by using <strong>r-hat</strong> (or Gelman-Rubin convergence diagnostics).</p>

<p>Fifth, different samplers have different performances. This is a big problem as we are interested in a high-dimensional, multi-modal posterior distributions where simple Metropolis-Hastings Sampler does not work. We will talk more about this in the future, but for now be aware that Metropolis-Hastings is a very basic MCMC algorithm. In practice we use more complicated MCMC algorithms such as NUTS (No U-Turn Sampler) and HMC (Hamiltonian Monte-Carlo). In fact NUTS is the default sampler of pymc3 module.</p>

<hr />

<h1>¬†</h1>

<h1 id="references">References</h1>

<ul>
  <li>Gelman et. al. 2013. <em>Bayesian Data Analysis</em>. 3rd ed.  CRC Press.</li>
  <li><a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Wikipedia-Metropolis Hastings</a></li>
</ul>

:ET