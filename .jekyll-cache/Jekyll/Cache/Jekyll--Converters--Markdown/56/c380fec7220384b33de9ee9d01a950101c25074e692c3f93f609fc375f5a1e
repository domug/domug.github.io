I"¿;<p>¬†</p>

<p>This post is a gentle introduction to the field of Bayesian Statistics focusing on its difference from the traditional frequentist statistics.</p>

<p>¬†</p>

<p>In the following series of posts, we will be talking about <strong>Bayesian Statistics</strong>, which is arguably one of the most probable field of applied statistics in 21st century for its practical and intuitive framework of quantifying the uncertainties.</p>

<p>Just like any other field of study that is built upon mathematics, Bayesian Statistics is also filled with heavy amount of mathematical formulas and proofs, and I don‚Äôt want to be overwhelmed by them. For this reason, I will try to keep the rigorous mathematical proofs as minimum as possible and rather try focus on the insight and the intuition Bayesian statistics can give us.</p>

<p>In latter posts, a renowned PPL (Probabilistic Programming Language) <em>pymc3</em> made for python environment will be used to actually implement algorithms for Bayesian inferences. As a first post of this series, we‚Äôll begin by talking about some of the fundamentals that differentiates Bayesian Statistics from traditional statistics.</p>

<p>¬†</p>

<hr />

<h1 id="1-defining-probability">1. Defining Probability</h1>

<p>What is <strong>probability</strong>? Defining probability is the very first step in statistics as it is a field of study that tries to quantify the uncertainties in our lives by making use of probabilities. One definition of probability is ‚Äúnumerical descriptions of <strong>how likely an event is to occur</strong>‚Äù. Although this definition might sound trivial, formally defining ‚Äú<strong>likeliness of an event</strong>‚Äù (don‚Äôt get confused with the <em>likelihood</em> in statistical sense) isn‚Äôt actually that simple. Let me introduce three different perspectives on defining probability.</p>

<p>¬†</p>

<ul>
  <li><strong>Classical Definition</strong>:
    <ul>
      <li>Events that are likely to yield the <strong>same outcome</strong> have the <strong>same probability</strong>. For example, when tossing a coin, heads and tails are equally likely to show up and thus probability is 1/2 for each.</li>
      <li>This is a primal definition long before the advance of statistics in 19th century.</li>
    </ul>
  </li>
</ul>

<p>¬†</p>

<ul>
  <li><strong>Frequentist (Classical Statistics)</strong>:
    <ul>
      <li>Assuming <strong>infinite sequence of events</strong>, probability is defined as the <strong>relative frequency</strong> of each events. For example, by infinitely repeating the tossing of a coin, heads and tails appear half&amp;half.</li>
      <li>By this perspective, definition of probability is <strong>consequential</strong> and this can sometimes contradict to our intuition (common sense) when we cannot assume infinite replication of events.</li>
      <li>For example, we want to know whether it will rain tomorrow or not. Considering these binary outcomes, the probability is either 0/1 (it rained / it didn‚Äôt rained). Why? - because we can observe ‚Äútomorrow‚Äù only once.</li>
    </ul>
  </li>
</ul>

<p>¬†</p>

<ul>
  <li><strong>Bayesian</strong>:
    <ul>
      <li>Accounts for <strong>subjectiveness</strong> in the probability as long as it satisfies the basic <a href="https://en.wikipedia.org/wiki/Probability_axioms">axioms of probability</a>.</li>
      <li>Compared to frequentist‚Äôs definition, interpretation of probability is way more intuitive and flexible (ex. The probability of rain tomorrow is 0.73).</li>
      <li>We can provide exact probabilistic answers to questions that were unanswerable in frequentist paradigm. (ex. What is the probability that Korea will be unified in the next 10 years?)</li>
    </ul>
  </li>
</ul>

<p>¬†</p>

<p>This might sound confusing, but the keyword here is that Bayesian‚Äôs probability is <strong>subjective</strong>. So when I ask ‚ÄúWhat do you think is the probability that COVID-19 will end in 2022?‚Äù the answer will likely to be different depending on people because <strong>each of us have different beliefs and concerns</strong> about this virus. Even if it‚Äôs not clear for now, we‚Äôll talk more in detail in a moment and so hang in there!</p>

<p>Then how exactly are we going to calculate probability in Bayesian sense? Here‚Äôs a good news. In Bayesian Statistics, a single theorem solves everything! Let‚Äôs take a look at Bayes‚Äô Theorem.</p>

<p>¬†</p>

<hr />

<h1 id="2-bayes-theorem">2. Bayes‚Äô Theorem</h1>

<p>Bayes‚Äô Theorem is defined as the following:</p>

<center>

$$
P(A|B) = \frac{P(A \cup B)}{P(B)}
$$

</center>

<p>Let‚Äôs try to interpret its meaning.</p>

<ul>
  <li>Left side of equation: <strong>probability of event A happening, presuming that event B had already happened</strong>.</li>
  <li>Right side of equation: <strong>probability of event A and B happening at the same time divided by probability of event B happening</strong>.</li>
</ul>

<p>From this equation, if we marginalize the numerator of the right side we have the following equation:</p>

<p>¬†</p>

<center>

$$
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
$$

</center>

<p>¬†</p>

<p>The implication of this equation is that we can derive a particular conditional probability (left side) by marginalizing the joint probability into a conditional and marginal probabilities. In a Bayesian sense, this conditional probability on the left side is called as the <strong>posterior</strong> and the numerator on the right side is each called as the <strong>likelihood</strong> and the <strong>prior</strong> while the denominator is evidence.</p>

<p>These three components - posterior, likelihood and prior - are the building blocks of Bayesian Statistics. In fact, the probability I mentioned to be calculated in Bayesian inference refers to posterior. Let‚Äôs consider a simple example and see how it‚Äôs done.</p>

<p>¬†</p>

<hr />

<h2 id="3-frequentist-vs-bayesian">3. Frequentist vs Bayesian</h2>

<p>For illustration, we will consider the following situation.</p>

<blockquote>
  <p>Q. We have a sack which contains fair coins and unfair coins (fair coin meaning probability of heads and tails is equal). We sampled a single coin from this sack and we want to know whether this coin is fair or not. So we filpped the coin for 10 times and the result was 3 heads and 7 tails. Under this situation, can we say that our sampled coin is fair?</p>
</blockquote>

<p>¬†</p>

<h4 id="frequentist">Frequentist</h4>

<p>First, let‚Äôs try to solve this question by frequentists‚Äô approach. Like I mentioned earlier, frequentists assume a repetition of event and it is clear that consecutive tossing of a coin follows binomial distribution. To be more specific, this example follows binomial distribution $B(n,p)$ where $n$ (number of repetition) is equal to 10 and our goal is to make inference on the unknown parameter $p$.</p>

<p>In frequentist paradigm, we have to define an <strong>estimator</strong> $\hat p$ to make inference on the true parameter $p$ (for more information about estimators check out this <a href="https://domug.github.io/2020/12/03/Estimation_Multicollinearity/">post</a>). There are several ways of defining an estimator, and for now let‚Äôs use the most commonly used <strong>Maximum Likelihood Method</strong> to derive $p$ analytically.</p>

<p>Maximum Likelihood Estimation in short, is assuming that our parameter is some <strong>unknown but fixed constant</strong> and trying to find the value that has the <strong>highest density in the probability distribution of events</strong> (likelihood) and use it as an estimator on behalf of the true parameter. In other words, considering our example, it is equivalent to finding $p$ that maximizes the binomial probability density $n \choose x$ $p^x (1-p)^{n-x}$.</p>

<p>Since our main topic is on bayesian we are going to skip ahead to the conclusion, but note that analytical solution for our example is relatively trivial in that as binomial density is a concave function, we can just set the differential equation to 0 and solve it to get the answer. Anyways, if we do the math then our MLE (Maximum Likelihood Estimator) is $\frac{\sum_{i=1}^{n} x_i}{n}$ which is a <strong>sample mean</strong> of the number of heads.</p>

<p>In conclusion, for our example we derived MLE of $p$ to be the sample mean, which is $\frac{3}{10}$. This indicates that if we toss the coin infinitely many times, the probability of heads is only about 30%, meaning that this coin is somehow biased (for more sophisticated justification we normally conduct <a href="https://www.statisticshowto.com/neyman-pearson-lemma/#:~:text=The%20Neyman%2DPearson%20Lemma%20is,the%20alternate%20hypothesis%20is%20true."><em>Neyman Pearson‚Äôs hypothesis testing</em></a>) . Anyhow, we can conclude as the following:</p>

<ul>
  <li><strong><em>Considering our observed results, the coin is not fair</em></strong>.</li>
</ul>

<p>This seems to make sense, but don‚Äôt you think it is a bit too irresponsible? The result is pretty lame and obvious. There‚Äôs not much of a new information gained by doing the inference in the first place. This is because frequentists consider parameter $p$ as a <strong>deterministic variable (fixed constant)</strong>. Since there is no space for variability in the conclusion, the result is either fair / not fair. These two constrasting state of outcome can‚Äôt co-exist at the same time. With this in mind, let‚Äôs take Bayesian approach and see how it is different.</p>

<p>¬†</p>

<h3 id="bayesian">Bayesian</h3>

<p>Now let‚Äôs take a look at Bayesian‚Äôs answer to our example. To find out whether our sampled coin is biased or not, we have to conduct a <strong>Bayesian Inference</strong> and as a first step, we need a concept of <strong>prior</strong>. Here‚Äôs what it is.</p>

<ul>
  <li><strong>Prior</strong> literally means our <strong>external prior knowledge about the subject</strong>. It can be some objective findings of rigorous research papers or it can just be our subjective beliefs on the topic.</li>
  <li>For example, let‚Äôs say that I‚Äôm interested in the probability of raining tomorrow in Seoul (today is Feb 09). Since I‚Äôve lived in Seoul for a long time, I know that it is very unlikely to rain in early Feburary. So for this problem, I would say my prior probability of raining tomorrow is 0.1.</li>
</ul>

<p>Likewise, for our example we can integrate any sort of our prior knowledge about the coin such as its physical properties (like there is a little crack on the coin) into our inference. Because of this flexibility in the inference, Bayesian Statistics can give realistic probabilities to questions that were hard to answer under Frequentist‚Äôs framework.</p>

<p>But always keep in mind that we shouldn‚Äôt abuse the use of priors. As Bayesian probabilities are innately subjective, we need some sort of formal justification of our priors for our inference to have credibility. I think we‚Äôll have more time for that in the future and for now let‚Äôs move on to how Bayesian Inference is done based on priors.</p>

<hr />

<p>Here are the steps:</p>

<ol>
  <li><strong>Set prior based on our external knowledge about the problem</strong>. (If we don‚Äôt have any knowledge we use <em>uninformative priors</em>, which will be discussed in the upcoming posts)</li>
  <li><strong>Use Bayes‚Äô Theorem to update your prior based on observed data (likelihood)</strong>.</li>
  <li><strong>Derive posterior</strong>.</li>
</ol>

<hr />

<p>To put it simply, a full bayesian inference consists of 3 elements - <strong>prior</strong>, <strong>likelihood</strong>, <strong>posterior</strong>. Let‚Äôs follow the steps and actually derive the posterior analytically.</p>

<p>¬†</p>

<p><strong>&lt;Step 1&gt;</strong></p>

<p>By inspecting our sampled coin carefully, I‚Äôve found out some small cracks on the edges of the coin. I suspected that these cracks will have influence on the coin‚Äôs fairness so I decided to set my prior probability of this coin to be fair as 0.4, a little less than 0.5.</p>

<p>¬†</p>

<p><strong>&lt;Step 2&gt;</strong></p>

<p>Based on our observation of 3 heads and 7 tails, use Bayes‚Äô Theorem to update our prior.</p>

<p>¬†</p>

<center>

$$
\begin{aligned}
f(p|x) &amp;= \frac{f(p,x)}{f(x)} \\[10pt]
&amp;= \frac{f(x|p)f(p)}{\sum_p f(x|p)f(p)} \\[10pt]

&amp;= \frac{\binom{10}{x} [(0.5)^{10} (0.4)  I_{fair} + (0.3)^x (0.7)^{10-x} (0.6) I_{unfair}]}{\binom{10}{x} [(0.5)^{10} (0.4) + (0.3)^x (0.7)^{10-x}(0.6)] } \\[10pt]

\end{aligned}
$$

</center>

<p>¬†</p>

<p><strong>&lt;Step 3&gt;</strong></p>

<p>Note that $I_{fair}$, $I_{unfair}$ are indicator functions. Since we have observed 3 heads, let‚Äôs plug in the value and calculate the posterior probability.</p>

<p>¬†</p>

<center>

$$
\begin{aligned}
f(p|x = 3) &amp;= \frac{\binom{10}{3} [(0.5)^{10} (0.4)  I_{fair} + (0.3)^3 (0.7)^{10-3} (0.6) I_{unfair}]}{\binom{10}{3} [(0.5)^{10} (0.4) + (0.3)^3 (0.7)^{10-3}(0.6)] } \\[10pt] 

&amp;= \frac{0.046875\;I_{fair} + 0.1600968\;I_{unfair}}{0.2069718} \\[10pt]

&amp;= 0.2264802\;I_{fair} + 0.7735198\;I_{unfair}
\end{aligned}
$$

</center>

<p>¬†</p>

<p>The result of the bayes theorem suggests that given our observed data, the posterior probability that our sampled coin is fair is only about 22.6%, which is enough to derive the following conclusion:</p>

<p>¬†</p>

<blockquote>
  <p><strong>Considering our observed results, our coin has probability of 0.226 to be fair</strong>.</p>
</blockquote>

<p>¬†</p>

<p>This conclusion, in contrast with that of frequentists, explicitly suggests a specific probabilistic statement about the amount of uncertainty in our inference (remember that from frequentist inference the conclusion was just ‚ÄúThe coin is unfair‚Äù). Thus it‚Äôs more intuitive and informative. This is also in line with our common sense that <strong>our prior belief about the coin being fair 40%, was decreased to 22.6% after we saw an actual outcome of 30%</strong>.</p>

<p>¬†</p>

<p align="center">
	<img width="400" height="300" src="/images/bayesian/bayes1.png" />
</p>
<p>¬†</p>

<p>Can you see how the probability was updated? One of the cool part of Bayesian Statistics is that we can also update the derived posterior by another observation. Let‚Äôs say that after this observation, we have conducted another set of coin tosses and got only 10% of head this time. If this is the case, our posterior probability will further be decreased from 22.6%.</p>

<p>This sequential updating makes Bayesian Inference flexible and informative. However, life isn‚Äôt so beautiful in practice. Above example was a really simple case where we could use binomial distribution, which makes the math easy for us to handle, where in real life situations it is often impossible to derive the posterior analytically.</p>

<p>Luckily, there is a way to address this problem by using our cutting-edge computers! We will talk more about them in detail in the following posts. This is end for now and it the next post, we are going to take a look at <strong>conjugacy of distributions</strong>.</p>

<hr />

<h1 id="references">References</h1>

<ul>
  <li>Gelman et. al. 2013. <em>Bayesian Data Analysis</em>. 3rd ed.  CRC Press</li>
</ul>

:ET