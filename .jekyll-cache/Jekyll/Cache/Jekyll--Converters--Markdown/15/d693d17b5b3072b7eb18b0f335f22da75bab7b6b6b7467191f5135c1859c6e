I"J<p>This post is an overall summary of Chapter 3, 4 of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<p><strong>&lt;Table of Contents&gt;</strong></p>

<p><strong>1.1. Sigma Algebra</strong></p>

<p><strong>1.2. Expected Values</strong></p>

<p><strong>1.3. Moment Generating Functions</strong></p>

<p><strong>1.4. Characteristic Functions</strong></p>

<p><strong>1.5. Statistical Independence</strong></p>

<p><strong>1.6. Covariance and Correlation</strong></p>

<p><strong>1.7. Conditional Distributions</strong></p>

<p>Â </p>

<hr />

<h2 id="ch-2-inequalities"><strong>Ch 2. Inequalities</strong></h2>

<p>One of the main goals in statistics and related fields is to make an inference about unknown parameters. For this purpose, we define an <strong>estimator</strong> that can reasonably surrogate the true parameter.</p>

<p>However, the problem is that there are many possible choices of estimators. So we often consider a <strong>risk function</strong> between the true parameter $\theta$ and its estimator $\hat \theta$ to evaluate its performance.</p>

<p>Nevertheless, the problem still exists in that computing a risk function is not so simple . Thus, we make use of various inequalities to <strong>upper bound</strong> the risk function by a function that is more easy to manipulate and study how fast the risk function goes to zero as sample sizes increase.</p>

<p>Â </p>

<h3 id="21-inequalities-for-expectations">2.1 Inequalities for expectations</h3>

<h4 id="jensens-inequality"><em>Jensenâ€™s Inequality</em></h4>

<p>For a convex function $g$ such that</p>

<center>

$$
\lambda g(x) + (1-\lambda)g(y) \geq g(\lambda x + (1-\lambda)y)
$$

</center>

<p>for all $\lambda \in (0,1)$ and $x,y \in \mathbb{R}$. Then provided that expectations exist for both random variables,</p>

<center>

$$
E\big[ g(X)\big] \geq g\big(E[X] \big)
$$

</center>

<ul>
  <li>An application of Jensenâ€™s Inequality is the relationship between different means.</li>
</ul>

<center>

$$
\begin{aligned}
&amp;\text{Arithmetric Mean (AM)} = \frac{\sum_{i=1}^n x_i}{n} \\[5pt]
&amp;\text{Geometric Mean (GM)} = \Big(\prod_{i=1}^n x_i\Big)^{1/n} \\[5pt]
&amp;\text{Harmonic Mean (HM)} = \frac{1}{\frac{1}{n}\sum_{i=1}^n\frac{1}{x_i}}
\end{aligned}
$$

$$
\Rightarrow \text{HM} \leq \text{GM} \leq \text{AM}
$$

</center>

<ul>
  <li>Another famous application is the positiveness of <strong><em>Kullback-Leibler divergence</em></strong>.</li>
</ul>

<center>

$$
D_{KL}P(||Q) = \sum_x p(x)log\Big(\frac{p(x)}{q(x)}\Big) = E\Big[ log\Big(\frac{p(X)}{q(X)}\Big)\Big] = E\Big[-log\Big(\frac{q(X)}{p(X)}\Big)\Big] \geq 0
$$

</center>

<p>Â </p>

<h4 id="hÃ¶lders-inequality"><em>HÃ¶lderâ€™s Inequality</em></h4>

<p>For $p, q \in (1,\infty)$ with $\frac{1}{p} + \frac{1}{q} = 1$,</p>

<center>

$$
\big|E[XY]\big| \leq E\big[|XY|\big] \leq \big(E\big[|X|^p\big]\big)^{1/p}  \big(E\big[|Y|^q\big]\big)^{1/q}
$$

</center>

<ul>
  <li>A special case of HÃ¶lderâ€™s Inequality when $p=q=2$ is the <strong><em>Cauchy-Schwarz inequality</em></strong> such that:</li>
</ul>

<center>

$$
\big|E[XY]\big| \leq E\big[|XY|\big] \leq \sqrt{E\big[X^2\big]  E\big[Y^2\big]}
$$

</center>

<ul>
  <li>By applying the <em>Cauchy-Schwarz inequality</em>, we have the covariance inequality such that:</li>
</ul>

<center>

$$
\begin{aligned}
Cov(X, Y) = E\big[(X-\mu_X)(Y-\mu_Y) \leq \{E\big[(X-\mu_X)^2\big] \}^{1/2} \{E\big[(Y-\mu_Y)^2\big] \}^{1/2}
\end{aligned}
$$

$$
\Leftrightarrow \{Cov(X, Y)\}^2 \leq Var(X)Var(Y)
$$

</center>

<p>Â </p>

<p>&lt;/center&gt;</p>

<p>Â </p>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET