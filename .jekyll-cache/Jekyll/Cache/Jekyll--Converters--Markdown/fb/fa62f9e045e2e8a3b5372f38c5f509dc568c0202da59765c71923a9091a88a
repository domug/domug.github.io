I"Ñ<p>The following series of posts is an overall summary of the textbook <em><a href="http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&amp;Roger%20L.Berger--Statistical%20Inference.pdf">Statistical Inference</a></em> by Casella and Berger.</p>

<h2 id="ch-1-foundations-of-statistics"><strong>Ch 1. Foundations of Statistics</strong></h2>

<p>Â </p>

<h3 id="11-sigma-algebra">1.1 Sigma Algebra</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>A collection of subsets of sample space <em>S</em> is called a <strong><em>sigma algebra</em></strong> denoted by <strong><em>B</em></strong>, if it satisfies the following three properties:
\(\begin{aligned}
&amp;1.\quad \emptyset \in B \quad(\text{ the empty set is an element of }B) \\[5pt]
&amp;2.\quad \text{If } A \in B, \text{ then }A^c\in B \quad(B\text{ is closed under complementation}) \\[5pt]
&amp;3. \quad \text{If }A_1,A_2,\dots \in B, \text{ then } \cup_{i=1}^\infty A_i \in B \quad(B\text{ is closed under countable unions})
\end{aligned}\)</p>

<p><strong>Q. Reason why we want to care about <em>sigma algebras</em>?</strong></p>

<ul>
  <li>when we deal with a large sample space like real line, there are some <strong>pathological sets</strong> that <strong>break down the probability theory</strong>.</li>
  <li>To avoid such crazy sets (i.e. <strong>non-measurable sets</strong>), we restrict our attention to smaller but nicer subsets (i.e. <strong>measurable sets</strong>) for which the probability measure is well-defined and satisfies the <strong><em>Kolmogorov axioms</em> of probability</strong>.</li>
</ul>

\[\begin{aligned}
&amp;1.\quad P(A) \geq 0 \text{ for all } A \in B \\[8pt]
&amp;2.\quad P(S) = 1 \\
&amp;3. \quad \text{If } A_1,A_2,\dots \in B \text{ are pairwise disjoint, then } P\big(\cup_{i=1}^\infty A_i \big) = \sum_{i=1}^\infty P(A_i)
\end{aligned}\]

<p>Â </p>

<h3 id="12-expected-values">1.2 Expected Values</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>The expected value or mean of a random variable $g(X)$, denoted as $E[g(X)]$ is defined as:
\(\begin{equation}
  E[g(X)]=\left\{
  \begin{array}{@{}ll@{}}
    \int_{-\infty}^{\infty} g(x)f_X(x)dx , &amp; \text{if } X \text{ is continuous} \\[10pt]
    \sum_{x\in \Chi} g(x)f_X(x) , &amp; \text{if } X \text{ is discrete}
  \end{array}\right.
\end{equation}\)
provided that the integral or sum exists.</p>

<p><strong>&lt;Relationship to tail probability&gt;</strong></p>

<p>Let X be a nonnegative random variable. Then in holds that:
\(E[X] = \int_{0}^{\infty} \big( 1 - F_X(x) \big)df = \int_{0}^{\infty}P(X &gt; x)dx\)</p>

\[\begin{aligned}
&amp;\Rightarrow x = \int_0^{x} 1dt = \int_0^{\infty} \mathbb{1}(t &lt; x)dt \\[10pt]
&amp;\Leftrightarrow E[X] = E\Big[\int_0^{\infty} \mathbb{1}(t &lt; x)dt\Big] = \int_0^{\infty} P(X &gt; t)dt \quad (\because \text{Tonelli's theorm})
\end{aligned}\]

<p>A slight extension of this property to any random variable is that:
\(E[X] = \int_0^\infty P(X &gt; x)dx - \int_{-\infty}^0 P(X &lt; x)dx\)</p>

\[\begin{aligned}
&amp;\text{Since } x\mathbb{1}(X\geq 0) = \int_{0}^x\mathbb{1}(X \geq 0)dt = \int_{0}^\infty \mathbb{1}(X \geq 0) \mathbb{1}(X &gt; t)dt = \int_0^\infty \mathbb{1}(X &gt; t)dt, \\[5pt]
&amp;\text{and }x = x\mathbb{1}(X \geq 0) +  x\mathbb{1}(X &lt; 0) =  x\mathbb{1}(X \geq 0) -  -(x)\mathbb{1}(-X &gt; 0),
\end{aligned}\]

\[\begin{aligned}
\Rightarrow &amp;E[ \mathbb{1}(X\geq 0)] = E\big[\int_0^\infty \mathbb{1}(X&gt;t)dt \big] = \int_0^\infty P(X&gt;t)dt \\[7pt]
&amp; E[-x\mathbb{1}(-X \geq 0)] = E\big[\int_0^\infty \mathbb{1}(-X &gt; t)dt\big] = \int_0^\infty P(X &lt; -t)dt = \int_{-\infty}^0P(X &lt; t)dt
\end{aligned}\]

\[\therefore E[X] = \int_0^\infty P(X &gt; x)dx - \int_{-\infty}^0 P(X &lt; x)dx\]

<p><strong>&lt;Variance&gt;</strong></p>

<p>A useful alternative expression of variance is that for any arbitrary i.i.d. copy of random variable $X$ denoted by $X^\prime$, the variance is:
\(Var(X) = \frac{1}{2} E\big[ (X-X^\prime)^2\big]\)
From this, for a bounded random variable $x \in [a, b]$ the variance is upper bounded by:
\(Var(X) \leq \frac{(b-a)^2}{4}\)
This boundary cannot be improved in general.</p>

<p>Â </p>

<h3 id="13-moment-generating-functions">1.3 Moment Generating Functions</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>For each integer $n$, the n-th central moment of a random variable $X$ is defined as:
\(\mu_n = E\big[(X-E[X])^n\big]\)
Then, the moment generating function (a.k.a. mgf) of a random variable $X$ is:
\(M_X(t) = E\big[ e^{tX} \big], \quad \;\forall t \in (-\delta, \delta), \;\delta &gt;0\)</p>

<p><strong>Q. Why is mgf useful?</strong></p>

<ul>
  <li>The mgf of $X$ gives us all moments of $X$.</li>
  <li>If the mgf exists, it uniquely determines the distribution of $X$.</li>
</ul>

\[M_X(t) = M_Y(t) \;\Leftrightarrow\; X \overset{d}{\approx} Y\]

<ul>
  <li>Convergence in mgf implies convergence in distribution. This property is used to prove the Central Limit Theorem.</li>
</ul>

\[\underset{n \to \infty}{\text{lim}}M_{X_n}(t) = M_X(t) \;\Leftrightarrow\; \underset{n \to \infty}{\text{lim}}F_{X_n}(t) = F_X(t)\]

<ul>
  <li>mgf is also useful to obtain a probability tail bound such as the <em>Hoeffdingâ€™s inequality</em>.</li>
</ul>

<p>Note that <strong>mgf doesnâ€™t necessarily exists for all random variables</strong> (e.g. <em>Cauchy random variable</em>)
\(\begin{aligned}
&amp;\text{For Cauchy Random Variable }X, \\[5pt]
&amp;f(x) = \frac{1}{\pi}\frac{1}{x^2+1}, \quad (-\infty &lt; x &lt; \infty) \\[5pt]
\end{aligned}\)</p>

\[\begin{aligned}
\Rightarrow \int_{-\infty}^\infty e^{tx}\frac{1}{\pi}\frac{1}{x^2+1}dx &amp;\geq \int_{0}^\infty e^{tx}\frac{1}{\pi}\frac{1}{x^2+1}dx \\[8pt]
&amp;\geq \int_{0}^\infty \frac{1}{\pi}\frac{tx}{x^2+1}dx \\[8pt]
&amp;= \underset{a \to \infty}{\text{lim}}\Big[\frac{t}{2\pi}log(a^2 + 1) \Big] = \infty
\end{aligned}\]

<p>Â </p>

<h3 id="14-characteristic-functions">1.4 Characteristic Functions</h3>

<p><strong>&lt;Definition&gt;</strong></p>

<p>The characteristic function of a random variable $X$ is defined as:
\(\phi_X(t) = E\big[ exp(itX) \big] = E\big[cos(tX) + isin(tX)\big], \quad t\in\mathbb{R}\)</p>

<ul>
  <li>Characteristic function serves similar purposes with the moment generating function, but it exists for any kind of random variable.</li>
</ul>

<p><strong>&lt;Properties&gt;</strong>
$$
\begin{aligned}
&amp;1.\quad \phi_X(0) = 1 \;\text{ and }\; |\phi_X(t)| \leq 0. \[7pt]
&amp;2.\quad \phi_X(t) \text{ is uniformly continuous } \big(\text{i.e. exists }\psi \text{ such that }  |\phi_X(t+n) - \phi_X(t)| \leq \psi(h)\big). \[5pt]
&amp;3.\quad \text{If } X \overset{d}{=} -X \text{ (i.e. symmetric)}, \phi_X(t) \text{ is real-valued}. \[5pt]
&amp;4.\quad X \overset{d}{=} Y \text{ if and only if } \phi_X(t) = \phi_Y(t).</p>

<p>\end{aligned}
$$</p>

<p>Â </p>

<h3 id="15-statistical-independence">1.5 Statistical Independence</h3>

<hr />

<h1 id="reference">Reference</h1>

<ul>
  <li>Casella, G., &amp; Berger, R. L. (2002). <em>Statistical inference.</em> 2nd ed. Australia ; Pacific Grove, CA: Thomson Learning.</li>
</ul>

:ET