I"<h3 id="previous-posts">Previous Posts</h3>

<p><a href="https://domug.github.io/2020/12/03/Estimation_Multicollinearity/">1. Statistical Estimation and Multicollinearity</a></p>

<p><a href="https://domug.github.io/2020/12/04/Ridge_Lasso/">2. Variable Selection Methods</a></p>

<p><a href="https://domug.github.io/2020/12/05/PCA/">3. Principal Component Analysis</a></p>

<p><a href="https://domug.github.io/2020/12/06/FA/">4. Factor Analysis</a></p>

<hr />

<h1 id="cluster-analysis-ca">Cluster Analysis (CA)</h1>

<p>In this post, we will be talking about <strong>Cluster Analysis</strong>. Until now we have seen PCA and FA which reduced dimensionality of the original data in terms of its ‚Äúvariables‚Äù. Both methods decomposed data matrix by its column vectors, so we were able to find a subset of variables that accounts for enough information of the original data. However, cluster analysis performs dimension reduction on <strong>observations</strong>, that is, by <strong>row vectors</strong>. This difference very important so keep that in mind!</p>

<p>The goal of cluster analysis is to find groups or clusters that can ‚Äúadequately‚Äù separate and group the observations. Since the word ‚Äúadequately‚Äù is a little vague in its meaning, we can define the adequacy as follows.</p>
<ul>
  <li><strong>observations inside the same clusters are similar as possible</strong></li>
  <li><strong>observations within different clusters are different as possible</strong></li>
</ul>

<hr />

<h3 id="distance">Distance</h3>

<p>However, even in the above definition we have another ambiguousness. How are we going to measure the similarity and dissimilarity of each observations? Here‚Äôs where the concept of <strong>‚Äúmathematical distance‚Äù</strong> comes in. To define distance, we foremost need a ‚Äúmeasure‚Äù and there are various definitions of different mathematical measures to define the distance between two objects.</p>

<p align="center">
	<img width="700" height="500" src="/images/ca/distances.png" />
</p>

<p>Above is an wikipedia example of lists of different distances, but all of them are defined to do one single thing - to measure the <strong>‚Äúcloseness‚Äù</strong> of objects. The ‚Äúcloser‚Äù the objects are, the more homogeneous or similar they are. Since going into the details of all of the distances above would be tedious and time consuming, let‚Äôs focus on some of the key points of distance and move on.</p>

<p>Normally when we think of distance between two things, we are mostly familiar with the <strong>‚ÄúEuclidean Distance‚Äù</strong>, which is the most famous and widely used distance measure for calculating <strong>‚Äúphysical distances‚Äù</strong>. ‚ÄúPhysical distance‚Äù here literally means the distance of two physical objects. We say Tokyo is around 1,150 km apart from Seoul. This is physical distance.</p>

<p>But it‚Äôs important to note that there also exists <strong>‚Äúpsychological distance‚Äù</strong> as well. Like when you are asked, ‚ÄúHow close do you think you are with your best friend?‚Äù, you will not likely to answer ‚ÄúOh, it‚Äôs about 10 meters‚Äù. In this situation, we need another measurement that can account for our psychological closeness.</p>

<p>Likewise, the measurement of distance can vary according to the goal of our interest and the distance measure to be used for cluster analysis should also be different with respect to the nature of our data. Although most of the data we‚Äôll face is going to be about physical properties, it is worth noticing that we should not blindly rely on the euclidean distance regardless of situation.</p>

<hr />

<h3 id="k-means-clustering">K-Means Clustering</h3>
<p>Anyways, let‚Äôs come back to our main topic. Now we are clear that cluster analysis groups observations to make clusters. This process is done in a predefined order and principles, which is formally called as ‚Äúalgorithm‚Äù. There are lots of clustering algorithms and research is still ongoing.</p>

<p>In this post, we will use an algorithm known as <strong>‚ÄúK-Means Clustering‚Äù</strong> which is the most widely implemented clustering methodology. The steps of clustering for K-Means algorithm is the following.</p>

<ol>
  <li>Select number of clusters and distribute the observations accordingly</li>
  <li>Calculate the centroid for each cluster (centroid means the center point)</li>
  <li>For each observation, calculate the within-group distances (= observation - centroid)</li>
  <li>Reallocate observations into the cluster whose centroid is the closest</li>
  <li>If the cluster of any observation has changed, return to step 2 and repeat</li>
</ol>

<p>As an illustration, these steps can be visualized as follows:</p>

<p align="center">
	<img width="400" height="100" src="/images/ca/kmeans.gif" />
</p>

<p>Can you see how the clusters change for each iteration? It seems like the observations are getting fairly divided as the algorithm continues to be repeated.</p>

<p>However, there are two important points to be aware of when using the k-means algorithm. The first point is that the <strong>‚Äústarting point‚Äù</strong> is of big importance. If the starting points are somewhat isolated, then our clusters might get stuck locally as the following example.</p>

<p align="center">
	<img width="400" height="200" src="/images/ca/local.png" />
</p>

<p>This example is from <a href="https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c">here</a>. We can see that if we chose the wrong staring point, then the calculation of within-group distances for each cluster is stuck locally. So we have to pay close attention to where we will initialize our algorithm.</p>

<p>The second point is that k-means algorithm is sorely based on minimizing within-group distances, so it <strong>doesn‚Äôt consider structure of the data</strong>. Each partitioning of observations is done separately, so if we decided to use 3 clusters, then our algorithm only focuses on minimizing the within group distances for the 3 clusters. This can be problematic if our data cannot be separated based on the within group distances in the first place. A nice example is the following. (from <a href="https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means">here</a>)</p>

<p align="center">
	<img width="400" height="200" src="/images/ca/hierarchical.png" />
</p>

<p>It is obvious that the clustering shouldn‚Äôt be done like that. This is the innate limitation of k-means clustering. For above data, we have to use another algorithm such as hierarchical clustering models.</p>

<p>Therefore, we have to always obey the working principle of our universe - there is no <em>one-size-fits-all-approach</em>. Although K-Means algorithm is really powerful, always try various number of clusters and starting points and be open minded towards other algorithms.</p>

<hr />

<hr />

:ET