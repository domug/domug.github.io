I"Ú,<p>Â </p>

<p>This post will briefly introduce the concept of multicollinearity and variable selection methods as a remedy to address multicollinearity issues.</p>

<p>Â </p>

<h1 id="multicollinearity">Multicollinearity</h1>

<p>As a gentle introduction, letâ€™s get a glimpse of the concept of multicollinearity by an example. As I am a huge fan of BTS, letâ€™s imagine a situation where we have a dataset containing biological information about the member Jin and we are going to make a regression model on Jinâ€™s health index. Among the dataset, we noticed that there are variables such as â€œwidth of eyeâ€, â€œlength of eyeâ€ and â€œcolor of eyeâ€. Regarding these variables, it is reasonable to question the validity of these features as they all seem to be related to one common feature - â€œeyeâ€. Intuitively, thereâ€™s gotta be some amount of information overlapped in them so we donâ€™t necessarily need all these variables.</p>

<p>This is the issue of multicollinearity. <strong>By formal definition, it is said that multicollinearity exists when the (explanatory) variables are highly â€œcorrelatedâ€ to each other.</strong> In other words, this is very inefficient in terms of the information because we are using all the variables to unnecessarilry expend the dimension of our model whereas only a few subset of variables is sufficient. Increase in dimensionality often results in â€œcurse of dimensionalityâ€, hence a remedy is required.</p>

<p>Luckily, there are statistical methods which can possibly solve this problem and in this post, <strong>Variable Selection Method</strong> will be introduced.</p>

<p>Â </p>

<hr />

<h1 id="variable-selection">Variable Selection</h1>

<p>The objective of variable selection is to find subset of variables that are most â€œrelevantâ€ for prediction or equivalently, â€œimportantâ€ in terms of the amount of information contained in them.</p>

<p>Our universe is <strong>sparse</strong>. Stretch out your hand in the air. Can you feel something? Although there are elements like oxygen in the air, we canâ€™t feel them nor can we see them. Thus letâ€™s only focus our attention to the matters we can directly interact with because you wouldnâ€™t be likely to explain the total amount of oxygen in your room when you are asked by your friend to describe your room. Rather, what you will mainly be mentioning for description are the nitty-gritty details such as your recently bought PS5 or a fancy poster of Lionel Messi.</p>

<p>In this sense, when we are looking at a particular dataset, we always have to keep mind that <strong>we donâ€™t necessarily need every information in the data because some variables are just too trivial or redundant.</strong> In statistics, these unimportant information often cause <strong>â€œnoiseâ€</strong> and by identifying only the relevant variables, we are able to reduce the noise contained in the data which can hopefully improve the performance of our estimator (or fitted model).</p>

<p>Then how can we select only these â€œrelevant informationâ€? The foremost and most straightforward way is by the rule of thumb. We can just exclude one variable from the model at a time, calculate and record the loss (mostly MSPE), compare the loss scores for each of the trials and select the best subset of variables that showed the minumum loss. Actually, when we use the software R and conduct regression analysis, R returns the best subset of variables by this method. This method is called as <strong>subset selection</strong>. The strength of subset selection is that it is very straightforward so it is easy to implement and it produces a model that is highly <strong>â€œinterpretableâ€</strong> because the discarded variables can be interpreted as irrelavent.</p>

<p>However, subset selection has few serious drawbacks. Since it is a discrete process, meaning that variables are either selected or discarded, all of the information in the discarded variables is completely lost. Moreover, it often exhibits high variance so its performance tends to shrink when new data is provided.</p>

<p>In order to improve these apparent weaknesses, the so called <strong>shrinkage methods</strong> had been developed, which is said to be more continuous and donâ€™t suffer from high variability as much as the subset method. The most famous shrinkage methods are <strong>Ridge Regression</strong> and <strong>Lasso Regression</strong>, so letâ€™s take a brief look at each of them and their implementation in R.</p>

<p>Â </p>

<hr />

<h1 id="ridge-regression">Ridge Regression</h1>

<p>For people familiar with linear regressions, letâ€™s recall the <strong>normal equation</strong>, which is used to derive the regression coefficients (LSE) of linear regression models. Here is the formula.</p>

<p>Â </p>

<center>

$$
\begin{aligned}

X^TX\beta &amp;= X^Ty \\
\Leftrightarrow \beta &amp;= (X^TX)^{-1}X^Ty, \;\;\;\; (where\;X\;is\;of\;full\;rank)

\end{aligned}
$$



</center>

<p>Â </p>

<p>Note that the regression coefficient beta can only be derived when the design matrix X is of <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#:~:text=A%20matrix%20is%20said%20to,does%20not%20have%20full%20rank.">full rank</a>. When multicollinearity exists between the columns of a matrix, then that matrix is not full rank and we canâ€™t derive the unique least squares estimate of beta.</p>

<p>In this situation, Ridge regression handles this problem by imposing a <strong>penalty</strong> on the size of the matrix to arbitrarily make normal equation solvable. Hereâ€™s the idea.</p>

<p>Â </p>

<center>

$$
\begin{aligned}

(X^TX + \lambda I)\beta &amp;= X^Ty \\
\therefore \;\;\;\;\hat \beta_{ridge} &amp;= (X^TX + \lambda I)^{-1}X^Ty \\
\Leftrightarrow \;\;\;\; \beta_{ridge} &amp;= \underset{\beta}{\operatorname{argmin}} [(y - X\beta)^T(y - X\beta) + \lambda \lVert \beta \rVert ^2], \;\;\;\; (0 &lt; \lambda &lt; 1)



\end{aligned}
$$



</center>

<p>Â </p>

<p>By adding Î» to the left side of the normal equation, the matrix is forced to have full rank (or invertible equivalently). Here, Î» is called as <strong>complexity parameter</strong> and it controls the amount of <strong>shrinkage</strong>, meaning that the regression coefficients of irrelevant variables converge to 0.</p>

<p>In ridge regression, the regression coefficients infinitely converge to 0 but doesnâ€™t become exactly 0. Thus the larger the value of Î», the greater the amount of shrinkage. It can also be seen that the coefficients can be derived precisely in a closed form because it is a linear combination of matrix X and Y.</p>

<p>Also, the normal equation for ridge regression can be expressed explicitly in terms of the constraint as the last line of the equation above. Specifically, the constraint for ridge regression is the <strong>square</strong> of the norm of Î², so this panelty is often called as <strong>L2 panelty</strong>. In this sense, as ridge regression sort of manages, or â€œregularizesâ€ Î² to control variance, another name for ridge regression is <strong>L2 Regularization</strong>.</p>

<p>Â </p>

<p align="center">
	<img src="/images/Variable_Selection/ridge_ols.png" />
	<img width="600" height="500" src="/images/Variable_Selection/ridge2.png" />
</p>
<p>Â </p>

<p>The graph above is a comparison between OLS (Ordinary Least Squares) coefficients and the ridge coefficients based on optimal Î» suggested by R. Each regression was fitted on a randomly created data points from the multivariate normal distribution. The y-axis represents ordinary regression coefficients whereas the x-axis is ridge coefficients. It is clear that most of the ridge coefficients converged nearly to 0 while the coefficients are all over the place in the ordinary linear regression .</p>

<p>Thus by using Ridge Regression, we can discard variables that has ridge coefficient close to 0. This is one way of performing a variable selection.</p>

<p>Â </p>

<hr />

<h1 id="lasso-regression">Lasso Regression</h1>

<p>Now letâ€™s take a took at another famous shrinkage method called as <strong>lasso regression</strong>. The basic idea is similar to that of ridge regression, where the only critical difference is that lasso regression uses <strong>L1 panelty</strong>. In fact, lasso is an abbreviation for <strong>â€œLeast Absolute Shrinkage and Selection Operatorâ€</strong>. As its name explicitly suggests, lasso regression uses <strong>absolute panelty</strong> and although this might seem trivial, it creates a lot of difference and complexity compared to the ridge regression.</p>

<p>Â </p>

<center>

$$
\begin{aligned}

\hat \beta_{ridge} &amp;= \underset{\beta}{\operatorname{argmin}} [(y-X\beta)^T(y - X\beta) + \lambda \lVert \beta \rVert ^2] \\

\hat \beta_{lasso} &amp;= \underset{\beta}{\operatorname{argmin}} [(y - X\beta)^T (y - X\beta) + \lambda |\beta|]



\end{aligned}
$$

</center>

<p>Â </p>

<p>Because of the absolute panelty, the estimated coefficients $\hat \beta_{lasso}$ cannot be derived in a closed form since the normal equation is non-linear in X and Y. Hence for lasso regression, numerical optimization methods such as <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton Rhapson Method</a> is used as a roundabout solution.</p>

<p>Â </p>

<p align="center">
  <img src="/images/Variable_Selection/lasso_ols.png" />
</p>
<p>Â </p>

<p>Above is the result of fitting lasso regression to the same data used in ridge regression. It can be seen that some of the coefficients have disappeared completely. This is because the absolute panelty of lasso regression makes the regression coefficients of â€œinsignificantâ€ variables exactly 0. It can be visualized as the following graph.</p>

<p>Â </p>

<p align="center">
  <img width="600" height="500" src="/images/Variable_Selection/lasso.png" />
</p>
<p>Â </p>

<p>The x-axis corresponds to the log scaled values of Î», which is the shrinkage parameter that we discussed earlier and y-axis is the corresponding lasso coefficients. The vertical red line is the best Î» suggested by R. It can visually be examined that for the best Î», most of the coefficients have become 0. We can then use this result as a guideline to exclude those variables in future analysis and modeling procedure.</p>

<p>As a last part of this post, letâ€™s try to understand the difference between ridge and lasso regression in terms of geometrical aspect. Why is lasso regression able to make coefficients exactly 0 while ridge regression cannot?</p>

<p>Â </p>

<h1 id="remarks">Remarks</h1>

<p>We have just conducted a variable selection! Keep in mind that the purpose of variable selection is to define only a subset of important variables to be used in further analysis. Therefore, variable selection itself is not the ultimate goal and is rather a first step in a complete statistical analysis procedure.</p>

<p>Although lasso and ridge methods can be used to reduce multicollinearity, variable selection methods have inherent limitations as they merely tell us which variables to be used or not. In other words, these methods are subordinated to the original data - they donâ€™t provide any new insight or viewpoint of the data. Also, they are discrete processes like I mentioned, so there is no way of retaining information in the discarded variables.</p>

<p>In the following post, we will see how we can manipulate the data using <strong>dimension reduction methods</strong>.</p>

:ET